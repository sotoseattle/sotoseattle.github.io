---
layout: post
title: "SVM: Graphical Intuition"
date: 2013-10-09 10:02
comments: true
categories: svm, theory
---

Let's start pointing to the best tutorial I have found in the Internet "[A Gentle Introduction to Support Vector Machines in Biomedicine](http://webdoc.nyumc.org/nyumc/files/chibi/user-content/Final.pdf)" by Alexander Statnikov, Douglas Hardin, Isabelle Guyon and Constantin F. Aliferis.

And without much ado, lets begin by simplifying things. We are going to work on two dimensions so we can visualize better how SVM works. 

## the simplest analogy

Consider that we want to classify object as houses or boats. We are given data that is represented in the pictures below. We know that all the green boxed objects are houses, and all the red circles are boats. That is our training data. Then we can 'draw' the shoreline with a yellow line as we realize that houses are on land (on one side of the shoreline) and boats are usually on water (on the other side of the decision boundary). The shoreline is our decision boundary, the line that separates one type of objects from the other. New objects will be asked on which side of the line they are, and depending on the answer they will be classified as boats or houses. Caveat: this is not foolproof since, for example, small catamarans could be moored on the sand and would be misclassified as houses.

{% img /images/oct13/house_boat_1.png 410 classifying houses vs boats %}
{% img /images/oct13/house_boat_2.png 410 classifying houses vs boats %}

## decision boundary in svm

We start with a set of points and their correct labels (north and south koreans in the border). A support vector machine constructs a decision boundary (hyper-plane or set of hyper-planes in a high or infinite dimensional space), which separate different classes. 

{% img /images/oct13/korea_0.png 410 korean buffer zone %}
{% img /images/oct13/korea_3.png 410 one possible boundary %}
{% img /images/oct13/korea_1.png 410 a better boundary %}
{% img /images/oct13/korea_2.png 410 choosing the boundary %}

If we need to choose a border that separates the armies of both Koreas, both lines (green and black) are valid. Nevertheless, we have the intuition that the black one will give us the most robust one. Not only because we can grasp the importance of a buffer zone between the armies, but because given a new point to classify, the black boundary seems to work better ('generalize'). Formally expressed, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since **in general the larger the margin the lower the generalization error**.

From before we are looking to solve :

$$
\begin{align}
min_{\theta} C \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{1}{2} \sum_{i=1}^m \theta_j^2 = min_{\theta} (C A + B)
\end{align}
$$

As we will see soon, C influences the size of the margin. A large C is good (extra safety and distance). But as it grows, it is more sensitive to outliers (it will include them all), which may be an undesired effect. A low C makes the decision surface simpler and smoother, while a high C aims at classifying all training examples correctly. 

## Now, for something entirely different: vectors...

In 2D, we have two vectors u and v stated as:

$$
u = 
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix}
\mbox{ and }
v = 
\begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}
$$

where the u1 and u2 are the u's components or projections along the axis. The length of u is also called the norm of u or $\|\|u\|\|$, and can derived from Pythagoras' theorem. 

Let's define also the projection of v onto u, what we call 'p', and which is the length of the shadow of v over u if the light is perpendicular to u. p is the projection of v onto u. It is a scalar, not a vector. 

The inner product of u and v is:

$$u^T . v = p . \|u\| = u_1.v_1 + u_2.v_2 = v^T . u = p^{'} . \|v\|$$

## ...and back to margins

As we were saying, we needed to minimize the cost function as in:

$$
\begin{align}
min_{\theta} C \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{1}{2} \sum_{i=1}^m \theta_j^2 = min_{\theta} (C A + B)
\end{align}
$$

Let's imagine we choose a huge C = 100.000. To minimize that sausage we'll need to make A as small as possible. Actually, we know that A = 0 if we can find parameters that perfectly comply with:

$$
\theta^T x^{(i)} >= 1 \mbox{ if } y^{(i)} = 1
\\
\theta^T x^{(i)} <= -1 \mbox{ if } y^{(i)} = 0 
$$

In that case, because A==0, our minimization problem will have become:

$$
\begin{align}
min_{\theta} \sum_{i=1}^m \theta_j^2
\\
\theta^T x^{(i)} >= 1 \mbox{ if } y^{(i)} = 1
\\
\theta^T x^{(i)} <= -1 \mbox{ if } y^{(i)} = 0 
\end{align}
$$

For clarity we are going to simplify our conditions by considering $\theta_0 = 0$ which means that the decision boundary crosses the origin and that the $\theta$ vector also starts at the origin. Also, since we are in 2D, our vectors have only two features.

$$
\begin{align}
min_{\theta} (\theta_1^2 + \theta_1^2) = \|\theta\|_2
\end{align}
$$

So all SVM is doing is minimizing the length of the parameter vector.

If we plot an example $x^{(i)}$ in its 2D space we can see that it can be expressed as a point in the space or as a vector. Since we also have the two components of $\theta$ we can also plot it as a vector. Then, from above, the inner product $\theta^T x^{(i)} = p^{(i)} \|\|\theta\|\| = \theta^1 x_1^{(i)} + \theta^2 x_2^{(i)}$ where p is the projection of $x^{(i)}$ into $\theta$.

So our optimization objective becomes:

$$
\begin{align}
min_{\theta} \|\theta\|^2
\\
p^{(i)} \|\theta\| >= 1 \mbox{ if } y^{(i)} = 1
\\
p^{(i)} \|\theta\| <= -1 \mbox{ if } y^{(i)} = 0 
\end{align}
$$

Article of faith: The $\theta$ vector is perpendicular to the decision boundary that separates the two classes. The idea is then, which $\theta$, or which decision boundary works best?

The simplification of $\theta_0 = 0$ only means that the decision boundary (and also the $\theta$ vector) passes through the origin.

DEVELOPING......




