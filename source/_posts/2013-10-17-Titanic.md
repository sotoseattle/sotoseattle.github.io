---
layout: post
title: "Kaggle Titanic: Features (top 5)"
date: 2013-10-17 18:02
comments: true
categories: kaggle, titanic, features, svm
---

I have spent the last few days in the [Titanic competition](http://www.kaggle.com/c/titanic-gettingStarted). It is another easy, entry level, tutorial example to get a grasp of how machine learning works. Again I didn't use the random forest model as suggested and got lazy with the only two things I know, logistic regression and SVM.

I am not going to delve in the details of the data structure, suffice to say that we are given a part of the Titanic's manifest with the survivorship rates. Our goal is for another set of passengers predict their survival.

I wanted to test the waters first so I used a very simple logistic regression with few variables: Sex, Pclass and Fare. That landed me in the bottom 15%, as usual, with a very low accuracy.


The main concept of the competition is the importance of the features we use. For example, it is obvious that Sex (gender) is an important feature (predictor), as it is Age (younger passengers have a better chance) and means (richer fare better than poor passengers). All this can be visualized through plots of different variables and their correlations.


So doing an SVM (Gaussian kernel) with the obvious candidates of Sex, Pclass, Fare gets us to a fair accuracy. If we add Age we don't improve things, mainly because many entries are NaN. To solve it, I substituted those unknown ages with 24, the median age of passengers of 3rd class (which I assume to be ones least known). This raised the bar a bit more.

## beware of rates and sets

Consider that the data set is small, so depending on how you partition it between training and cross validation the accuracy on CV varies. I started seting up two sets:

- training of the first 691 records, and the following 200 for cross validation.
- training of the last 691 records, and the previous 200 for cross validation.

I achieved a higher accuracy when both were high (aprox. 85%) than when they were apart (86.5% and 82%). Furthermore, I always got much higher accuracies on the CV set than on the test set, so 85% translated into 80% in the test set.

Then I realized that I could use the whole training set and use the StratifiedKFold utility of scikit. Much easier way to go about it. 

{% img center /images/kaggle_titanic/hypeparams.png 600 hyperparams search%}

## python pandas manipulation

Some tips to remember as to how munge data in pandas:

```python dataframa manipulation
# basic mapping
data.Sex = data.Sex.map(lambda x: 1. if x=='male' else 0.)

# regexp extraction
import re

def extract_patt(patt, linea):
    matchObj = re.match(patt, linea)
    result = NaN
    if matchObj:
        return matchObj.group(1).lower()
    else:
        print "No match!!"
        return NaN

def extract_title(linea):
    return extract_patt('^.+,\s(.+)\..+', linea)

# apply functions to dataframe => Fare NaN
fares_by_class = data.groupby('Pclass').Fare.median()

def getFare(example):
    if isnull(example['Fare']):
        example['Fare'] = fares_by_class[example['Pclass']]
    return example

data = data.apply(getFare, axis=1)

# plotting
bp = data.boxplot(column='Age', by='Pclass', grid=False)
for i in [1,2,3]:
    y = data.Age[data.Pclass==i].dropna()
    # Add some random "jitter" to the x-axis
    x = np.random.normal(i, 0.04, size=len(y))
    plot(x, y, 'r.', alpha=0.2)
```

## and then I got creative:

I established as base features the Sex, Pclass, Fare and Age. To fill the missing values of Age I first extracted the titles from the names, then used the median of the title group as best guess. I used similar approaches for other missing values.

Then I added 3 identity feature vectors [C, Q, S] depending on where the passenger embarked on. So if Mss. Roberts embarked on Southampton the three vectors would be [0, 0, 1].

I wanted to give more power to the mix experience with money so I created another vector that resulted from multiplying Age and Pclass. The intuition behind was that when the moment came to choose among passengers, an elder statesman would have more 'presence' to win a social standoff.

I didn't like the Parch data because it mixes parents with children, which I think is key. And neither I liked SibSp because it mixes spouses with siblings. So I added them all in a single vector with the idea that people with family would fare differently that those without.

I added another identity vector that described if the person was married based on the title associated in the name (1 for Mr and Mrs).

Finally I created a last identity vector with the following characteristics:

- if the passenger is a woman, get a 1 if a relative (searched by same last name) or cabin fellow (someone in the same cabin) survived.
- if the passenger is a man, get a 1 if the same applies but the survivor is older.

The intuition was based on the assumption that if someone survives, he/she would try to also help a younger male (son) or relative.

I also tried many other weird features like using the letter of the Cabin which didn't give good results (a scatter plot of this with Pclass is enough to see there is not much there).

## better way to fill in missing data

Age seems to be a key feature and many of its values are missing. The median per title is a good approach but we can do better. Why not train an SVM? I created bins of age <= [14,30,60,...], and trained an SVM with Pclass, Parch, SibSp, isMarried and title_level (a different int for each unique title) to assign each missing age to one of the new age bins. My accuracy on a cv set was a mere 67.5%.

Now, consider that I left charisma as it was because it proved to be a good feature (computed as the product between the class and the age computed as given or median of title).

Finally, I reduced the number of features so instead of the initial 12 I went down to 10. (Used a loop of possible features combinations and let it self evaluate the best combo).

## conclusions

With my latest iteration, number 13, I landed on the 77 spot of 6,978 participants (as of today).

{% blockquote %}
Score on the test set (Kaggle): 81.818%. 
{% endblockquote %}


The key is the feature set. It needs to be expressive, but too much bullshit only drives the model down. So only add important information (remember the boost got from using 20x20 instead of the 28x28 images on the OCR competition).

Treat the results from your cross validation with a grain of salt. Obsessing too much about them may push you to overfit your data! My best score resulted from a model that gave me a lower 85% on my cross validation sets. A good hint comes from the number of support vectors distilled by the model!

I have only spent a couple of days on features, with a creative team I am sure that the results can be improved greatly. There are many ideas to try like:

- use a neural network to get a better age estimate.
- mix and match models and make them vote on predictions.
- try new narratives than can be converted into features (like the intuition that a father who survives will do the utmost for his children to survive too).



