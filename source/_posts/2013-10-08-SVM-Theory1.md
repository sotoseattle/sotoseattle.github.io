---
layout: post
title: "SVM: Derivation from Logit"
date: 2013-10-08 09:02
comments: true
categories: svm, theory
---

Compared to both logistic regression and neural networks, a Support Vector Machine (SVM) sometimes gives a cleaner way for learning non-linear functions.

The logistic regression hypothesis and cost functions for a single example are:

$$
h_{\theta}(x) = \frac{1}{ 1  + e^{ -(\theta)^T x } } = g(z) \mbox{ where } z = \theta^T x
\\
J_{\theta}(x) = -(\log h_{\theta}(x) + (1-y) \log(1 - h-{\theta}(x)))
$$

Looking at the sigmoid activation function, how does logistic regression work?

{% img center /images/oct13/sigmoid.png 400 400 sigmoid activation function %}

If we have an example x where y = 1, we hope that h(x) is close to 1. For that to happen $z = Î¸^T x$ must be much larger than 0. If we look at the cost function, we see that only the first term works (for y = 1).  So a single example with y = 1 will have a z >> 0, and its cost contribution will be very small.

<div style="text-align:center">
{% img /images/oct13/term1.png 400 400 sigmoid activation function %}
{% img /images/oct13/term2.png 400 400 sigmoid activation function %}
</div>

Similarly if y = 0, we need h to be close to 0, for which z needs to be very negative. And in the cost function now only the second term is active. So a single example with y = 0 will have a z << 0, and its cost contribution will be very big.

SVM just takes the above cost functions and simplifies them with straight lines in the following manner:

<div style="text-align:center">
{% img /images/oct13/term1b.png 400 400 sigmoid activation function %}
{% img /images/oct13/term2b.png 400 400 sigmoid activation function %}
</div>

We call this simplified cost curves $cost_1$ and $cost_2$. The key is that SVM does not outputs probabilities like before, but still classifies pretty well. Actually, for learning non linearly, it gives us a cleaner and efficient approach.

So finding the parameters from the cost function in logit:

$$
\begin{align}
min_{\theta} \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} (-\log h_\theta(x^{(i)})) + (1-y^{(i)}) (-\log (1-h_\theta(x^{(i)})))\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2 
\end{align}
$$

becomes:

$$
\begin{align}
min_{\theta} \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2 
\end{align}
$$

Since we don't care about probabilities, only classification results, as we search for the minimizing parameters we neither care about multiplying constants. So by convention, SVM notation:

- gets rid of constant 1/m.
- use C = 1/$\lambda$ instead of $\lambda$.

$$
\begin{align}
J(\theta) = min_{\theta} C \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{1}{2} \sum_{i=1}^m \theta_j^2 
\end{align}
$$

So we have that if $\theta^T x >= 1 => h_{\theta}(x) = 1$.

Consider that in logistic regression we were classifying as y = 1 if z was positive. SVM needs it to be >= 1. In a sense, SVM wants to be more sure and moves the goalpost by adding an extra safety margin.

