---
layout: post
title: "PGN: Variable Elimination"
date: 2013-12-13 08:01
comments: true
categories: PGN, theory
---

Given a network of random variables (Bayes of Markov) defined by a set of factors the way to do Variable Elimination is straightforward:

- Reduce all factors by evidence and have set of factors (F)
- Choose the variable to eliminate (z)
- Gather the factors that have z in their scope 
- Multiply them together into a new factor ($\lambda$)
- Marginalize $\lambda$ for z to produce $\tau$
- The new set of factors is reduced by the factors 'sum-producted' plus the new one $\tau$
- ... repeat until ...
- the last set of factors we just multiply together and renormalize

The complexity of this algorithm depends linearly on:

- the size of the model (number variables and factors), and 
- on the number of variables of the largest factor generated in the process ($tau$), whose size is in itself exponential in its scope (i.e. 9 binary vars => 2^9).

Therefore, the elimination ordering is the key to the algorithm's efficiency (because the order defines the size of the generated factors).

```python variable elimination single pass
def eliminateVar(self, F, varList, E, var2elim):
        # separate factors into two lists, those that use var2elim (F_Cluster) and the rest (F_Rest)
        F_Cluster, F_Rest, Lambda_vars = [], [], []
        for f in F:
            if var2elim in f.variables:
                F_Cluster += [f]
                Lambda_vars += f.variables
            else:
                F_Rest += [f]
        
        if F_Cluster!=[]:
            Lambda_vars = tuple(set(sorted(Lambda_vars)))

            # multiply the factors in Cluster... (lambda) ...and marginalize by var2elim (tau)
            tau = F_Cluster.pop(0)
            for f in F_Cluster:
                tau = FactorOperations.multiply(tau, f)
            if tau.variables != [var2elim]:
                tau = FactorOperations.marginalize(tau, var2elim)
            
            # add to unused factor list the resulting tau ==> new factor list with var eliminated
            F_Rest += [tau]
            
            # update the edges (connect all vars inside new cluster, & disconnect the eliminated variable)
            for vi in Lambda_vars:
                for vj in Lambda_vars:
                    E[varList.index(vi), varList.index(vj)] = 1
            E[varList.index(var2elim),:] = 0
            E[:, varList.index(var2elim)] = 0
            
            F = F_Rest

        return [F, E]
```

Graphically, to find the best ordering we:

- transform the directed graph to undirected (from Bayes Net to Induced Markov Net)
- moralize the V structures by connecting the parents. The key being that every factor's scope is represented in the graph, so if Fa(A,B,C) => there must be connections between eacg pair of them: AB, AC, BC.
- as we eliminate variables two things happen:
	- the eliminated variables are removed from the graph (and their links)
	- the resulting scope of the $\tau$ must also be reflected in the graph with additional connections! (filled edges). Another way to see it, all the variables connected to the eliminated one, become connected among themselves.

The initial graph plus all the filled edges is called the Induced Graph. This graph is very important because:

- every factor produced during VE is a clique in the Induced Graph, and
- every maximal clique in the Induced Graph is factor produced during VE
- an Induced Graph is triangulated (no loops of length > 3 without a bridge)

Ways to find a good ordering are:


1. performing a greedy search using an ad-hoc cost function and at each point eliminate the node with the smallest cost. Examples: 
- min-neigbors: pick the node with the minimum number of neighbors (smallest factor), 
- min-weight: considering that different variables have different cardinalities it would be better a factor of 10 binary vars than a factor of two vars but each of 100 possible values.
- min -fill: choose vars that create the min amount of new connections (a very good approach),
- weighted min-fill.
2. finding a low-width triangulation of original graph (beyond my reach)

```python min neighbors variable selection
@classmethod
def firstMinNeighborVar(cls, varList, edgematrix):
    connections = tuple(np.sum(edgematrix, axis=1))
    minE = float('+inf')
    for e in connections:
        if e > 0 and e < minE:
            minE = e
    return varList[connections.index(minE)]
```

