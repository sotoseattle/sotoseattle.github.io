---
layout: post
title: "MCMC MH Swendsen Wang"
date: 2014-02-15 8:01
comments: true
categories: PGN
---

We continue with Assignment 5 from Prof. Koller's Coursera course: "Probabilistic Graphical Models". I am including the explanation from the assignment paper because it is the most clear explanation I have found for the Swendsen Wang sampling method, which is a hair rasing trip in on itself.

## Theory

In Ising models and image segmentation applications, pairwise Markov networks are frequently used. This means specially for strong potentials adjacent variables tend to take on the same values. This makes it hard to explore the space for possible assignments by changing the value of one variable at a time, like Gibbs does. The Swendsen-Wang algorithm has been specifically designed for this kind of networks and overcomes this difficulty by changing, at the same time, a group of adjacent variables that have the same value.

In Swendsen-Wang, we start with a model that has variables connected in pairwise factors in a Markov graph. All the variables can take on the same values. To do MCMC, we start as usual in some state x. To get to the next state, we start with the graph. We break all the edges between variables that have different values. Then, we also break the rest of the edges between any nodes i and j with probability $$(1−q_{i,j})$$, where $$q_{i,j}$$ is a probability that can depend on i and j but not their current assignment. 

Once we’ve broken all those edges, we’re left with a graph with connected groups of variables, with all variables in a group sharing the same assignment. We randomly pick one with a uniform distribution. That’s Y . We choose a new assignment, using a probability distribution R. With the new assignment to the variables in Y , we now have the new state x'; in other words, the new assignment x' is the same as x, except that the variables in Y have changed value. As with all MH, we accept the state with a probability A.

{% img center /images/feb14/SW_procedure.png 500 Swendsen Wang Procedure 1%}

To compute Q, R, and A, we use the following equations. Let C(Y\|x) be the probability that a set Y is selected to be updated using this procedure, given the current state is x. Then we somehow divine Q(x → x'), the chance that we’ll try a transition from x to x' by changing Y , as the following:

$$
Q(x \rightarrow x') = C(Y|x) R(Y = l'|x_Y )
$$

So we can compute the ratio for A (acceptance probability):

$$
\frac{Q(x' \rightarrow x)}{Q(x \rightarrow x')} =\frac{C(Y|x') R(Y = l|x'_Y )}{C(Y|x) R(Y = l'|x_Y )}
$$

And we realize that obviously the following is a truism:

$$
\frac{C(Y|x')}{C(Y|x)} = \frac{\prod_{(i,j) \in \epsilon (Y, X_{l'}^{'}-Y)}(1 - q_{i,j})}{\prod_{(i,j) \in \epsilon (Y, X_{l}-Y)}(1 - q_{i,j})}
$$

Where: $$X_l$$ is the set of vertices with label l in x, $$X'_{l'}$$ is the set of vertices with label l' in x'; and where $$\epsilon(Y,Z)$$ (between two disjoint sets Y,Z) is the set of edges connecting nodes in Y to nodes in Z.

## SW Uniform variant

Although the intuition behind breaking the graph and changing the assignment of a connected component makes sense, coming up with Q and R are beyond my grasp. Nevertheless, we compute SW in two variants. The first one is easy, it uses a Q that is uniform, so we set all the $$q_{ij}$$ to 0.5 (our Ising net is all binomial). We also set R to be uniform. Nevertheless Let's go step by step:


The first thing we have to take into consideration is to create a data structure in our MCMC_MHSW object that holds, for each edge between nodes i and j the value of $$q_{ij}$$. In our variant all Q are equal, being the uniform.

```python Q by edges
self.q_list = [None]*(self.G.V)
for i in range(self.G.V):
    self.q_list[i] = {}
    for k in self.G.adj[i]:
        if self.variant==1:
            self.q_list[i][k] = 0.5
```


Now we start processing. Given the graph and a complete assignment we remove the edges connecting variables that have different values. Also, we take the opportunity to break edges based on the probability Q.

```python Breaking Edges Based on Assignment
def common_components_and_Q(self, ass):
    '''return connected edges between nodes that share assignment, and pass Q probability'''
    edges = [[]]*self.G.V
    for i,j in self.one_pass_edges:
        if ass[self.G.v[i]]==ass[self.G.v[j]] and random.uniform(0,1) < self.q_list[i][j]:
            edges[i] = edges[i] + [j]
            edges[j] = edges[j] + [i]
    return edges
```

With this new graph (the same as the original but with rearranged edges) we need is a way to find its connected components. That is easy with DFS (Depth First Search) and the Princeton Algorithm's course from Coursera. Given a graph G we can find how many components there are, and to which one each variable belongs.

```python Connected Components
class CC(object):
    def __init__(self, G):
        self.G = G
        self.dni = [[]]*self.G.V
        self.marked = [False]*self.G.V
        self.count = 0
        self.size = [[]]*self.G.V
        for v in range(self.G.V):
            if self.marked[v]==False:
                self.dfs(v)
                self.count += 1
    
    def dfs(self, v):
        self.marked[v] = True
        self.dni[v] = self.count
        self.size[self.count] = self.size[self.count] + [1]
        for w in self.G.adj[v]:
            if (self.marked[w]==False):
                self.dfs(w)
```

The code for a method to return a new assignment begins to take form:

```python Finding a new Assignment
def MH_Swendsen_Wang(self, from_ass):
    # build new graph breaking edges where nodes differ in assignment and by Q
    G_prime = copy.copy(self.G)
    G_prime.adj = self.rearrange_edges(from_ass)

    # get connected components
    common_comps = CC(G_prime)
    ...
```

With the components identified we can select one (randomly), and retrieve all its variables.

```python cont
def MH_Swendsen_Wang(self, from_ass):
    ...
    # select a common component randomly
    Y_index = random.randint(0,common_comps.count-1)
    Y_v_id = [i for i, x in enumerate(common_comps.dni) if x == Y_index]
    cardio = [self.G.v[x].totCard() for x in Y_v_id] 
    assert(len(unique(cardio))==1) # check all have same ass and cardinality
    d = cardio[0]
    ...
```

We compute the R (actually the log of R for underflows). In our case of first variant and given that all variables are bianry, this  uniform distribution R becomes ```[log(0.5), log(0.5)]```. The old value of Y can be taken from any variable of Y (we choose whichever is first), and the new assignment is randomly extracted from the R distribution. The new complete assignment becomes then the same as the original except for the variables in Y, which take now on the new value derived from R.

```python cont
def MH_Swendsen_Wang(self, from_ass):
    ...
    logR = [math.log(1./d)]*d
    old_value = from_ass[self.G.v[Y_v_id[0]]]
    new_value = self.randSampleDist(d, np.exp(logR))
    to_ass = dict((k,new_value) if k.id in Y_v_id else (k, from_ass[k]) for k in from_ass)
    ...
```

Now comes the part of the messy equation and the epsilons. We need to compute the ratio of the products of Qs for all edges. We do it in log space with sums.

```python cont
def MH_Swendsen_Wang(self, from_ass):
    ...
    # log-ratio of the prob of picking Y given we transition from_ass to_ass
    # we need to iterate over all edges
    log_QY_ratio = 0.0
    for i,j in self.one_pass_edges:
        if (i in Y_v_id and j not in Y_v_id) or (i not in Y_v_id and j in Y_v_id):
            x,y = self.G.v[i], self.G.v[j]
            if from_ass[x] == old_value and from_ass[y] == old_value:
                log_QY_ratio = log_QY_ratio - log(1 - self.q_list[i][j])
            if to_ass[x] == new_value and to_ass[y] == new_value:
                log_QY_ratio = log_QY_ratio + log(1 - self.q_list[i][j])
    ...
```

Finally, we have everything to compute A. We compute the stationary probabilities as we did with MH Uniform, easy. Then the acceptance probability just follows the theoretical formula given before. We check randomly if pass or not the acceptance threshold, and if we do we send the new complete assignment as the new sample (if not , we stay put).

```python cont
def MH_Swendsen_Wang(self, from_ass):
    ... 
    acceptance, from_pi, to_pi = 0.0, 0.0, 0.0
    for fu in self.G.factors:
        from_pi += math.log(FactorOperations.reduce_to_value(fu, from_ass))
        to_pi += math.log(FactorOperations.reduce_to_value(fu, to_ass))
    acceptance = (math.exp(to_pi)/math.exp(from_pi))*math.exp(log_QY_ratio)*\
                      math.exp(logR[old_value])/math.exp(logR[new_value])
    
    if random.uniform(0,1) < acceptance:
        from_ass = to_ass
        
    return from_ass
```

## Analysis

Following the same procedures and initial inputs as we did for Gibbs and MH Uniform we produce the following chart for comparison purposes. All pretty impressive.

Mixing time:

{% img center /images/feb14/MHSW_1_mix.png 500 Mixing Windows Run 1%}

Sample Size:

{% img center /images/feb14/MHSW_1_size.png 500 Sample Size Run 1%}

Marginals Convergence for the extreme case [0.95, 0.05]:

{% img center /images/feb14/MHSW_1_cross.png 500 %}

Histogram of dist to true marginal based on 100 runs on 16 node Ising:

{% img center /images/feb14/MHSW_1_hist.png 500 %}

Although the hsitogram runs over 70,000 is overkill (WS arrives at the stationary distribution way earlier), the results are impressive. An Average error of 0.04 with standard deviation of 0.02. All very impressive. Cannot wait to see if the Swendsen Wang variant 2 achieves even better results!



