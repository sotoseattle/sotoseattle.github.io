---
layout: post
title: "MCMC MH Swendsen Wang v.2."
date: 2014-02-16 8:01
comments: true
categories: PGN
---

In variant II we change the way we compute Q and R.

## The Q Continuum

Q, the transition probability, now depends on the probabilities of the factors on each edge. If I understand it all correctly, it is defined as a ratio between the sum of probabilities of both nodes in the edge maintaining its old assignment, to the the sum of probabilities of all possible assignment for both nodes. Remember that in variant I this was uniformly set to 0.5.


$$
q_{ij} := \frac{\sum_u{\Phi_{i,j}(u,u)}}{\sum_{u,v}{\Phi_{i,j}(u,v)}}
$$

The way we compute each $$q_{ij}$$ is not complicated. Beware that we are assuming there is a single factor in each edge.

```python Q variant II
def qij_var_2(self, i, j):
    x,y = self.G.v[i], self.G.v[j]
    fuedge = [fu for fu in self.G.factors if (x in fu.variables) and (y in fu.variables)][0]
    num, den = 0., 0.
    for ass_1 in range(x.totCard()):
        for ass_2 in range(y.totCard()):
            value = FactorOperations.reduce_to_value(fuedge, {x:ass_1, y:ass_2})
            den += value
            if ass_1==ass_2:
                num += value
    return num/den
```

## R, not as simple as it seems

R will be the logDistAss probability that we computed for Gibbs, but now instead of observing one var at a time, we observe a group. It has taken me my sweet time to figure out how to do it (based on what I already did in PA5 but more out of luck than reasoning).

```python R is Simple
def MH_Swendsen_Wang(self, from_ass):
	...
    elif self.variant==2:
        logR = self.logDistAss([self.G.v[i] for i in Y_v_id], from_ass)
    ...
```

The key for LogDistAss is that when we are looking at extracting from a factor the probability of a variable given that all the other variables in the factor have an assignment, like in the Bigss sampling code, it is very fast and we already have utility method that does that.

When instead of one, we have a set of variables, we need to reduce the factor by the assigned variables and then marginalize by the rest up to obtaining the distribution. The following code is messy, my understanding of it is not completly firm, and it should be refactored, which I leave for a future time.

```python LogProbability of Partial Assignment Not So Easy
def logDistAss(self, sample_vars, evidence):
    cardio = set([e.totCard() for e in sample_vars])
    if len(cardio)>1:
        raise Exception("All sampling vars must have the same cardinality")
    logbp = [0.]*cardio.pop()
    
    all_f = list(set(fu for a in sample_vars for fu in self.var_to_factors[a]))
    for fu in all_f:
        vs = [x for x in fu.variables if x in sample_vars]
        
        evi, mar = {}, []
        for k in fu.variables:
            if k not in vs:
                evi[k] = evidence[k]
            else:
                mar.append(k)
                
        if len(vs)==1:
            dist = FactorOperations.reduce_to_dist(fu, evi)
        else:
            gg = FactorOperations.observe(fu, evi, False)
            for x in mar:
                if len(gg.variables)>1:
                    gg = FactorOperations.marginalize(gg, x)
            dist = gg.values
            
    logbp += np.log(dist)

    logbp = logbp - min(logbp) # to avoid underflow when back in normal space
    return logbp
```

## Analysis

Following the same procedures and initial inputs as we did for Gibbs, MH Uniform and SW-I we produce the following chart for comparison purposes.

Mixing time:

{% img center /images/feb14/MHSW_2_mix.png 500 Mixing Windows Run 1%}

Sample Size:

{% img center /images/feb14/MHSW_2_size.png 500 Sample Size Run 1%}

Marginals Convergence for the extreme case [0.95, 0.05]:

{% img center /images/feb14/MHSW_2_cross.png 500 %}

Histogram of dist to true marginal based on 100 runs on 16 node Ising:

{% img center /images/feb14/MHSW_2_hist.png 500 %}

Amazing. An Average error of 0.04 0with standard deviation of 0.01.



