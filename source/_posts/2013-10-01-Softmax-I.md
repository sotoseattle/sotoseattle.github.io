---
layout: post
title: "Softmax Theory: $h_θ(x)$"
date: 2013-10-01 09:02
comments: true
categories: regression, softmax, theory
---


These notes are based on the material publicly available at [UFLDL](http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression) site from Stanford University (check it out, is a great resource) and Wikipedia.

## intro

Softmax regression is a.k.a multinomial logistic regression or multinomial logit. In fields like NLP, when a classifier is implemented using softmax regression, it is commonly known as a maximum entropy classifier, conditional maximum entropy model or MaxEnt model for short.

It is used when the dependent variable in question is nominal (equivalently categorical, meaning that it falls into any one of a set of categories which cannot be ordered in any meaningful way) and for which there are more than two categories. Some examples would be:


We are interested in multi-class classification (as opposed to only binary classification), and so the label y can take on k different values, rather than only two (for logistic regression). Being a classification problem, all label classes are equivalently categorical, meaning that the set of categories cannot be ordered in any meaningful way and we just decide which to assign. Examples:

- Which major will a college student choose, given their grades, stated likes and dislikes, etc.?
- Which blood type does a person have, given the results of various diagnostic tests?
- In a hands-free mobile phone dialing application, which person's name was spoken, given various properties of the speech signal?
- Which candidate will a person vote for, given particular demographic characteristics?
- Which country will a firm locate an office in, given the characteristics of the firm and of the various candidate countries?

## hypothesis function

We will have a set of features (independent variables) that will predict the dependent variable ($y$), the assigned class. Softmax regression is a particular solution to the classification problem that <b>assumes that a linear combination of the observed features</b> and some problem-specific parameters can be used to determine the probability of each particular outcome of the dependent variable. 

As with other types of regression, there is no need for the independent variables to be statistically independent from each other (unlike, for example, in a naive Bayes classifier); however, collinearity is assumed to be relatively low, as it becomes difficult to differentiate between the impact of several variables if they are highly correlated.



In our training set $(x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)})$, we now have that $y^{(i)} \in \\{1, 2, \ldots, k\\}$ instead of taking the values $ \\{0, 1\\} $.

[Note] Since we'll be working in Python our convention in the code will be to index the classes starting from 0, rather than from 1 (in these notes and coding in Octave).

Given a test input x, we want our hypothesis to estimate the probability that p(y = j\|x) for each value of $j = \\{1, \ldots, k\\}$. In other words, give the input x, we want to estimate the probability that its label is each of the k different possible values. Thus, our hypothesis will output a k dimensional vector (whose elements sum to 1) giving us our k estimated probabilities. Concretely, our hypothesis $h_θ(x)$ takes the form:


$$
\begin{align}
h_\theta(x^{(i)}) =
\begin{bmatrix}
p(y^{(i)} = 1 | x^{(i)}; \theta) \\
p(y^{(i)} = 2 | x^{(i)}; \theta) \\
\vdots \\
p(y^{(i)} = k | x^{(i)}; \theta)
\end{bmatrix}
=
\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix}
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix}
\end{align}
$$


The term 

$$\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }$$ 

normalizes the distribution, so that it sums to one. $θ$ is the set of parameters of our model, a (k, n) matrix obtained by stacking up $\\{ \theta_1, \theta_2, \ldots, \theta_k \\}$ in rows, so that

$$
\theta = \begin{bmatrix}
\mbox{---} \theta_1^T \mbox{---} \\
\mbox{---} \theta_2^T \mbox{---} \\
\vdots \\
\mbox{---} \theta_k^T \mbox{---} \\
\end{bmatrix}
$$

Assuming input data x as a matrix (m, n) the code would be:


```python hypothesys function
import numpy as np

def h(thetas, x):
    H = thetas.dot(x.T)                        # basic product to compute h
    H = H - np.amax(H, axis=0)                 # to avoid overflow subtract max
    H = np.exp(H).T                            # exponentiate
    suma = np.sum(H, axis=1).reshape((-1,1))   # compute normalization constant
    H = np.divide(H, suma)                     # and normalize
    return H
```

## overflow trick

When the products $\theta_i^T x^{(i)}$ are large, the exponential function $e^{\theta_i^T x^{(i)}}$ will become very large and possibly overflow. To avoid this there is an easy solution - observe that we can multiply the top and bottom of the hypothesis by some constant without changing the output:

$$
\begin{align} 
h(x^{(i)}) &=
 
\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix} \\
&=

\frac{ e^{-\alpha} }{ e^{-\alpha} \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix} \\

&=

\frac{ 1 }{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} - \alpha }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} - \alpha } \\
e^{ \theta_2^T x^{(i)} - \alpha } \\
\vdots \\
e^{ \theta_k^T x^{(i)} - \alpha } \\
\end{bmatrix} \\

\end{align}
$$

So, to prevent overflow simply subtract some large constant value from each of the $\theta_j^T x^{(i)}$ terms before computing the exponential. In practice, for each example, you can use the maximum of the $\theta_j^T x^{(i)}$ terms as the constant.

## when to use

When classes are mutually exclusive a softmax regression classifier is appropriate. Otherwise it may be more appropriate to build separate logistic regression classifiers.

For a digit OCR recognition softmax is ok because the labels are mutually exclusive and we'll just choose one among them. One label will have a high probability and the rest almost null.


For a letter OCR recognition that not only relies on the simple probability of the label but, for example, in adjacent letters, it may not be appropiate because we'll need the different probabilities so we can weight them in different ways with other parts of the model. For example with logistic regression: a character may be an "i" (p=0.3) or an "l" (p=0.5), but it is followed by "ng".

## overparametrization

Softmax regression has an unusual property that it has a "redundant" set of parameters. To explain what this means, suppose we take each of our parameter vectors $θj$, and subtract some fixed vector $ψ$ from it, so that every $θj$ is now replaced with $θj − ψ (\forall j=1, \ldots, k)$. Our hypothesis now estimates the class label probabilities as

$$
\begin{align}
p(y^{(i)} = j | x^{(i)} ; \theta)
&= \frac{e^{(\theta_j-\psi)^T x^{(i)}}}{\sum_{l=1}^k e^{ (\theta_l-\psi)^T x^{(i)}}}  \\
&= \frac{e^{\theta_j^T x^{(i)}} e^{-\psi^Tx^{(i)}}}{\sum_{l=1}^k e^{\theta_l^T x^{(i)}} e^{-\psi^Tx^{(i)}}} \\
&= \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}}}.
\end{align}
$$

In other words, it does not affect our hypothesis' predictions at all! This shows that softmax regression's parameters are "redundant." More formally, we say that our softmax model is over parameterized, meaning that for any hypothesis we might fit to the data, **there are multiple parameter settings that give rise to exactly the same hypothesis function** $hθ$ mapping from inputs x to the predictions.

## softmax is a generalization of logistic regression

In the special case where k = 2, the softmax regression hypothesis outputs:

$$
\begin{align}
h_\theta(x) &=

\frac{1}{ e^{\theta_1^Tx}  + e^{ \theta_2^T x^{(i)} } }
\begin{bmatrix}
e^{ \theta_1^T x } \\
e^{ \theta_2^T x }
\end{bmatrix}
\end{align}
$$

Taking advantage of the fact that this hypothesis is over parameterized and setting $ψ$ = $θ$1, we can subtract $θ$1 from each of the two parameters, giving us

$$
\begin{align}
h(x) &=

\frac{1}{ e^{\vec{0}^Tx}  + e^{ (\theta_2-\theta_1)^T x^{(i)} } }
\begin{bmatrix}
e^{ \vec{0}^T x } \\
e^{ (\theta_2-\theta_1)^T x }
\end{bmatrix} \\


&=
\begin{bmatrix}
\frac{1}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
\frac{e^{ (\theta_2-\theta_1)^T x }}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } }
\end{bmatrix} \\

&=
\begin{bmatrix}
\frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
1 - \frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
\end{bmatrix}
\end{align}
$$

Thus, replacing $θ$2 − $θ$1 with a single parameter vector $$', we find that softmax regression predicts the probability of each class like with logistic regression. 

$$
\frac{1}{ 1  + e^{ (\theta')^T x^{(i)} } }
\\
1 - \frac{1}{ 1 + e^{ (\theta')^T x^{(i)} } }
$$
