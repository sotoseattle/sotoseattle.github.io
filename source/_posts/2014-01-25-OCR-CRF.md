---
layout: post
title: "OCR Words (III): Conditional Random Fields"
date: 2014-01-25 08:01
comments: true
categories: PGN, theory
---

And so we finally can run our own character recognition software. As before, we use the data from Prof. Daphne Koller, Stanford University, Coursera course. We have a set of 99 words. Each word contains a list of images where each image correspond to a handwritten (lowercase) letter. We also have the ground truth for each word so we can rate the accuracy of the code.

For each word, we already knwo how to build all the factors involved:

```python OCR Framework
# We hold words in dictionary allWords. For each word tow keys, 'gT' (ground truth) and 'img' (the image)
alpha = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']
k = len(alpha)
image_px = 128

def MAP_Word(word):
    chars = len(word['gT'])
    vall = [None]*chars
    for i in range(chars):
        vall[i] = Rvar.Rvar(i, 26)
    f = []
    for i in range(chars):
        f.append(singletonFactor(vall[i], word['img'][i]))
    for i in range(chars-1):
        f.append(pairwiseFactor(vall[i], vall[i+1]))
    for i in range(chars-2):
        f.append(tripletFactor(vall[i], vall[i+1], vall[i+2]))

    # choose the top two similar images
    ss = []
    for i in range(chars):
        for j in range(i+1, chars):
            ss.append([vall[i], vall[j], similarity(word['img'][i], word['img'][j])])
    ss = sorted(ss, key = lambda x: x[2])
    top1 = ss.pop()
    f.append(image_simil_factor(top1[0], top1[1], top1[2]))
    top2 = ss.pop()
    f.append(image_simil_factor(top2[0], top2[1], top2[2]))
```

Now that we have all factors listed we can compute the clique tree and perform belief propagation to find the MAP assignment.

```python (...cont)
def MAP_Word(word):
	...
	cc = CliqueTree.CliqueTree(f)
    cc.calibrate(isMax=True)
    
    sol = []
    for vari in vall:
        for beta in cc.beta:
            if vari in beta.variables:
                fu = copy.copy(beta)
                for g in (set(beta.variables) - set([vari])):
                    fu = FactorOperations.max_marginalize(fu, g)
                maxi = np.max(fu.values)
                sol.append(list(fu.values).index(maxi))
                break
    return sol
```

For example, consider the first word in the set: 'torturing'. It has 9 variables and initially 26 factors (9 singletons, 8 pairwise, 7 triplets and 2 similiraty). The top similarities are between the 't's (var 0 and 3) and 'r's (var 2 and 5). Without the similarity factors the tree would have 7 nodes with three consecutive letters each. The similarities force some nodes together and we end up with just four nodes. The less nodes, with the most variables imply the more complex the computation since we end up with huge factors. Finally, see that in this case, with factors among adjacent variables, our tree becomes a chain.

{% img center /images/jan14/ocr_crf_example.png 600 'torturing' Example%}

Running a simple scoring method over the whole 99 words tells us how well we do:

```python Measure infered vs. ground truth
def score():
    tot_chars, tot_words, chars_ok, words_ok = 0., 0., 0., 0.
    for i in range(len(allWords)):
        tot_words +=1.
        w = allWords[i]
        count = 0.
        num_letters = len(w['gT'])
        map = MAP_Word(w)
        for e in range(num_letters):
            tot_chars +=1.
            if map[e]==w['gT'][e]:
                chars_ok +=1.
                count +=1.
        if count==num_letters:
            words_ok +=1.
    print 'words:', words_ok, (100.*words_ok/tot_words),'%'
    print 'letters:', chars_ok, (100.*chars_ok/tot_chars),'%'
    pass

score()
```

And these are the results:

|---------------|-----------|----|
||||
|:--------------|:---------:|:--:|
|---------------|-----------|----|
|Accuracy (%) 	|Words 		|Letters|
||||
|Only Singleton	|22.2		|76.8|
|+ Pairwise 	|26.3 		|79.2|
|+ Triplet 		|34.3 		|80.1|
|+ Similarity 	|37.4 		|81.7|
|---------------|-----------|----|
{:.widetable}
<br/>

Not amazing but not bad for an easily extendable model. Just add factors between variables, watching out to not make too many connections among distant variables.
