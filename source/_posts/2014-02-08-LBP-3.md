---
layout: post
title: "Loopy Belief Propagation (III)"
date: 2014-02-8 08:01
comments: true
categories: PGN, theory, inference
---

These are different approaches to improve convergence and accuracy of the marginals.

##1. Smoothing Messages

Very easy to implement. When we compute every new message, instead of updating the delta, we update with smoothing: ```coef*(new delta) + (1-coef)*(prev stored delta)```. This slows convergence in one hand but also avoids big jumps in the beliefs, giving time to achieve convergence at a more controlled pace. Trying for different coeficients I did not improve the marginals.

##2. Tree Reparametrization

Also easy to implement. Instead of using BFS for message scheduling of all nodes, we find a set of trees in the graph so all edges are covered. It is better if we have spanning trees (as long as possible). Then we propagate like with Clique Trees, only twice, forward and backward. The easiest way to get trees from a graph is using Depth First Search (DFS). Still, I don't know how to get the minimum set of trees that encompass the whole graph, and I had to do it by hand :( Here is an example of 2 trees that span the grid:

{% img center /images/feb14/TRP.png 500 Trees for LBP%}


```python DFS
class DFS_Paths:
    def __init__(self, G, s):
        self.G = G
        self.source = s
        self.discoveryPath = [] # only the forward path, add backwards
        V = self.G.V
        self.marked = [False]*V
        self.edgeTo = [None]*V
        self.dfs(self.source) # start only at designated root
        pass
    
    def dfs(self, v):
        self.marked[v] = True
        for w in self.G.adj[v]:
            if self.marked[w]==False:
                self.discoveryPath.append([v,w])
                self.dfs(w)
                self.edgeTo[w] = v
        pass
    
    def pathTo(self, v):
        if self.marked[v]==False:
            return None
        else:
            path = [v]
            x = v
            while x != self.source: 
                x = self.edgeTo[x]
                path.insert(0,x)
            return path
```

When I run on indiscriminate sets of trees I converge to the bad marginals. But if I use two trees, one long spanning DFS tree plus a manually computed one for the missed edges, I get somewhat better results (sqr error from 0.99 to 0.75):

|var| exact marginal | approx. marg (LBP)|
|:-:|---------------:|---------------:|
||||
|0 | 0.537310955208 | 0.723905954527|
|1 | 0.556558936419 | 0.728300716867|
|2 | 0.552439017812 | 0.592195736634|
|3 | 0.573509953445 | 0.835859875384|
|4 | 0.6            | 0.911873353528|
|5 | 0.607090867757 | 0.900624901637|
|6 | 0.611810074317 | 0.8791687735  |
|7 | 0.621715017707 | 0.927292899198|
|8 | 0.626241528067 | 0.905241175589|
{:.widetable}
<br/>


##3. Residual Belief Propagation

This last effort is based on the fact that not all messgaes have the same importance. Some change the beliefs a great deal, while others are less important. It is easy to look at the way messages achieve convergence, andd see that not all reach equilibrium at the same time. The same happens with the betas.

What I did was implement a priority queue, and as compute each message I store it in the queue with a key that measures how much the message changed. Since all messages are univariate (check cluster graph and realize that there are only pairwise potentials), each message has two values (p, 1-p). The measure I use is the abs of the difference in p. The first pass (BFS) goes through all edges and computes all messages, but starting with the second pass, we compute only the messages that change beyond a tolerance and ordered so the more important run first. 

```python Calibrate with RBP
def calibrate(self):
    self.beta = [None]*self.V

    path = self.computePath()
    q = PriorityQueue()

    keep_going = True
    cycles, messages = 0, 0
    while keep_going:
        cycles +=1
        keep_going = False
        for e in path:
            from_v, to_w = e
            pos_to = self.adj[from_v].index(to_w)
            messages +=1
            prev_res = self.delta[from_v][pos_to].values
            self.delta[from_v][pos_to] = self.mssg(from_v, to_w)
            new_res = self.delta[from_v][pos_to].values
            
            mag = abs(new_res[0] - prev_res[0])/prev_res[0]
            if not np.allclose(new_res, prev_res, 1e-6):
                q.put({abs(mag):[from_v, to_w]})
                keep_going = True
            
        if cycles>1000:
            ...
        
        path = []
        while not q.empty():
            a = q.get()
            path.append(a.values()[0])
        path.reverse() # because it is a minQ
                    
    print 'Cycles:', cycles, 'Messages passed', messages
    # compute the beliefs
    ...
```

This way we only need 3 cycles and 104 messages. The first cycle goes through all 48 edges. the second also computes 48 messages but in a different order (prioritized by heuristic importance), and the third only comutes 8 messages. The results are even better:

|var| exact marginal | approx. marg (LBP)|
|:-:|---------------:|---------------:|
||||
|0 | 0.537310955208 | 0.614445509854|
|1 | 0.556558936419 | 0.632046274666|
|2 | 0.552439017812 | 0.592793285825|
|3 | 0.573509953445 | 0.699814842287|
|4 | 0.6            | 0.759645004711|
|5 | 0.607090867757 | 0.749109157114|
|6 | 0.611810074317 | 0.739850591712|
|7 | 0.621715017707 | 0.822523867844|
|8 | 0.626241528067 | 0.848549594174|
{:.widetable}
<br/>

Square distance error is 0.42. If instead of using the BSF as the initial path, we compute all messages and create an initial path based on a priority queue as before, the error can be taken further down to 0.33.

|var| exact marginal | approx. marg (LBP)|
|:-:|---------------:|---------------:|
||||
|0 | 0.537310955208 | 0.399020732163|
|1 | 0.556558936419 | 0.378280542986|
|2 | 0.552439017812 | 0.443100330486|
|3 | 0.573509953445 | 0.465753424658|
|4 | 0.6            | 0.641605971227|
|5 | 0.607090867757 | 0.662337662338|
|6 | 0.611810074317 | 0.641605971227|
|7 | 0.621715017707 | 0.770344431088|
|8 | 0.626241528067 | 0.719502074689|
{:.widetable}
<br/>

And finally, if instead of prioritizing the queue based on the importance of the messages we do it based on how each new message changes the marginalized belief of the receiving node we lower the error further to 0.24, although we need to cycle 13 times and pass 296 messages.

|var| exact marginal | approx. marg (LBP)|
|:-:|---------------:|---------------:|
||||
|0 | 0.537310955208 | 0.444436529385|
|1 | 0.556558936419 | 0.605683356431|
|2 | 0.552439017812 | 0.494869086601|
|3 | 0.573509953445 | 0.562421013105|
|4 | 0.6            | 0.692305538097|
|5 | 0.607090867757 | 0.64302214095|
|6 | 0.611810074317 | 0.682604471501|
|7 | 0.621715017707 | 0.745950622502|
|8 | 0.626241528067 | 0.748513253977|
{:.widetable}
<br/>

```python Code to compute the magnitude based on belief change
def magnitude(self, from_v, to_w):
    beta = copy.copy(self.factors[to_w])
    past_beta = copy.copy(beta)
        
    affected_var = [e for e in beta.variables if e not in self.box[from_v]]
    for x in self.adj[to_w]:
        posX = self.adj[x].index(to_w)
        delta = self.delta[x][posX]
        beta = FactorOperations.multiply(beta, delta, True) #### NORMALIZING
        
    if affected_var!=[]:
        past_beta = FactorOperations.marginalize(past_beta, affected_var[0])
        beta = FactorOperations.marginalize(beta, affected_var[0])            
    mag = abs(beta.values[0]-past_beta.values[0])/past_beta.values[0]
    return mag
```

Nevertheless, although it gives good results for a 9 node Ising grid, the more the nodes the worse it works. The RBP is the best approach found and I am sure that with a better designed key for the queue (to show which messages should be prioritized) bigger grids can also be optimized.






