<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: OCR | SotoSeattle]]></title>
  <link href="http://sotoseattle.github.io/blog/categories/ocr/atom.xml" rel="self"/>
  <link href="http://sotoseattle.github.io/"/>
  <updated>2017-03-11T16:35:56-08:00</updated>
  <id>http://sotoseattle.github.io/</id>
  <author>
    <name><![CDATA[Javier Soto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OCR Words (I)]]></title>
    <link href="http://sotoseattle.github.io/blog/2013/11/16/OCR-Words/"/>
    <updated>2013-11-16T08:01:00-08:00</updated>
    <id>http://sotoseattle.github.io/blog/2013/11/16/OCR-Words</id>
    <content type="html"><![CDATA[<p>We are starting the PGN assignment for week 3, with the goal in mind of re-coding it in python.</p>

<p>The Assignment is to construct a character recognition model based on a Markov network where certain variables are always ‘observed’ (the scanned images) and we work on hidden variables. In this manner we compute probabilities of a set of target hidden variables based on a different set of observed ones (characteristics or features). These markov nets are also called Conditional Random Fields and are an extremely powerful tool for all kinds of applications, from face recognition to Chinese parsers. We will build the model from the ground up and start with singleton factors.</p>

<h2 id="singleton-factors">singleton factors</h2>

<p><img class="center" src="/images/nov13/ocr_singleton.png"></p>

<p>The blue nodes are random variables that represent the scanned image of a handwritten single letter. These variables are always observed. The image is 16 by 8 pixels for a total of 128 pixels (0,1) per image. As a random variable it can take different assignments in those 128 positions, so the number of possible images is huge.</p>

<p>The red nodes are variables that represent the letter we are after, the letter represented in the image. There is a connection between each blue and red nodes because the probability of the red node being a certain letter is conditioned on what the image looks like. There are 26 possible assignments of each red node, the 26 letters of the alphabet in lower case.</p>

<p>There is a factor that tells us the probability of the red node being a particular letter for each possible image. Since the variability and dimensions of all possible images is huge we create a function that, given an image, computes the probabilities of the character being each letter. This is a singleton factor because for all purposes it only depends on one variable, the red node itself. </p>

<p>Then, the model can be said that operates on the red nodes as random variables alone, since the blue ones are already observed and therefore conditioned/observed and not variable any more.</p>

<p>We use Softmax regression (theory and code seen in October), for which the parameters have already being precomputed (so we only need to load them), and the code directly transplanted from our Softmax code.</p>

<p>```python singleton factors
data = [float(line.strip()) for line in open(‘examples/letterOCR/data/softmax_params_singleton.csv’)]
theta = np.array(data[0:3200]).reshape(128, 25) # 128 x 25
bias = np.array(data[3200:]).reshape(1,-1)      # 1   x 25
theta = np.vstack([bias, theta])</p>

<p>def computeImageFactor(img, thetas):
    ‘'’we use softmax regression h() method’’’
    x = np.hstack([1., np.array(img)]) # 1. for the bias param
    H = np.append(x.dot(thetas), 0.)   # the 0 is to add the last letter
    H = H - np.amax(H)
    H = np.exp(H)
    suma = np.sum(H)
    H = np.divide(H, suma)
    return H</p>

<p>def singletonFactor(var_char, img):
    f = Factor.Factor([var_char])
    f.values = computeImageFactor(img, theta)
    return f
```
For the image bellow we compute its image factor as an array of probabilities that add up to 1.0. Check that, in this case, the softmax regression works pretty well and assigns a probability of 92% to the image being a ‘c’, the third letter of the alphabet and the third cell in the factor array.</p>

<p><img class="left" src="/images/nov13/letter_c.png"></p>

<p>```python
Factor(red_node | blue_node = scanned_image) = </p>

<pre><code>  [  5.51526624e-04   2.51680889e-05   9.19861547e-01   3.84932931e-07
     5.27830262e-02   5.84130106e-05   9.81360425e-04   7.36016497e-04
     4.03551392e-08   2.11407006e-05   2.03932346e-04   3.93086506e-03
     9.96780992e-11   9.17523169e-04   1.58037104e-03   5.37762710e-08
     1.67113194e-05   4.04578301e-03   1.28410142e-03   9.86783809e-03
     5.20212172e-04   4.19693268e-04   8.62777153e-15   2.63680673e-07
     1.24025217e-04   2.07000202e-03] ```
</code></pre>

<p>This primitive model, when run against a set of 99 words and 691 handwritten characters, gives us a character accuracy of 76.85% and 22% probability of getting the whole word right (all letters right in each word).</p>

<h2 id="pairwise-and-triplet-factors">pairwise and triplet factors</h2>

<p><img class="center" src="/images/nov13/ocr_triplet.png"></p>

<p>Pairwise: Certain letters are followed by certain others. ‘cr’ is more probable than ‘cz’. We can easily compute these probabilities from a big enough corpus into a 26 x 26 matrix and add a factor that pairs two nodes and gives the probabilities of each pair of assignments. Consider that this factor’s values are the same for each couple of adjacent random variables. There would be n-1 factors per n letter word.</p>

<p>Triplets: Something similar could be done for groups of three consecutive letters (‘ing’ being more probable than ‘nnn’). The problem being that it results in a matrix of more than 17,000 assignments. Instead, we focus on the top 2,000 most probable triplets for which we compute the probabilities, we normalize all values so the computed 2,000th most probable’s probability is 1.0, and make the rest equal to 1.0 too (in effect saying that after the top 2000, all 3 letter groupings are equally probable). There would be n-2 triplet factors per n letter word. Consider that this factor’s values are the same for each triplet of adjacent random variables.</p>

<p>```python pairwise and triplet factors
# loading precomputed probabilities
file = open(‘examples/letterOCR/data/pairwiseModel.csv’)
prob_pairs = np.array(file.readline().split(‘,’)).astype(‘float64’)
prob_pairs = np.reshape(prob_pairs, (26,26), order=’F’)</p>

<p>file = open(‘examples/letterOCR/data/tripletModel.csv’)
prob_triplets = np.ones((26,26,26))
for i in range(2000):
    [a,b,c, p] = file.readline().split(‘,’)
    a,b,c = int(a), int(b), int(c)
    prob_triplets[a,b,c] = float(p)</p>

<h1 id="building-factors">building factors</h1>
<p>def pairwiseFactor(var1, var2):
    f = Factor.Factor([var1, var2])
    f.values = prob_pairs
    return f</p>

<p>def tripletFactor(var1, var2, var3):
    f = Factor.Factor([var1, var2, var3])
    f.values = prob_triplets
    return f
```</p>

<p>With these factors we can start inferring, but before we connect the factors into a model we are going to need an inference engine that allows us to efficiently compute the MAP. So before continuing the assignment we are going to take a detour for a couple of posts and build an inference engine that we can use for this and other models.</p>
]]></content>
  </entry>
  
</feed>
