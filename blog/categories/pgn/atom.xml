<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: PGN | SotoSeattle]]></title>
  <link href="http://sotoseattle.github.io/blog/categories/pgn/atom.xml" rel="self"/>
  <link href="http://sotoseattle.github.io/"/>
  <updated>2016-11-21T12:06:10-08:00</updated>
  <id>http://sotoseattle.github.io/</id>
  <author>
    <name><![CDATA[Javier Soto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[MCMC MH Swendsen Wang v.2.]]></title>
    <link href="http://sotoseattle.github.io/blog/2014/02/16/MCC-MH-SW2/"/>
    <updated>2014-02-16T08:01:00-08:00</updated>
    <id>http://sotoseattle.github.io/blog/2014/02/16/MCC-MH-SW2</id>
    <content type="html"><![CDATA[<p>In variant II we change the way we compute Q and R.</p>

<h2 id="the-q-continuum">The Q Continuum</h2>

<p>Q, the transition probability, now depends on the probabilities of the factors on each edge. If I understand it all correctly, it is defined as a ratio between the sum of probabilities of both nodes in the edge maintaining its old assignment, to the the sum of probabilities of all possible assignment for both nodes. Remember that in variant I this was uniformly set to 0.5.</p>

<script type="math/tex; mode=display">
q_{ij} := \frac{\sum_u{\Phi_{i,j}(u,u)}}{\sum_{u,v}{\Phi_{i,j}(u,v)}}
</script>

<p>The way we compute each <script type="math/tex">q_{ij}</script> is not complicated. Beware that we are assuming there is a single factor in each edge.</p>

<p><code>python Q variant II
def qij_var_2(self, i, j):
    x,y = self.G.v[i], self.G.v[j]
    fuedge = [fu for fu in self.G.factors if (x in fu.variables) and (y in fu.variables)][0]
    num, den = 0., 0.
    for ass_1 in range(x.totCard()):
        for ass_2 in range(y.totCard()):
            value = FactorOperations.reduce_to_value(fuedge, {x:ass_1, y:ass_2})
            den += value
            if ass_1==ass_2:
                num += value
    return num/den
</code></p>

<h2 id="r-not-as-simple-as-it-seems">R, not as simple as it seems</h2>

<p>R will be the logDistAss probability that we computed for Gibbs, but now instead of observing one var at a time, we observe a group. It has taken me my sweet time to figure out how to do it (based on what I already did in PA5 but more out of luck than reasoning).</p>

<p><code>python R is Simple
def MH_Swendsen_Wang(self, from_ass):
	...
    elif self.variant==2:
        logR = self.logDistAss([self.G.v[i] for i in Y_v_id], from_ass)
    ...
</code></p>

<p>The key for LogDistAss is that when we are looking at extracting from a factor the probability of a variable given that all the other variables in the factor have an assignment, like in the Bigss sampling code, it is very fast and we already have utility method that does that.</p>

<p>When instead of one, we have a set of variables, we need to reduce the factor by the assigned variables and then marginalize by the rest up to obtaining the distribution. The following code is messy, my understanding of it is not completly firm, and it should be refactored, which I leave for a future time.</p>

<p>```python LogProbability of Partial Assignment Not So Easy
def logDistAss(self, sample_vars, evidence):
    cardio = set([e.totCard() for e in sample_vars])
    if len(cardio)&gt;1:
        raise Exception(“All sampling vars must have the same cardinality”)
    logbp = [0.]*cardio.pop()</p>

<pre><code>all_f = list(set(fu for a in sample_vars for fu in self.var_to_factors[a]))
for fu in all_f:
    vs = [x for x in fu.variables if x in sample_vars]
    
    evi, mar = {}, []
    for k in fu.variables:
        if k not in vs:
            evi[k] = evidence[k]
        else:
            mar.append(k)
            
    if len(vs)==1:
        dist = FactorOperations.reduce_to_dist(fu, evi)
    else:
        gg = FactorOperations.observe(fu, evi, False)
        for x in mar:
            if len(gg.variables)&gt;1:
                gg = FactorOperations.marginalize(gg, x)
        dist = gg.values
        
logbp += np.log(dist)

logbp = logbp - min(logbp) # to avoid underflow when back in normal space
return logbp ```
</code></pre>

<h2 id="analysis">Analysis</h2>

<p>Following the same procedures and initial inputs as we did for Gibbs, MH Uniform and SW-I we produce the following chart for comparison purposes.</p>

<p>Mixing time:</p>

<p><img class="center" src="/images/feb14/MHSW_2_mix.png" width="500" title="Mixing Windows Run 1" ></p>

<p>Sample Size:</p>

<p><img class="center" src="/images/feb14/MHSW_2_size.png" width="500" title="Sample Size Run 1" ></p>

<p>Marginals Convergence for the extreme case [0.95, 0.05]:</p>

<p><img class="center" src="/images/feb14/MHSW_2_cross.png" width="500"></p>

<p>Histogram of dist to true marginal based on 100 runs on 16 node Ising:</p>

<p><img class="center" src="/images/feb14/MHSW_2_hist.png" width="500"></p>

<p>Amazing. An Average error of 0.04 0with standard deviation of 0.01.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MCMC MH Swendsen Wang]]></title>
    <link href="http://sotoseattle.github.io/blog/2014/02/15/MCMC-MH-SW/"/>
    <updated>2014-02-15T08:01:00-08:00</updated>
    <id>http://sotoseattle.github.io/blog/2014/02/15/MCMC-MH-SW</id>
    <content type="html"><![CDATA[<p>We continue with Assignment 5 from Prof. Koller’s Coursera course: “Probabilistic Graphical Models”. I am including the explanation from the assignment paper because it is the most clear explanation I have found for the Swendsen Wang sampling method, which is a hair rasing trip in on itself.</p>

<h2 id="theory">Theory</h2>

<p>In Ising models and image segmentation applications, pairwise Markov networks are frequently used. This means specially for strong potentials adjacent variables tend to take on the same values. This makes it hard to explore the space for possible assignments by changing the value of one variable at a time, like Gibbs does. The Swendsen-Wang algorithm has been specifically designed for this kind of networks and overcomes this difficulty by changing, at the same time, a group of adjacent variables that have the same value.</p>

<p>In Swendsen-Wang, we start with a model that has variables connected in pairwise factors in a Markov graph. All the variables can take on the same values. To do MCMC, we start as usual in some state x. To get to the next state, we start with the graph. We break all the edges between variables that have different values. Then, we also break the rest of the edges between any nodes i and j with probability <script type="math/tex">(1−q_{i,j})</script>, where <script type="math/tex">q_{i,j}</script> is a probability that can depend on i and j but not their current assignment. </p>

<p>Once we’ve broken all those edges, we’re left with a graph with connected groups of variables, with all variables in a group sharing the same assignment. We randomly pick one with a uniform distribution. That’s Y . We choose a new assignment, using a probability distribution R. With the new assignment to the variables in Y , we now have the new state x’; in other words, the new assignment x’ is the same as x, except that the variables in Y have changed value. As with all MH, we accept the state with a probability A.</p>

<p><img class="center" src="/images/feb14/SW_procedure.png" width="500" title="Swendsen Wang Procedure 1" ></p>

<p>To compute Q, R, and A, we use the following equations. Let C(Y|x) be the probability that a set Y is selected to be updated using this procedure, given the current state is x. Then we somehow divine Q(x → x’), the chance that we’ll try a transition from x to x’ by changing Y , as the following:</p>

<script type="math/tex; mode=display">
Q(x \rightarrow x') = C(Y|x) R(Y = l'|x_Y )
</script>

<p>So we can compute the ratio for A (acceptance probability):</p>

<script type="math/tex; mode=display">
\frac{Q(x' \rightarrow x)}{Q(x \rightarrow x')} =\frac{C(Y|x') R(Y = l|x'_Y )}{C(Y|x) R(Y = l'|x_Y )}
</script>

<p>And we realize that obviously the following is a truism:</p>

<script type="math/tex; mode=display">
\frac{C(Y|x')}{C(Y|x)} = \frac{\prod_{(i,j) \in \epsilon (Y, X_{l'}^{'}-Y)}(1 - q_{i,j})}{\prod_{(i,j) \in \epsilon (Y, X_{l}-Y)}(1 - q_{i,j})}
</script>

<p>Where: <script type="math/tex">X_l</script> is the set of vertices with label l in x, <script type="math/tex">X'_{l'}</script> is the set of vertices with label l’ in x’; and where <script type="math/tex">\epsilon(Y,Z)</script> (between two disjoint sets Y,Z) is the set of edges connecting nodes in Y to nodes in Z.</p>

<h2 id="sw-uniform-variant">SW Uniform variant</h2>

<p>Although the intuition behind breaking the graph and changing the assignment of a connected component makes sense, coming up with Q and R are beyond my grasp. Nevertheless, we compute SW in two variants. The first one is easy, it uses a Q that is uniform, so we set all the <script type="math/tex">q_{ij}</script> to 0.5 (our Ising net is all binomial). We also set R to be uniform. Nevertheless Let’s go step by step:</p>

<p>The first thing we have to take into consideration is to create a data structure in our MCMC_MHSW object that holds, for each edge between nodes i and j the value of <script type="math/tex">q_{ij}</script>. In our variant all Q are equal, being the uniform.</p>

<p><code>python Q by edges
self.q_list = [None]*(self.G.V)
for i in range(self.G.V):
    self.q_list[i] = {}
    for k in self.G.adj[i]:
        if self.variant==1:
            self.q_list[i][k] = 0.5
</code></p>

<p>Now we start processing. Given the graph and a complete assignment we remove the edges connecting variables that have different values. Also, we take the opportunity to break edges based on the probability Q.</p>

<p><code>python Breaking Edges Based on Assignment
def common_components_and_Q(self, ass):
    '''return connected edges between nodes that share assignment, and pass Q probability'''
    edges = [[]]*self.G.V
    for i,j in self.one_pass_edges:
        if ass[self.G.v[i]]==ass[self.G.v[j]] and random.uniform(0,1) &lt; self.q_list[i][j]:
            edges[i] = edges[i] + [j]
            edges[j] = edges[j] + [i]
    return edges
</code></p>

<p>With this new graph (the same as the original but with rearranged edges) we need is a way to find its connected components. That is easy with DFS (Depth First Search) and the Princeton Algorithm’s course from Coursera. Given a graph G we can find how many components there are, and to which one each variable belongs.</p>

<p>```python Connected Components
class CC(object):
    def <strong>init</strong>(self, G):
        self.G = G
        self.dni = [[]]<em>self.G.V
        self.marked = [False]</em>self.G.V
        self.count = 0
        self.size = [[]]*self.G.V
        for v in range(self.G.V):
            if self.marked[v]==False:
                self.dfs(v)
                self.count += 1</p>

<pre><code>def dfs(self, v):
    self.marked[v] = True
    self.dni[v] = self.count
    self.size[self.count] = self.size[self.count] + [1]
    for w in self.G.adj[v]:
        if (self.marked[w]==False):
            self.dfs(w) ```
</code></pre>

<p>The code for a method to return a new assignment begins to take form:</p>

<p>```python Finding a new Assignment
def MH_Swendsen_Wang(self, from_ass):
    # build new graph breaking edges where nodes differ in assignment and by Q
    G_prime = copy.copy(self.G)
    G_prime.adj = self.rearrange_edges(from_ass)</p>

<pre><code># get connected components
common_comps = CC(G_prime)
... ```
</code></pre>

<p>With the components identified we can select one (randomly), and retrieve all its variables.</p>

<p><code>python cont
def MH_Swendsen_Wang(self, from_ass):
    ...
    # select a common component randomly
    Y_index = random.randint(0,common_comps.count-1)
    Y_v_id = [i for i, x in enumerate(common_comps.dni) if x == Y_index]
    cardio = [self.G.v[x].totCard() for x in Y_v_id] 
    assert(len(unique(cardio))==1) # check all have same ass and cardinality
    d = cardio[0]
    ...
</code></p>

<p>We compute the R (actually the log of R for underflows). In our case of first variant and given that all variables are bianry, this  uniform distribution R becomes <code>[log(0.5), log(0.5)]</code>. The old value of Y can be taken from any variable of Y (we choose whichever is first), and the new assignment is randomly extracted from the R distribution. The new complete assignment becomes then the same as the original except for the variables in Y, which take now on the new value derived from R.</p>

<p><code>python cont
def MH_Swendsen_Wang(self, from_ass):
    ...
    logR = [math.log(1./d)]*d
    old_value = from_ass[self.G.v[Y_v_id[0]]]
    new_value = self.randSampleDist(d, np.exp(logR))
    to_ass = dict((k,new_value) if k.id in Y_v_id else (k, from_ass[k]) for k in from_ass)
    ...
</code></p>

<p>Now comes the part of the messy equation and the epsilons. We need to compute the ratio of the products of Qs for all edges. We do it in log space with sums.</p>

<p><code>python cont
def MH_Swendsen_Wang(self, from_ass):
    ...
    # log-ratio of the prob of picking Y given we transition from_ass to_ass
    # we need to iterate over all edges
    log_QY_ratio = 0.0
    for i,j in self.one_pass_edges:
        if (i in Y_v_id and j not in Y_v_id) or (i not in Y_v_id and j in Y_v_id):
            x,y = self.G.v[i], self.G.v[j]
            if from_ass[x] == old_value and from_ass[y] == old_value:
                log_QY_ratio = log_QY_ratio - log(1 - self.q_list[i][j])
            if to_ass[x] == new_value and to_ass[y] == new_value:
                log_QY_ratio = log_QY_ratio + log(1 - self.q_list[i][j])
    ...
</code></p>

<p>Finally, we have everything to compute A. We compute the stationary probabilities as we did with MH Uniform, easy. Then the acceptance probability just follows the theoretical formula given before. We check randomly if pass or not the acceptance threshold, and if we do we send the new complete assignment as the new sample (if not , we stay put).</p>

<p>```python cont
def MH_Swendsen_Wang(self, from_ass):
    … 
    acceptance, from_pi, to_pi = 0.0, 0.0, 0.0
    for fu in self.G.factors:
        from_pi += math.log(FactorOperations.reduce_to_value(fu, from_ass))
        to_pi += math.log(FactorOperations.reduce_to_value(fu, to_ass))
    acceptance = (math.exp(to_pi)/math.exp(from_pi))<em>math.exp(log_QY_ratio)</em>\
                      math.exp(logR[old_value])/math.exp(logR[new_value])</p>

<pre><code>if random.uniform(0,1) &lt; acceptance:
    from_ass = to_ass
    
return from_ass ```
</code></pre>

<h2 id="analysis">Analysis</h2>

<p>Following the same procedures and initial inputs as we did for Gibbs and MH Uniform we produce the following chart for comparison purposes. All pretty impressive.</p>

<p>Mixing time:</p>

<p><img class="center" src="/images/feb14/MHSW_1_mix.png" width="500" title="Mixing Windows Run 1" ></p>

<p>Sample Size:</p>

<p><img class="center" src="/images/feb14/MHSW_1_size.png" width="500" title="Sample Size Run 1" ></p>

<p>Marginals Convergence for the extreme case [0.95, 0.05]:</p>

<p><img class="center" src="/images/feb14/MHSW_1_cross.png" width="500"></p>

<p>Histogram of dist to true marginal based on 100 runs on 16 node Ising:</p>

<p><img class="center" src="/images/feb14/MHSW_1_hist.png" width="500"></p>

<p>Although the hsitogram runs over 70,000 is overkill (WS arrives at the stationary distribution way earlier), the results are impressive. An Average error of 0.04 with standard deviation of 0.02. All very impressive. Cannot wait to see if the Swendsen Wang variant 2 achieves even better results!</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Factors Vectorized]]></title>
    <link href="http://sotoseattle.github.io/blog/2013/11/13/Factors-Vectorized/"/>
    <updated>2013-11-13T08:01:00-08:00</updated>
    <id>http://sotoseattle.github.io/blog/2013/11/13/Factors-Vectorized</id>
    <content type="html"><![CDATA[<p>I have vectorized the Factor implementation, getting rid of loops and using as many ndarray operations as possible. To begin with we have a basic Factor class that still holds variables, cardinalities and values. The main difference is that instead of a 1D array for the values we have an ndarray with as many dimensions as variables.</p>

<p>```python Factor objects
class Factor(object):
    ‘'’Factor : CPD Conditional Probability (discrete) Distribution
      var    Vector of variables in the factor, e.g. [1 2 3] =&gt; X1, X2, X3. always ordered by id
      card   Vector of cardinalities of var, e.g. [2 2 2] =&gt; all binomials
      val    Value table with the conditional probabilities. n dimensional array
      size   Length of val array = prod(card)’’’</p>

<pre><code>def __init__(self, variables):
    self.variables = variables
    self.cards = [c.totCard() for c in self.variables]
    self.values = np.zeros(self.cards).astype('float64')

def idx2ass(self, arr):
    '''mapping between all combinatorial ordered assignments and a flatten vector of values'''
    t = [[e]*len(self.cards) for e in range(len(arr))]
    order, dic = [], {}
    for q, row in enumerate(t):
        prod, ass = 1, []
        for i, e in enumerate([1] + self.cards[:-1]):
            prod *= e
            ass.append((row[i]/prod) % self.cards[i])
        key = tuple(ass)
        dic[key] = arr[q]
        order += [key]
    return [order, dic]

def fill_values(self, arr):
    order, dic = self.idx2ass(arr)
    for e in order:
        self.values[e] = dic[e] ```
</code></pre>

<p>The idx2ass is a holdout from the previous version that helps us understand the assignment process, allows us to fill the values with a 1D array and, is useful when comparing with the Octave implementation for debugging purposes. This method returns two sets that can be used in many ways as idx2ass and ass2idx. Much of this code can be simplified and cleaned further.</p>

<h2 id="marginalization">Marginalization</h2>

<p>The key advantage of using an ndarray with variables mapped to axes (dimensions) is that marginalize becomes a trivial operation. We only need to sum up all the values of the marginalized variable.</p>

<p><code>python Factor Reduction
def marginalize(fA, v):
    new_vs = [e for e in fA.variables if e != v ]
    if new_vs==[]:
        raise Exception("Error: Resultant factor has empty scope")
    f = Factor.Factor(new_vs)
    pos_m = fA.variables.index(v)
    f.values = np.sum(fA.values, axis=pos_m)
    return f
</code>
For example, for a factor with two random variables (v1, v2), reducing on v2 means selecting the axis for v2 and for each row of v1, adding up all columns of v2.</p>

<p><img class="center" src="/images/nov13/margin.png"></p>

<h2 id="conditioning">Conditioning</h2>

<p>Also trivial, we modify the ndarray values by </p>

<ul>
  <li>focusing on the observed variable axis and leaving all other axis untouched</li>
  <li>for the selected axis, setting to 0. all cells that are not in the observation column</li>
</ul>

<p><code>python Conditioning on evidence
def observe(fA, evidence, norma=True):
    f = Factor.Factor(fA.variables)
    M = fA.values.copy()
    for cond_var in evidence.keys():
        if cond_var in fA.variables:
            a = []
            for v in fA.variables:
                if v.id == cond_var.id:
                    a += [[x for x in range(v.totCard()) if x!=evidence[cond_var]]]
                else:
                    a += [slice(None)]
            M[a]=0.
    f.values = normalize(M) if norma else M
    return f
</code></p>

<p>Conditioning renormalizes at the end to ensure proper probabilities. </p>

<h2 id="multiplication">Multiplication</h2>

<p>The difficult one. Given two factors I modify each one by:</p>

<ul>
  <li>adding all the variables of the resulting multiplication factor (Union of all variables, sorted by id)</li>
  <li>each added variable inserts a new axis in the right order</li>
  <li>we expand the values ndarray on each new axis by repeating a number of times equal to the cardinality of the axi’s variable.</li>
</ul>

<p>Continuing with the graphic example, to expand our previous factor (variables v1 and v2) by another variable (v3) we would start with the 2D values along axes v1, v2. Then we add a third dimension for v3.</p>

<p><img class="center" src="/images/nov13/multiply1.png"></p>

<p>And then we repeat the 2D matrix (v1,v2) along the v3 axis. In our case v1 and v2 have cardinality 2 and v3 has cardinality 3 so we repeat the 2D matrix twice more along the v3 axis.</p>

<p><img class="center" src="/images/nov13/multiply2.png"></p>

<p>At the end of the process we have two ndarrays that represent the same variables, aligned and of the same shape. To multiply we only need to multiply elementwise. Convince yourself with the following example:</p>

<p>```text basic example
factor<em>1(v1) with values [1,2] =&gt; expand to [[1,1][2,2]] inserting v2 and resulting in (v1, v2)
factor</em>2(v2) with values [3,4] =&gt; expand to [[3,4][3,4]] inserting v1 and resulting in (v1, v2)</p>

<p>factor<em>1_expanded x factor</em>2_expanded = factor_product
[1, 1]            x [3, 4]            = [3, 4]
[2, 2]              [3, 4]              [6, 8]
```</p>

<p>We end by renormalizing to ensure proper probabilities.</p>

<p>```python Factor product
def realign(M, chaos):
    ‘'’realign by swapping its axes’’’
    order = range(len(chaos))
    for i in order:
        have_here = chaos[i]
        should_be = order[i]
        index_of_missing_should = chaos.index(should_be)
        if have_here != should_be:
            M = np.swapaxes(M, i, index_of_missing_should)
            chaos[index_of_missing_should] =  have_here
            chaos[i] = should_be
    return M</p>

<p>def new_axes(whole, partial):
    ‘'’list of new axis needed for partial to become whole’’’
    sol = []
    j = 0
    for e in whole:
        if e in partial:
            sol += [j]
            j += 1
        else:
            sol += [np.newaxis]
    return sol</p>

<p>def expand_rvars(whole, fA):
    ‘'’expand a factor to new dimensions by inserting new axes and repeating values along them’’’
    f = Factor.Factor(whole)</p>

<pre><code>chorizo = fA.values.copy()
order = sorted(fA.variables)
qt = [order.index(e) for e in fA.variables] # [0,1,3,2] indexes of fA.vars vs order [0,1,2,3]
if qt!=range(len(qt)):                      # if not equal, we need to realign before proceeding
    chorizo = realign(chorizo, qt)

sol = new_axes(whole, order)                # see which new axis we need
for i,e in enumerate(sol):
    if e==None:                             # insert new axes and fill values
        chorizo = np.expand_dims(chorizo, axis=i)
        chorizo = np.repeat(chorizo, whole[i].totCard(), axis=i)    
f.values = chorizo
return f
</code></pre>

<p>def multiply(fA, fB, norma=True):
    ‘'’expanding factors to the whole set of variables, then we can multiply element-wise’’’
    allVars = sorted(list(set(fA.variables) | set(fB.variables)))
    f = Factor.Factor(allVars)
    FA = expand_rvars(allVars, fA)
    FB = expand_rvars(allVars, fB)
    f.values = FA.values * FB.values
    if norma:
        f.values = normalize(f.values)
    return f
```</p>

<p>As always this code is a first pass that needs cleaning and refactoring (most probably I can still simplify the subprocesses for the expansion of factors).</p>

]]></content>
  </entry>
  
</feed>
