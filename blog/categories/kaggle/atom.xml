<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: kaggle, | SotoSeattle]]></title>
  <link href="http://sotoseattle.github.io/blog/categories/kaggle/atom.xml" rel="self"/>
  <link href="http://sotoseattle.github.io/"/>
  <updated>2016-12-13T10:46:51-08:00</updated>
  <id>http://sotoseattle.github.io/</id>
  <author>
    <name><![CDATA[Javier Soto]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Data Science London]]></title>
    <link href="http://sotoseattle.github.io/blog/2013/10/28/Data-Science-London/"/>
    <updated>2013-10-28T21:02:00-07:00</updated>
    <id>http://sotoseattle.github.io/blog/2013/10/28/Data-Science-London</id>
    <content type="html"><![CDATA[<p>This is a post about my third tutorial competition, Data Science London.</p>

<p>The training data consists of 1,000 records with 40 features. We need to classify each record as 1 or 0. The test set consists of 9,000 additional records.</p>

<p>After playing a bit with the training set one realizes that there is a ton of noise in the data, and that we need to carefully distill the features to get something of value. Furthermore, the noisy data is supported by the fact that the features were created artificially for the purpose of this tutorial competition.</p>

<h2 id="feature-selection">feature selection</h2>

<p>To start cleaning up the data a first approach is to see which features give the most predictive power. To do this I use an SVM with Gaussian kernel. Then I find the feature that gives the most predictive power. Then I repeat the process finding the feature that added to the previously selected achieves the highest accuracy. </p>

<p>Another way is to use the ExtraTreesClassifier from the scikit package which results in the following chart of the importance of features (<a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py">link</a>):</p>

<p><img class="center" src="/images/kaggle_scilondon/features_power_svm.png" width="600"></p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">SVM</td>
      <td style="text-align: right">14</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">29</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">36</td>
      <td style="text-align: right">39</td>
      <td style="text-align: right">7</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">28</td>
      <td style="text-align: right">19</td>
      <td style="text-align: right">22</td>
      <td style="text-align: right">18</td>
      <td style="text-align: right">38</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">32</td>
      <td style="text-align: right">34</td>
      <td style="text-align: right">13</td>
    </tr>
    <tr>
      <td style="text-align: left">TreesClassif</td>
      <td style="text-align: right">14</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">39</td>
      <td style="text-align: right">36</td>
      <td style="text-align: right">18</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">32</td>
      <td style="text-align: right">38</td>
      <td style="text-align: right">29</td>
      <td style="text-align: right">28</td>
      <td style="text-align: right">23</td>
      <td style="text-align: right">22</td>
      <td style="text-align: right">34</td>
      <td style="text-align: right">7</td>
      <td style="text-align: right">9</td>
      <td style="text-align: right">10</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<p>Only using the feature number 14 we already guess right more than 2/3 of the times. As we add features we increase the accuracy until it stabilizes somewhere between the first 10 and 22 features. Then, the added noise starts to degrade the model and lose predictive power. Optimizing hyper-parameters for the first 18 features we achieve 93.6% on cross validation and 91.84% on the 9,000 samples test set.</p>

<p>ALTERNATIVE WAYS TO CHOOSE FEATURES FROM SCIKIT</p>

<h2 id="dimensionality-reduction">dimensionality reduction</h2>

<p>Even after choosing the set of features we still have way too much noise in the data. One way to ‘clean it up’ is through dimensionality reduction. This has a good and a bad aspect. On one hand, it is a dangerous practice since what we do is project into a lower dimension space all the features and in the process we loose data, maybe valuable. On the other hand, it allows us to apply ‘whitening’, removing data points that are close by and highly correlated.</p>

<p>In this case it pays to apply PCA because I rather start with less valuable data but more salient, than more noisy and diffuse information. Here are two images with accuracy on the first 100 records of training an SVM on the subsequent 900 for different sets of features and dimensions. The left one comes from the main features derived from the tree classifier, the right most from the cycling svm:</p>

<p><img class="left" src="/images/kaggle_scilondon/feat_dim_trees.png" width="420" title="Features from trees" ></p>

<p><img class="right" src="/images/kaggle_scilondon/feat_dim_svm.png" width="420" title="Features from SVM" ></p>

<p><br /></p>

<p>My SVM set achieves a maximum accuracy of 97% for the first 14 features reduced to 12 dimensions. The forest one achieves a top 96% for a wider set of combinations (&gt; 12 features and 12-13 dimensions).</p>

<p>As an aside, I tried all kinds of different approaches available in the scikit package for dim reduction, except for the ‘kernel pca’ that took too long. The best result was achieved with Random PCA on my reduced set of 14 features and reduced to 12 dimensions. These 12 dimensions explain something like half of the variance when considering the whole feature set, and around 90% of the variance in the reduced feature set.</p>

<p><img class="center" src="/images/kaggle_scilondon/explained_var.png" width="500" title="Variance explained from dimensions" ></p>

<p>One final note. It is easy to code the PCA algorithm. Nevertheless I used the SciKit tool because it includes the Random PCA and whitening (which still escapes me). Here is the code for bare-bones PCA:</p>

<p>```python barebones PCA
def pca(X):
	m, n = X.shape
	U = np.zeros(n)
	S = np.zeros(n)
	Sigma = np.cov(X.T)
	print ‘Sigma shape should be nxn:’, Sigma.shape
	print ‘U will be m x ‘, np.min(Sigma.shape)
	U, S, V = np.linalg.svd(Sigma, full_matrices=False)
	return [U, S]</p>

<p>def shrink(example_norm, U, dimensions):
	Ureduce = U[:, 0:dimensions]
	Z = example_norm.dot(Ureduce)
	return Z</p>

<p>def inflate(shrunk_data, U, dimensions):
	Ureduce = U[:, 0:dimensions]
	X_rec = shrunk_data.dot(Ureduce.T)
	return X_rec
```</p>

<h2 id="engorging-the-training-by-guessing">engorging the training by guessing</h2>

<p>We have an inherent asymmetry: 1000 training records and 9000 for testing. If we could pass records from the testing to the training set we could boost the model’s power. The problem is that we don’t have the labels of the testing set records. But they are so many that we could run our model on the testing set and believe that the few predictions made with very high probability are so certain as to take them as ‘real’. </p>

<p>In an initial pass of the SVM over the testing set we select the predicted records with probability higher than 99% and add them to our training set. We measure the accuracy on a separated cross validation set and store the result. We repeat the process over and over and only stop when the accuracy on the CV set goes down. We end up with around 8,508 records, 2,316 support vectors, and an accuracy of around 97% on cross validation.</p>

<p>The intuition behind the idea is that although we don’t gain anything from certain added records (because we were already pretty sure where they fell), the addition to the training set will allow other less apparent features and subtleties gain contrast and predictive power.</p>

<p>At the end of this process we have a huge, if slightly screwed up training set that gives us the following accuracies against our cross validation set:</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">precision</th>
      <th style="text-align: center">recall</th>
      <th style="text-align: center">f1-score</th>
      <th style="text-align: right">support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">label 0</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">149</td>
    </tr>
    <tr>
      <td style="text-align: left">label 1</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">151</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: left">avg / total</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">300</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h2 id="experiment-with-nn">experiment with NN</h2>

<p>As an experiment, we train a neural network with quite an excess of nodes [input: 18, hidden_1: 36, hidden_2: 36, output: 2] on the engorged training set. Then for each prediction made by SVM and NN, in case the models disagree, whoever assigns the highest probability to his choice decides.</p>

<p>Run on the CV set we got the same result:</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">precision</th>
      <th style="text-align: center">recall</th>
      <th style="text-align: center">f1-score</th>
      <th style="text-align: right">support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">label 0</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">149</td>
    </tr>
    <tr>
      <td style="text-align: left">label 1</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">151</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: left">avg / total</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">300</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<p>Both models differed on just 10 samples (3%) and in 5 cases the NN decided. The NN didn’t do better than the SVM, it just had a different opinion on some borderline cases.</p>

<h2 id="final-submission">final submission</h2>

<p>I have rerun my algorithm multiple times trying to find the best initial feature set and best randomized path that engorges the data. My last few consistent scores were between 95.45 and 95.827.</p>

<p>My final submissions came down to: </p>

<ul>
  <li>a newly distilled feature set of 20 features, </li>
  <li>tweaked hyper-parameters,</li>
  <li>the patience to run the engorgement algorithm until I got 97.66% on CV</li>
  <li>8,808 training records guessed with p&gt;0.99 and 2,635 support vectors</li>
  <li>and using the mix model</li>
</ul>

<p>which achieves a 95.566% (a tad better than the SVM alone). </p>

<p>Two days back, out of chance with the same mix strategy on a randomly optimized set I got my best (though I suspect overfit) result:</p>

<p><blockquote><p>Kaggle’s position 9 with 95.827% accuracy.</p></blockquote></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Neurotic Titanic]]></title>
    <link href="http://sotoseattle.github.io/blog/2013/10/22/Neurotic-Titanic/"/>
    <updated>2013-10-22T21:02:00-07:00</updated>
    <id>http://sotoseattle.github.io/blog/2013/10/22/Neurotic-Titanic</id>
    <content type="html"><![CDATA[<p>I have spent the last 13th submissions to beat my previous position (77<sup>th</sup> with 81.81% accuracy) without result. I tried concocting and adding new features, combining them in weird ways, estimating the missing values to the highest accuracy possible. All to no avail.</p>

<p>I have tried not only screwing with the SVM, but Neural Networks with the code of previous posts. The Neural Nets work great but tend to overfit tremendously. I get 84-88% in CV only to get 75-80% on the Kaggle test set. The overfitting got me thinking about ways to simplify so the model could generalize better. This is what I have learn:</p>

<h2 id="less-features-are-better-than-more">less features are better than more</h2>

<p>From my previous like 10 features I have realized that we get the same results with less. Actually, the title level alone will give you 79% accuracy on the cross validation set. Then you can experiment adding new features to see how well you do and realize that in order of incremental power come: family (as Parch + SibSp), Fare paid, Pclass, overseer, charisma (binAge * Pclass), Port_Q (embarked on Queenssomething), ….</p>

<p>After the first 5 you don’t add much more and a bit further you star to add noise and lose generalization power. This means that things like Age, Sex or Embarked port are not even considered!</p>

<h2 id="simpler-beats-complex-sometimes">simpler beats complex (sometimes)</h2>

<p>I tried to simplify as much as possible the features. First I only use 5 (‘title_level’, ‘family’, ‘Fare’, ‘Pclass’, ‘overseer’). Then I tried to simplify them:</p>

<p>I started estimating the correct Age (as bin) with a SVM and then with a Neural Network. The SVM got 68% right on CV, and the NN north of 90%. The problem is that Age is not key, the title already incorporates all the age information that is relevant. So out it went.</p>

<p>The same with Sex. Title already incorporates the info. Adding Sex, in this case, only confuses things.</p>

<p>On title I aggregate it to just [‘mr’, ‘mrs’, ‘mss’, ‘master’, ‘rev’, ‘major’, ‘sir’, ‘col’, ‘capt’, ‘the countess’]. The less the merrier.</p>

<p>Family went from a sum of relatives to 3 classes, [traveling alone, 1 relative, more than 1 relative on board].</p>

<p>Overseer, was a bit more built up. essentially for each passenger I look for other relatives and companions that survived. For women, if the survivor was older give 10 points, if not get just 1. For men, if the survivor was a man older by 20 years give 10 points, if not get 1. Any other way get -1. The idea already explained that in a crisis we gather with who we know, and that the elder look after the younger (up to a point).</p>

<p>By the way, I have been trying avoid 0 values, in the suspicion (material or not) that zeros are traps from which is difficult to escape (when most we do is multiply stuff).</p>

<h2 id="svm-or-nn--why-not-both">SVM or NN =&gt; why not both?</h2>

<p>With the above simplifications either SVM or NN got stuck under 80%. There was no way to move the dial so I tried a crazy approach: run them both (with optimized hyperparameters) and then:</p>

<ul>
  <li>commit when both agree,</li>
  <li>when they disagree on the outcome, choose the one with the highest assigned probability (yes, SVM can estimate the probability too!)</li>
</ul>

<p>Stupidly simple, and with great results. With my latest iteration, number 26, I landed on the 57 spot of 7,035 participants (as of now).</p>

<p><blockquote><p>Score on the test set (Kaggle): 82.775%.</p></blockquote></p>

<p>The NN alone (submission 27) is a big network with three hidden layers and overfits the data to death (Kaggle 80.38%). The SVM alone does not get past 80% either. But both of them working together work wonders. I am sure more could be extracted from this approach, but I need to get going and I should try a new competition and learn a new model.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kaggle Titanic: Features (top 5)]]></title>
    <link href="http://sotoseattle.github.io/blog/2013/10/17/Titanic/"/>
    <updated>2013-10-17T18:02:00-07:00</updated>
    <id>http://sotoseattle.github.io/blog/2013/10/17/Titanic</id>
    <content type="html"><![CDATA[<p>I have spent the last few days in the <a href="http://www.kaggle.com/c/titanic-gettingStarted">Titanic competition</a>. It is another easy, entry level, tutorial example to get a grasp of how machine learning works. Again I didn’t use the random forest model as suggested and got lazy with the only two things I know, logistic regression and SVM.</p>

<p>I am not going to delve in the details of the data structure, suffice to say that we are given a part of the Titanic’s manifest with the survivorship rates. Our goal is for another set of passengers predict their survival.</p>

<p>I wanted to test the waters first so I used a very simple logistic regression with few variables: Sex, Pclass and Fare. That landed me in the bottom 15%, as usual, with a very low accuracy.</p>

<p>The main concept of the competition is the importance of the features we use. For example, it is obvious that Sex (gender) is an important feature (predictor), as it is Age (younger passengers have a better chance) and means (richer fare better than poor passengers). All this can be visualized through plots of different variables and their correlations.</p>

<p>So doing an SVM (Gaussian kernel) with the obvious candidates of Sex, Pclass, Fare gets us to a fair accuracy. If we add Age we don’t improve things, mainly because many entries are NaN. To solve it, I substituted those unknown ages with 24, the median age of passengers of 3rd class (which I assume to be ones least known). This raised the bar a bit more.</p>

<h2 id="beware-of-rates-and-sets">beware of rates and sets</h2>

<p>Consider that the data set is small, so depending on how you partition it between training and cross validation the accuracy on CV varies. I started seting up two sets:</p>

<ul>
  <li>training of the first 691 records, and the following 200 for cross validation.</li>
  <li>training of the last 691 records, and the previous 200 for cross validation.</li>
</ul>

<p>I achieved a higher accuracy when both were high (aprox. 85%) than when they were apart (86.5% and 82%). Furthermore, I always got much higher accuracies on the CV set than on the test set, so 85% translated into 80% in the test set.</p>

<p>Then I realized that I could use the whole training set and use the StratifiedKFold utility of scikit. Much easier way to go about it. </p>

<p><img class="center" src="/images/kaggle_titanic/hypeparams.png" width="600" title="hyperparams search" ></p>

<h2 id="python-pandas-manipulation">python pandas manipulation</h2>

<p>Some tips to remember as to how munge data in pandas:</p>

<p>```python dataframa manipulation
# basic mapping
data.Sex = data.Sex.map(lambda x: 1. if x==’male’ else 0.)</p>

<h1 id="regexp-extraction">regexp extraction</h1>
<p>import re</p>

<p>def extract_patt(patt, linea):
    matchObj = re.match(patt, linea)
    result = NaN
    if matchObj:
        return matchObj.group(1).lower()
    else:
        print “No match!!”
        return NaN</p>

<p>def extract_title(linea):
    return extract_patt(‘^.+,\s(.+)..+’, linea)</p>

<h1 id="apply-functions-to-dataframe--fare-nan">apply functions to dataframe =&gt; Fare NaN</h1>
<p>fares_by_class = data.groupby(‘Pclass’).Fare.median()</p>

<p>def getFare(example):
    if isnull(example[‘Fare’]):
        example[‘Fare’] = fares_by_class[example[‘Pclass’]]
    return example</p>

<p>data = data.apply(getFare, axis=1)</p>

<h1 id="plotting">plotting</h1>
<p>bp = data.boxplot(column=’Age’, by=’Pclass’, grid=False)
for i in [1,2,3]:
    y = data.Age[data.Pclass==i].dropna()
    # Add some random “jitter” to the x-axis
    x = np.random.normal(i, 0.04, size=len(y))
    plot(x, y, ‘r.’, alpha=0.2)
```</p>

<h2 id="and-then-i-got-creative">and then I got creative:</h2>

<p>I established as base features the Sex, Pclass, Fare and Age. To fill the missing values of Age I first extracted the titles from the names, then used the median of the title group as best guess. I used similar approaches for other missing values.</p>

<p>Then I added 3 identity feature vectors [C, Q, S] depending on where the passenger embarked on. So if Mss. Roberts embarked on Southampton the three vectors would be [0, 0, 1].</p>

<p>I wanted to give more power to the mix experience with money so I created another vector that resulted from multiplying Age and Pclass. The intuition behind was that when the moment came to choose among passengers, an elder statesman would have more ‘presence’ to win a social standoff.</p>

<p>I didn’t like the Parch data because it mixes parents with children, which I think is key. And neither I liked SibSp because it mixes spouses with siblings. So I added them all in a single vector with the idea that people with family would fare differently that those without.</p>

<p>I added another identity vector that described if the person was married based on the title associated in the name (1 for Mr and Mrs).</p>

<p>Finally I created a last identity vector with the following characteristics:</p>

<ul>
  <li>if the passenger is a woman, get a 1 if a relative (searched by same last name) or cabin fellow (someone in the same cabin) survived.</li>
  <li>if the passenger is a man, get a 1 if the same applies but the survivor is older.</li>
</ul>

<p>The intuition was based on the assumption that if someone survives, he/she would try to also help a younger male (son) or relative.</p>

<p>I also tried many other weird features like using the letter of the Cabin which didn’t give good results (a scatter plot of this with Pclass is enough to see there is not much there).</p>

<h2 id="better-way-to-fill-in-missing-data">better way to fill in missing data</h2>

<p>Age seems to be a key feature and many of its values are missing. The median per title is a good approach but we can do better. Why not train an SVM? I created bins of age &lt;= [14,30,60,…], and trained an SVM with Pclass, Parch, SibSp, isMarried and title_level (a different int for each unique title) to assign each missing age to one of the new age bins. My accuracy on a cv set was a mere 67.5%.</p>

<p>Now, consider that I left charisma as it was because it proved to be a good feature (computed as the product between the class and the age computed as given or median of title).</p>

<p>Finally, I reduced the number of features so instead of the initial 12 I went down to 10. (Used a loop of possible features combinations and let it self evaluate the best combo).</p>

<h2 id="conclusions">conclusions</h2>

<p>With my latest iteration, number 13, I landed on the 77 spot of 6,978 participants (as of today).</p>

<p><blockquote><p>Score on the test set (Kaggle): 81.818%.</p></blockquote></p>

<p>The key is the feature set. It needs to be expressive, but too much bullshit only drives the model down. So only add important information (remember the boost got from using 20x20 instead of the 28x28 images on the OCR competition).</p>

<p>Treat the results from your cross validation with a grain of salt. Obsessing too much about them may push you to overfit your data! My best score resulted from a model that gave me a lower 85% on my cross validation sets. A good hint comes from the number of support vectors distilled by the model!</p>

<p>I have only spent a couple of days on features, with a creative team I am sure that the results can be improved greatly. There are many ideas to try like:</p>

<ul>
  <li>use a neural network to get a better age estimate.</li>
  <li>mix and match models and make them vote on predictions.</li>
  <li>try new narratives than can be converted into features (like the intuition that a father who survives will do the utmost for his children to survive too).</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kaggle Digit OCR: SVM (top 5)]]></title>
    <link href="http://sotoseattle.github.io/blog/2013/10/13/Kaggle-Digit-SVM/"/>
    <updated>2013-10-13T18:02:00-07:00</updated>
    <id>http://sotoseattle.github.io/blog/2013/10/13/Kaggle-Digit-SVM</id>
    <content type="html"><![CDATA[<p>SVM is much easier to implement than our previous regressions because it is already been coded in the package <a href="http://scikit-learn.org/stable/modules/svm.html">scikit learn</a> (sklearn). All that is needed is to tweak the hyperparameters to achieve a high accuracy.</p>

<p>My data set is divided as before, 30.000 images for training and 12.000 for evaluation. SVM works best with scaled input so my Xs have been pre-processed so all pixel values are in the range {0,1}, and each feature has zero mean and unit variance. For this we also use sklearn preprocessing utilities.</p>

<p>I have tried two non-linear approaches gaussian and 4<sup>th</sup> degreee polynomial, to see how they compare to the most rudimentary linear approach of past runs.</p>

<h2 id="gaussian-kernel">gaussian kernel</h2>

<p>For the Kaggle digit recognition competition I have employed a Gaussian kernel (RBF) with a sigma equal to 0.001. The safety margin constant C is set to 14. Both parameters were found after a few trail and error runs. SVM is pretty slow so the trick is to use very small training/eval sets (2000 images) to zero in the coeficients. Later on we can better the result with the whole sets.</p>

<p><code>python svm rbf on test data
m = x_test.shape[0]
clf = svm.SVC(C=14, kernel='rbf', gamma=0.001, cache_size=200)
clf.fit(x[0:42000,:], y[0:42000])
predictions = clf.predict(x_test).values
</code></p>

<p><blockquote><p>The accuracy on the evaluation set is 96.55% and on the test set (Kaggle) is 96,91%.</p></blockquote></p>

<p>The result beat the previous 2-regressions approach (93.6%) by a high margin.</p>

<h2 id="polynomial-kernel">polynomial kernel</h2>

<p>4<sup>th</sup> degreee : $(\gamma \langle x, x’\rangle + r)^4$ with a hyperparameter r of 0.38, and a regularization constant C of 8.3. The hyperparameters were found using the StratifiedKFold and GridSearchCV from the scikit package from which we can also produce a heatmap.</p>

<p><img src="/images/kaggle_ocr/choose_params_svm.png" title="searching for hyperparams" ></p>

<p><code>python svm call
clf = svm.SVC(C=6.2, kernel='poly', degree=4, coef0=0.48, cache_size=200)
</code></p>

<p>The accuracy on the evaluation set is 97.9%. </p>

<h2 id="twerking-the-training-data">twerking the training data</h2>

<p>But I am not done. I want to try a crazy idea I just found, Virtual SVM! </p>

<p>Kudos to <a href="http://www.jetgoodson.com/blogged.php?ident=49">Jet Goodson</a> and his clever way to put this Virtual SVM to work. It is based on the paper <a href="http://www.cs.berkeley.edu/~malik/cs294/decoste-scholkopf.pdf">Training Invariant Support Vector Machines</a> by Dennis DeCoste and Bernhard Scholkopf.</p>

<p>The trick is that when you run an SVM you get as a byproduct a set of Support Vectors, which are nothing but the subset of the training examples that the SVM actually bases its decisions on. Those are the training examples that define the hyperplanes.</p>

<p><code>clf.support_vectors_</code> gives us a matrix of support vectors, which in our case are around 10,000. And <code>clf.support_</code> gives an array with the index of the training examples that are the support vectors. We can check this:</p>

<p><code>python python console
clf.support_
# array([    5,    69,   146, ..., 41962, 41992, 41999], dtype=int32)
assert np.allclose(xt[5,:], clf.support_vectors_[0,:])
</code></p>

<p>So if we want to artificially create new examples from the old ones (moving, blurring, etc.), we should only modify the ones that count. In order to modify the image, since I am a newbie, I’ll just move the image to each side [N,S,W,E] a tiny bit (one pixel), and rotate 10 degrees clockwise and counter-clockwise. </p>

<p>For every image of the support vectors set I can get an additional 6 twerked images. This totals a new training set of around 10.000 x 7 = 70.000 images. This is a stretch for a  SVM but nothing compared that to 42.000 x 7 = 294.000 of directly applying the transformations to the training set.</p>

<p><blockquote><p>The accuracy on the evaluation set is 98.225% and on the test set (Kaggle): 98.086%.</p></blockquote></p>

<p><a href="http://scipy-lectures.github.io/advanced/image_processing/index.html">Important resource</a> to start understanding how to manipulate images in python.</p>

<p>```python Twerk SVM for test set
def twerk(x_old_set, y_old_set):
    m,n = x_old_set.shape
    x_new_set = np.zeros((7<em>m, n))
    y_new_set = np.zeros((7</em>m,))
    j = 0
    for i in range(m):
        # add the original one
        x_new_set[j] = x_old_set[i,:]
        y_new_set[j] = y_old_set[i]</p>

<pre><code>    x = x_old_set[i,:].reshape(28,28)
    y = y_old_set[i]
    
    # move north 1 px
    j +=1
    x_new_set[j] = np.vstack([x[1:,:], x[27:,:]]).flatten()
    #x_new_set[j] = np.roll(x, 2*k[0], k[1]).flatten()
    y_new_set[j] = y
    # move south 1 px
    j +=1
    x_new_set[j] = np.vstack([x[0:1,:], x[0:27,:]]).flatten()
    y_new_set[j] = y
    # move left 1 px
    j +=1
    x_new_set[j] = np.hstack([x[:,1:], x[:,0:1]]).flatten()
    y_new_set[j] = y
    # move right 1 px
    j +=1
    x_new_set[j] = np.hstack([x[:,27:], x[:,0:27]]).flatten()
    y_new_set[j] = y
    # rotate 10 degrees
    j +=1
    x_new_set[j] = ndimage.rotate(x, 10, reshape=False).flatten()
    y_new_set[j] = y
    # rotate -10 degrees
    j +=1
    x_new_set[j] = ndimage.rotate(x, -10, reshape=False).flatten()
    y_new_set[j] = y
        
    j += 1
return [x_new_set, y_new_set]
</code></pre>

<p>def prep_submit():
    clf = svm.SVC(C=6.2, kernel=’poly’, degree=4, coef0=0.48, cache_size=200)
    clf.fit(x[0:42000,:], y[0:42000])</p>

<pre><code>x_support = clf.support_vectors_
y_support = np.array(y[clf.support_])
[x_new, y_new]= twerk(x_support, y_support)
clf.fit(x_new, y_new)

a = clf.predict(x_test)
b = np.arange(1,28001,1)
predictions = np.vstack([b, a]).T
print 'To submitt add header: ImageId,Label'
np.savetxt('./data/kaggle/predictions_svm.csv', predictions, fmt='%i,%i') ```
</code></pre>

<h2 id="last-experiment">last experiment</h2>

<p>For SVM the size of the training set makes a difference. I am going to try to run it all on the 70,000 images, the training set plus the testing set with the added predictions from the best run. </p>

<p>This is totally endogamy and an article of faith no-no but I figured:</p>

<ul>
  <li>since I am already geting 98% right, if at least I can reinforce the good ones, the better performance from that reinforcement of the good ones may be enough to counteract the added errors from that 2% wrongly labeled.</li>
  <li>it is great way to confirm the article of faith.</li>
</ul>

<p>With twerking et all, we do much worse, the new Kaggle rating is 97.95%</p>

<h2 id="no-seriously-the-last-one">no, seriously, the last one</h2>

<p>I thought: why 28x28 and not bigger? Or smaller? The difference is the amount of padding around the image. I have read that the samples are already pretty centered and aligned, so why not give more importance to the center of the picture and less as we go out? What would happen if we initially crop the image from 28x28 to 20x20? It seems that those 4 pixel bands are pretty much empty. Maybe focusing in the inner image we can better train the model.</p>

<p><code>python croping the images
m = x.shape[0]
new_x = np.zeros((m,400))
for i in range(m):
    old = x[i,:].reshape(28,28)
    new_x[i,:] = old[4:24,4:24].flatten()
x = new_x
</code></p>

<p>Doing the same as before (re-optimizing hyper params =&gt; {C= 2.6, r= 0.34}, twerking as before, etc) we get:</p>

<p><blockquote><p>Accuracy on evaluation set: 98.7%<br/>Score on the test set (Kaggle): 98.87%.</p></blockquote></p>

<p>It is a huge jump of almost a percentage point only based on munging the input!</p>

<p>The new position is 80 out of 1956 participants (as of today), in the top 5%! Consider that my first trial a few days ago (a simple linear softmax model) placed me at position 1,654, in the bottom 15%.</p>

<h2 id="conclusion">conclusion</h2>

<p>Again, once reached the plateau of the model it is very difficult to raise the bar.</p>

<p>The key is to be creative and look for out-of-the-box approaches.</p>

<p>The single most important aspect is to know the data, visualize in different ways to understand it and to analyze correctly where the model fails.
Finally, I suspect that, with a bit of further image manipulation, the accuracy can be raised a bit more. I’ll leave this to a future competition.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kaggle Digit OCR: 2 Step Regression]]></title>
    <link href="http://sotoseattle.github.io/blog/2013/10/07/Kaggle-Digit-2-regress/"/>
    <updated>2013-10-07T09:02:00-07:00</updated>
    <id>http://sotoseattle.github.io/blog/2013/10/07/Kaggle-Digit-2-regress</id>
    <content type="html"><![CDATA[<p>Based on the previous posts we are going to extend our softmax model so we classify the handwritten images by using logistic regressions in two steps:</p>

<ol>
  <li>
    <p>First we run our Softmax (multivariate logistic regression) classifier as before.</p>
  </li>
  <li>
    <p>When classifying a particular image, if the first choice has less than 99% probability and the second choice has more than 1%, then we run a logistic regression between those two top choices to make sure we pick the right one. These limits are completely arbitrary, pretty irrelevant and stated to make the concept clearer.</p>
  </li>
</ol>

<p>This means computing the 784+1 parameters of the logystic regression 43 times, which is quite hair-raising but not heart-stopping (30 mins in my old mac).</p>

<p>So if it is very clear we go softmax, but if there is a doubt we rely on a more accurate logistic regression between the top two softmax choices. </p>

<p><blockquote><p>The new accuracy from Kaggle is 93.64%.</p></blockquote></p>

<p>Here is the modified code:</p>

<p>```python kaggle 2 step regression model
import math
import pandas as pd
import numpy as np</p>

<p>import sys
sys.path.append( ‘../../.’ ) # regression modules address
import Logysoft as soft
import Logysterical as logit</p>

<p>class KaggleTop2(object):
    def <strong>init</strong>(self):
        self.LABS = 10              # N. of possible values of Y (labels)
        # load data files
        training_data = pd.read_csv(‘./data/kaggle/train.csv’, header=0)
        testing_data = pd.read_csv(‘./data/kaggle/test.csv’, header=0)
        self.xt = np.array(training_data.ix[:, 1:]).astype(‘float64’)
        self.yt = np.atleast_2d(training_data.ix[:, 0]).T</p>

<pre><code>    # evaluation set
    self.xe = self.xt[30000:, :]
    self.ye = self.yt[30000:, :]
    
    # testing set
    self.x_test = np.array(testing_data.ix[:,:]).astype('float64')

def optimize_logit_for(self, pair):
    # extract the right images from the set
    [a,b] = [int(pair[0]), int(pair[1])]
    
    data = np.array(pd.read_csv('./data/kaggle/train.csv', header=0)).astype('float64')
    data_t = data[0:, :]
    
    data_a = data_t[data_t[:,0]==a]
    data_b = data_t[data_t[:,0]==b]
    data_ab = np.vstack([data_a, data_b])
    xt = data_ab[:, 1:]
    yt = data_ab[:, 0].astype('int8')
    yt = np.atleast_2d(yt).T

    # Perform logistic regression
    xt2 = np.column_stack([np.ones((xt.shape[0],1)), xt])
    yt2 = yt.flatten()
    count = 0
    for i in range(yt2.size):
        if yt2[i]==a:
            yt2[i] = 1
            count +=1
        else:
            yt2[i] = 0    
    
    ini_thetas = 0.005*np.random.rand(xt2.shape[1],1)
    L = 1e+5
    opt_thetas = logit.optimizeThetas(ini_thetas, xt2, yt2, L, visual=False)
    return opt_thetas

def test_model_submit(self):
    logit_thetas = {}
    
    soft_thetas = np.array(pd.read_csv('./data/kaggle/submit_optimized_thetas.csv', header=None))
    soft_thetas = soft_thetas.reshape(self.LABS, -1)

    m, n = self.x_test.shape
    h = soft.h(soft_thetas, self.x_test)
    predictions = np.zeros((m,2))
    for i in range(m):
        [ml_1, ml_2] = h[i,:].argsort()[-2:][::-1] # 1st and 2nd model choices
        p1,p2 = h[i,:][ml_1], h[i,:][ml_2]
        right_order = True
        if ml_1 &gt; ml_2:
            right_order = False
            s = `ml_2`+`ml_1`
        else:
            s = `ml_1`+`ml_2`
        
        if p1&lt;0.99 and p2&gt;0.01:
            if s not in logit_thetas:
                logit_thetas[s] = self.optimize_logit_for(s)

            l_t = logit_thetas[s]
            logix = np.hstack([1, self.x_test[i,:]])

            p = logit.h(l_t, logix)
            if (p&gt;0.5):
                predictions[i,:] = ([i+1, ml_1] if right_order else [i+1, ml_2])
            else:
                predictions[i,:] = ([i+1, ml_2] if right_order else [i+1, ml_1])
        else:
            predictions[i,:]=[i+1, ml_1]

    print 'To submitt add header: ImageId,Label'
    print predictions[0:10,:]
    np.savetxt('./data/kaggle/predictions_2steps.csv', predictions, fmt='%i,%i')
    pass
</code></pre>

<p>k2t = KaggleTop2()
k2t.test_model_submit()
```</p>

<h2 id="conclusion">conclusion</h2>

<p>We have gained a small improvement of 1.6%. The right direction, yet far away from the results achieved with other models. This seems to suggest a limitation for this problem of a learning function that is linear. In future posts I’ll look for models that rely on non-linear functions.</p>

<p>I also wonder what would be the result of using as model logistic regression one-vs-all for each label. And how it compares to plain softmax.</p>

]]></content>
  </entry>
  
</feed>
