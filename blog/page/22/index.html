
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="There is no known closed-form way to solve for the minimum of $J(θ)$, and thus as usual we’ll resort to an iterative optimization algorithm such as &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/22">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Software Interloper Developer</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/03/Softmax-III/">Softmax Theory: $∇(θ)$</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-03T09:02:00-07:00" pubdate data-updated="true">Oct 3<span>rd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>There is no known closed-form way to solve for the minimum of $J(θ)$, and thus as usual we’ll resort to an iterative optimization algorithm such as gradient descent or L-BFGS. Taking derivatives, one can show that the gradient is:</p>

<script type="math/tex; mode=display">
\begin{align}
\nabla_{\theta_j} J(\theta) = - \frac{1}{m} \sum_{i=1}^{m}{ \left[ x^{(i)} ( 1\{ y^{(i)} = j\}  - p(y^{(i)} = j | x^{(i)}; \theta) ) \right]  } + \lambda \theta_j
\end{align}
</script>

<p>By minimizing $J(θ)$ with respect to $θ$, we will have a working implementation of softmax regression. Note that the gradient output is flatten from a matrix form to a vector form. This is needed format for the gradient descent algorithm to work.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>gradient function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">):</span>
</span><span class="line">    <span class="n">thetas</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">numLabels</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">    <span class="n">hx</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span><span class="line">    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">grad</span> <span class="o">=</span> <span class="p">((</span><span class="n">groundTruth</span><span class="o">-</span><span class="n">hx</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="o">-</span><span class="n">m</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">regul_lambda</span><span class="o">*</span><span class="n">thetas</span><span class="p">)</span>
</span><span class="line">    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">grad</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="checking-the-gradient-computation">checking the gradient computation</h2>

<p>It is very a healthy habit to check gradients numerically before proceeding to train the model. The norm of the difference between the numerical gradient and your analytical gradient should be small, on the order of 10 − 9. The following code uses some random data to compare the above gradient against a numerical approximation based on the formula</p>

<p>The key to compute the gradient by hand is to keep all $θ$ fixed while we change a single one by a small amount (epsilon) $\theta_j := \theta_j - \alpha \nabla_{\theta_j} J(\theta) (for each j=1,\ldots,k)$.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>checking gradient with numerical approximation</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">grad_by_hand</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">):</span>
</span><span class="line">    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-4</span>
</span><span class="line">    <span class="n">t</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">    <span class="n">n</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span>
</span><span class="line">    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span><span class="line">        <span class="n">t1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span><span class="line">        <span class="n">t2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span><span class="line">        <span class="n">t1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
</span><span class="line">        <span class="n">t2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
</span><span class="line">        <span class="n">a</span> <span class="o">=</span> <span class="n">j</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">)</span>
</span><span class="line">        <span class="n">b</span> <span class="o">=</span> <span class="n">j</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">)</span>
</span><span class="line">        <span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">epsilon</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">grad</span>
</span><span class="line">
</span><span class="line"><span class="n">x_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
</span><span class="line"><span class="n">y_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line"><span class="n">g_check</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">groundTruth</span><span class="p">(</span><span class="n">y_check</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">)</span>
</span><span class="line"><span class="n">t_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line"><span class="n">g_theo</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">t_check</span><span class="p">,</span> <span class="n">x_check</span><span class="p">,</span> <span class="n">g_check</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
</span><span class="line"><span class="n">g_hand</span> <span class="o">=</span> <span class="n">grad_by_hand</span><span class="p">(</span><span class="n">t_check</span><span class="p">,</span> <span class="n">x_check</span><span class="p">,</span> <span class="n">g_check</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
</span><span class="line"><span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_hand</span> <span class="o">-</span> <span class="n">g_theo</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_hand</span> <span class="o">+</span> <span class="n">g_theo</span><span class="p">)</span>
</span><span class="line"><span class="k">assert</span> <span class="n">diff</span> <span class="o">&lt;=</span> <span class="mf">1e-9</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/02/Softmax-II/">Softmax Theory: $J(θ)$</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-02T09:02:00-07:00" pubdate data-updated="true">Oct 2<span>nd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>The cost function of logistic regression was defined as:</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
J(\theta)
&= - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=0}^{1} 1\left\{y^{(i)} = j\right\} \log p(y^{(i)} = j | x^{(i)} ; \theta) \right]
\end{align}
 %]]></script>

<p>Softmax is just a generalization of logistic regression from 2 to k classes. In our case we have:</p>

<script type="math/tex; mode=display">
p(y^{(i)}=j | x^{(i)} ; \theta) = \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}} }
</script>

<p>And the softmax cost function is</p>

<script type="math/tex; mode=display">
\begin{align}
J(\theta) = - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y^{(i)} = j\right\} \log \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)} }}  \right]
              + \frac{\lambda}{2} \sum_{i=1}^k \sum_{j=0}^n \theta_{ij}^2
\end{align}
</script>

<p>…which is similar to the cost function of logistic regression, except that we now sum over the k different possible values of the class label. </p>

<h2 id="regularization">regularization</h2>

<p>We have included in the cost function a regularization component, a weight decay term which penalizes large values of the parameters.</p>

<script type="math/tex; mode=display">
\frac{\lambda}{2} \sum_{i=1}^k \sum_{j=0}^{n} \theta_{ij}^2
</script>

<p>With this weight decay term (for any $λ &gt; 0$), the cost function $J(θ)$ is now strictly convex, and is guaranteed to have a unique solution. The Hessian is now invertible, and because $J(θ)$ is convex, algorithms such as gradient descent, L-BFGS, etc. are guaranteed to converge to the global minimum.</p>

<h2 id="indicator-function">indicator Function</h2>

<p>In the equation above, $1\{\cdot\}$ is the indicator function, so that 1{a true statement} = 1, and 1{a false statement} = 0. For example, 1{2 + 2 = 4} evaluates to 1; whereas 1{1 + 1 = 5} evaluates to 0.</p>

<p>The following code takes in a vector y with observed labels (from training set) and outputs the indicator function. So for example, for…</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align} 
y &=

\begin{bmatrix} 
3 \\
0 \\
8 \\
\end{bmatrix} \\
\end{align} 
 %]]></script>

<p>…it ouputs</p>

<script type="math/tex; mode=display">
[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\\
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
</script>

<p>An (m,1) label vector is converted into a (m, num_labels) sparse matrix.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>indicator function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">groundTruth</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">):</span>
</span><span class="line">    <span class="n">m</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">groundTruth</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">))</span>
</span><span class="line">    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">        <span class="n">groundTruth</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line">    <span class="k">return</span> <span class="n">groundTruth</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Once we have the indicator function built, the code for the cost function is simple.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>cost function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">j</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">):</span>
</span><span class="line">    <span class="n">thetas</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">numLabels</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>                    <span class="c"># just making sure well formed</span>
</span><span class="line">    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">hx</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>                                         <span class="c"># compute hypothesys matrix</span>
</span><span class="line">    <span class="n">b</span> <span class="o">=</span> <span class="n">groundTruth</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">hx</span><span class="p">))</span>                              <span class="c"># indicator function computation</span>
</span><span class="line">    <span class="n">lambdaEffect</span> <span class="o">=</span> <span class="p">(</span><span class="n">regul_lambda</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">thetas</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c"># regularization cost component</span>
</span><span class="line">    <span class="n">J</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">b</span><span class="p">))</span><span class="o">/</span><span class="n">m</span> <span class="o">+</span> <span class="n">lambdaEffect</span>                   <span class="c"># add up to compute scalar cost</span>
</span><span class="line">    <span class="k">return</span> <span class="n">J</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/01/Softmax-I/">Softmax Theory: $h_θ(x)$</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-01T09:02:00-07:00" pubdate data-updated="true">Oct 1<span>st</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>These notes are based on the material publicly available at <a href="http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression">UFLDL</a> site from Stanford University (check it out, is a great resource) and Wikipedia.</p>

<h2 id="intro">intro</h2>

<p>Softmax regression is a.k.a multinomial logistic regression or multinomial logit. In fields like NLP, when a classifier is implemented using softmax regression, it is commonly known as a maximum entropy classifier, conditional maximum entropy model or MaxEnt model for short.</p>

<p>It is used when the dependent variable in question is nominal (equivalently categorical, meaning that it falls into any one of a set of categories which cannot be ordered in any meaningful way) and for which there are more than two categories. Some examples would be:</p>

<p>We are interested in multi-class classification (as opposed to only binary classification), and so the label y can take on k different values, rather than only two (for logistic regression). Being a classification problem, all label classes are equivalently categorical, meaning that the set of categories cannot be ordered in any meaningful way and we just decide which to assign. Examples:</p>

<ul>
  <li>Which major will a college student choose, given their grades, stated likes and dislikes, etc.?</li>
  <li>Which blood type does a person have, given the results of various diagnostic tests?</li>
  <li>In a hands-free mobile phone dialing application, which person’s name was spoken, given various properties of the speech signal?</li>
  <li>Which candidate will a person vote for, given particular demographic characteristics?</li>
  <li>Which country will a firm locate an office in, given the characteristics of the firm and of the various candidate countries?</li>
</ul>

<h2 id="hypothesis-function">hypothesis function</h2>

<p>We will have a set of features (independent variables) that will predict the dependent variable ($y$), the assigned class. Softmax regression is a particular solution to the classification problem that <b>assumes that a linear combination of the observed features</b> and some problem-specific parameters can be used to determine the probability of each particular outcome of the dependent variable. </p>

<p>As with other types of regression, there is no need for the independent variables to be statistically independent from each other (unlike, for example, in a naive Bayes classifier); however, collinearity is assumed to be relatively low, as it becomes difficult to differentiate between the impact of several variables if they are highly correlated.</p>

<p>In our training set $(x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)})$, we now have that $y^{(i)} \in \{1, 2, \ldots, k\}$ instead of taking the values $ \{0, 1\} $.</p>

<p>[Note] Since we’ll be working in Python our convention in the code will be to index the classes starting from 0, rather than from 1 (in these notes and coding in Octave).</p>

<p>Given a test input x, we want our hypothesis to estimate the probability that p(y = j|x) for each value of $j = \{1, \ldots, k\}$. In other words, give the input x, we want to estimate the probability that its label is each of the k different possible values. Thus, our hypothesis will output a k dimensional vector (whose elements sum to 1) giving us our k estimated probabilities. Concretely, our hypothesis $h_θ(x)$ takes the form:</p>

<script type="math/tex; mode=display">
\begin{align}
h_\theta(x^{(i)}) =
\begin{bmatrix}
p(y^{(i)} = 1 | x^{(i)}; \theta) \\
p(y^{(i)} = 2 | x^{(i)}; \theta) \\
\vdots \\
p(y^{(i)} = k | x^{(i)}; \theta)
\end{bmatrix}
=
\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix}
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix}
\end{align}
</script>

<p>The term </p>

<script type="math/tex; mode=display">\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }</script>

<p>normalizes the distribution, so that it sums to one. $θ$ is the set of parameters of our model, a (k, n) matrix obtained by stacking up $\{ \theta<em>1, \theta</em>2, \ldots, \theta_k \}$ in rows, so that</p>

<script type="math/tex; mode=display">
\theta = \begin{bmatrix}
\mbox{---} \theta_1^T \mbox{---} \\
\mbox{---} \theta_2^T \mbox{---} \\
\vdots \\
\mbox{---} \theta_k^T \mbox{---} \\
\end{bmatrix}
</script>

<p>Assuming input data x as a matrix (m, n) the code would be:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hypothesys function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span class="line">    <span class="n">H</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>                        <span class="c"># basic product to compute h</span>
</span><span class="line">    <span class="n">H</span> <span class="o">=</span> <span class="n">H</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>                 <span class="c"># to avoid overflow subtract max</span>
</span><span class="line">    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>                            <span class="c"># exponentiate</span>
</span><span class="line">    <span class="n">suma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>   <span class="c"># compute normalization constant</span>
</span><span class="line">    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">suma</span><span class="p">)</span>                     <span class="c"># and normalize</span>
</span><span class="line">    <span class="k">return</span> <span class="n">H</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="overflow-trick">overflow trick</h2>

<p>When the products $\theta_i^T x^{(i)}$ are large, the exponential function $e^{\theta_i^T x^{(i)}}$ will become very large and possibly overflow. To avoid this there is an easy solution - observe that we can multiply the top and bottom of the hypothesis by some constant without changing the output:</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align} 
h(x^{(i)}) &=
 
\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix} \\
&=

\frac{ e^{-\alpha} }{ e^{-\alpha} \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix} \\

&=

\frac{ 1 }{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} - \alpha }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} - \alpha } \\
e^{ \theta_2^T x^{(i)} - \alpha } \\
\vdots \\
e^{ \theta_k^T x^{(i)} - \alpha } \\
\end{bmatrix} \\

\end{align}
 %]]></script>

<p>So, to prevent overflow simply subtract some large constant value from each of the $\theta_j^T x^{(i)}$ terms before computing the exponential. In practice, for each example, you can use the maximum of the $\theta_j^T x^{(i)}$ terms as the constant.</p>

<h2 id="when-to-use">when to use</h2>

<p>When classes are mutually exclusive a softmax regression classifier is appropriate. Otherwise it may be more appropriate to build separate logistic regression classifiers.</p>

<p>For a digit OCR recognition softmax is ok because the labels are mutually exclusive and we’ll just choose one among them. One label will have a high probability and the rest almost null.</p>

<p>For a letter OCR recognition that not only relies on the simple probability of the label but, for example, in adjacent letters, it may not be appropiate because we’ll need the different probabilities so we can weight them in different ways with other parts of the model. For example with logistic regression: a character may be an “i” (p=0.3) or an “l” (p=0.5), but it is followed by “ng”.</p>

<h2 id="overparametrization">overparametrization</h2>

<p>Softmax regression has an unusual property that it has a “redundant” set of parameters. To explain what this means, suppose we take each of our parameter vectors $θj$, and subtract some fixed vector $ψ$ from it, so that every $θj$ is now replaced with $θj − ψ (\forall j=1, \ldots, k)$. Our hypothesis now estimates the class label probabilities as</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
p(y^{(i)} = j | x^{(i)} ; \theta)
&= \frac{e^{(\theta_j-\psi)^T x^{(i)}}}{\sum_{l=1}^k e^{ (\theta_l-\psi)^T x^{(i)}}}  \\
&= \frac{e^{\theta_j^T x^{(i)}} e^{-\psi^Tx^{(i)}}}{\sum_{l=1}^k e^{\theta_l^T x^{(i)}} e^{-\psi^Tx^{(i)}}} \\
&= \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}}}.
\end{align}
 %]]></script>

<p>In other words, it does not affect our hypothesis’ predictions at all! This shows that softmax regression’s parameters are “redundant.” More formally, we say that our softmax model is over parameterized, meaning that for any hypothesis we might fit to the data, <strong>there are multiple parameter settings that give rise to exactly the same hypothesis function</strong> $hθ$ mapping from inputs x to the predictions.</p>

<h2 id="softmax-is-a-generalization-of-logistic-regression">softmax is a generalization of logistic regression</h2>

<p>In the special case where k = 2, the softmax regression hypothesis outputs:</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
h_\theta(x) &=

\frac{1}{ e^{\theta_1^Tx}  + e^{ \theta_2^T x^{(i)} } }
\begin{bmatrix}
e^{ \theta_1^T x } \\
e^{ \theta_2^T x }
\end{bmatrix}
\end{align}
 %]]></script>

<p>Taking advantage of the fact that this hypothesis is over parameterized and setting $ψ$ = $θ$1, we can subtract $θ$1 from each of the two parameters, giving us</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
h(x) &=

\frac{1}{ e^{\vec{0}^Tx}  + e^{ (\theta_2-\theta_1)^T x^{(i)} } }
\begin{bmatrix}
e^{ \vec{0}^T x } \\
e^{ (\theta_2-\theta_1)^T x }
\end{bmatrix} \\


&=
\begin{bmatrix}
\frac{1}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
\frac{e^{ (\theta_2-\theta_1)^T x }}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } }
\end{bmatrix} \\

&=
\begin{bmatrix}
\frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
1 - \frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
\end{bmatrix}
\end{align}
 %]]></script>

<p>Thus, replacing $θ$2 − $θ$1 with a single parameter vector $$’, we find that softmax regression predicts the probability of each class like with logistic regression. </p>

<script type="math/tex; mode=display">
\frac{1}{ 1  + e^{ (\theta')^T x^{(i)} } }
\\
1 - \frac{1}{ 1 + e^{ (\theta')^T x^{(i)} } }
</script>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/21/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/12/05/ruby-continuations/">Ruby Continuations</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/04/Seattlerb-GoL/">Seattle Ruby Game Of Life</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/01/CodeFellows/">Article Published</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/15/Scheme-GOL/">Game of Life in Scheme</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/18/4-PSD/">4 Principles of Simple Design Reflections</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
