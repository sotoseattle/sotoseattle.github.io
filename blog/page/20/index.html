
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="These notes are based on the material publicly available at UFLDL site from Stanford University (check it out, is a great resource) and Wikipedia. &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/20">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Developer of software, ideas, startups</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/01/Softmax-I/">Softmax Theory: $h_θ(x)$</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-01T09:02:00-07:00" pubdate data-updated="true">Oct 1<span>st</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>These notes are based on the material publicly available at <a href="http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression">UFLDL</a> site from Stanford University (check it out, is a great resource) and Wikipedia.</p>

<h2 id="intro">intro</h2>

<p>Softmax regression is a.k.a multinomial logistic regression or multinomial logit. In fields like NLP, when a classifier is implemented using softmax regression, it is commonly known as a maximum entropy classifier, conditional maximum entropy model or MaxEnt model for short.</p>

<p>It is used when the dependent variable in question is nominal (equivalently categorical, meaning that it falls into any one of a set of categories which cannot be ordered in any meaningful way) and for which there are more than two categories. Some examples would be:</p>

<p>We are interested in multi-class classification (as opposed to only binary classification), and so the label y can take on k different values, rather than only two (for logistic regression). Being a classification problem, all label classes are equivalently categorical, meaning that the set of categories cannot be ordered in any meaningful way and we just decide which to assign. Examples:</p>

<ul>
  <li>Which major will a college student choose, given their grades, stated likes and dislikes, etc.?</li>
  <li>Which blood type does a person have, given the results of various diagnostic tests?</li>
  <li>In a hands-free mobile phone dialing application, which person’s name was spoken, given various properties of the speech signal?</li>
  <li>Which candidate will a person vote for, given particular demographic characteristics?</li>
  <li>Which country will a firm locate an office in, given the characteristics of the firm and of the various candidate countries?</li>
</ul>

<h2 id="hypothesis-function">hypothesis function</h2>

<p>We will have a set of features (independent variables) that will predict the dependent variable ($y$), the assigned class. Softmax regression is a particular solution to the classification problem that <b>assumes that a linear combination of the observed features</b> and some problem-specific parameters can be used to determine the probability of each particular outcome of the dependent variable. </p>

<p>As with other types of regression, there is no need for the independent variables to be statistically independent from each other (unlike, for example, in a naive Bayes classifier); however, collinearity is assumed to be relatively low, as it becomes difficult to differentiate between the impact of several variables if they are highly correlated.</p>

<p>In our training set $(x^{(1)}, y^{(1)}), \ldots, (x^{(m)}, y^{(m)})$, we now have that $y^{(i)} \in \{1, 2, \ldots, k\}$ instead of taking the values $ \{0, 1\} $.</p>

<p>[Note] Since we’ll be working in Python our convention in the code will be to index the classes starting from 0, rather than from 1 (in these notes and coding in Octave).</p>

<p>Given a test input x, we want our hypothesis to estimate the probability that p(y = j|x) for each value of $j = \{1, \ldots, k\}$. In other words, give the input x, we want to estimate the probability that its label is each of the k different possible values. Thus, our hypothesis will output a k dimensional vector (whose elements sum to 1) giving us our k estimated probabilities. Concretely, our hypothesis $h_θ(x)$ takes the form:</p>

<script type="math/tex; mode=display">
\begin{align}
h_\theta(x^{(i)}) =
\begin{bmatrix}
p(y^{(i)} = 1 | x^{(i)}; \theta) \\
p(y^{(i)} = 2 | x^{(i)}; \theta) \\
\vdots \\
p(y^{(i)} = k | x^{(i)}; \theta)
\end{bmatrix}
=
\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix}
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix}
\end{align}
</script>

<p>The term </p>

<script type="math/tex; mode=display">\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }</script>

<p>normalizes the distribution, so that it sums to one. $θ$ is the set of parameters of our model, a (k, n) matrix obtained by stacking up $\{ \theta<em>1, \theta</em>2, \ldots, \theta_k \}$ in rows, so that</p>

<script type="math/tex; mode=display">
\theta = \begin{bmatrix}
\mbox{---} \theta_1^T \mbox{---} \\
\mbox{---} \theta_2^T \mbox{---} \\
\vdots \\
\mbox{---} \theta_k^T \mbox{---} \\
\end{bmatrix}
</script>

<p>Assuming input data x as a matrix (m, n) the code would be:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>hypothesys function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span class="line">    <span class="n">H</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>                        <span class="c"># basic product to compute h</span>
</span><span class="line">    <span class="n">H</span> <span class="o">=</span> <span class="n">H</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>                 <span class="c"># to avoid overflow subtract max</span>
</span><span class="line">    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">H</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>                            <span class="c"># exponentiate</span>
</span><span class="line">    <span class="n">suma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>   <span class="c"># compute normalization constant</span>
</span><span class="line">    <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">suma</span><span class="p">)</span>                     <span class="c"># and normalize</span>
</span><span class="line">    <span class="k">return</span> <span class="n">H</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="overflow-trick">overflow trick</h2>

<p>When the products $\theta_i^T x^{(i)}$ are large, the exponential function $e^{\theta_i^T x^{(i)}}$ will become very large and possibly overflow. To avoid this there is an easy solution - observe that we can multiply the top and bottom of the hypothesis by some constant without changing the output:</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align} 
h(x^{(i)}) &=
 
\frac{1}{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix} \\
&=

\frac{ e^{-\alpha} }{ e^{-\alpha} \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} } \\
e^{ \theta_2^T x^{(i)} } \\
\vdots \\
e^{ \theta_k^T x^{(i)} } \\
\end{bmatrix} \\

&=

\frac{ 1 }{ \sum_{j=1}^{k}{e^{ \theta_j^T x^{(i)} - \alpha }} }
\begin{bmatrix} 
e^{ \theta_1^T x^{(i)} - \alpha } \\
e^{ \theta_2^T x^{(i)} - \alpha } \\
\vdots \\
e^{ \theta_k^T x^{(i)} - \alpha } \\
\end{bmatrix} \\

\end{align}
 %]]></script>

<p>So, to prevent overflow simply subtract some large constant value from each of the $\theta_j^T x^{(i)}$ terms before computing the exponential. In practice, for each example, you can use the maximum of the $\theta_j^T x^{(i)}$ terms as the constant.</p>

<h2 id="when-to-use">when to use</h2>

<p>When classes are mutually exclusive a softmax regression classifier is appropriate. Otherwise it may be more appropriate to build separate logistic regression classifiers.</p>

<p>For a digit OCR recognition softmax is ok because the labels are mutually exclusive and we’ll just choose one among them. One label will have a high probability and the rest almost null.</p>

<p>For a letter OCR recognition that not only relies on the simple probability of the label but, for example, in adjacent letters, it may not be appropiate because we’ll need the different probabilities so we can weight them in different ways with other parts of the model. For example with logistic regression: a character may be an “i” (p=0.3) or an “l” (p=0.5), but it is followed by “ng”.</p>

<h2 id="overparametrization">overparametrization</h2>

<p>Softmax regression has an unusual property that it has a “redundant” set of parameters. To explain what this means, suppose we take each of our parameter vectors $θj$, and subtract some fixed vector $ψ$ from it, so that every $θj$ is now replaced with $θj − ψ (\forall j=1, \ldots, k)$. Our hypothesis now estimates the class label probabilities as</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
p(y^{(i)} = j | x^{(i)} ; \theta)
&= \frac{e^{(\theta_j-\psi)^T x^{(i)}}}{\sum_{l=1}^k e^{ (\theta_l-\psi)^T x^{(i)}}}  \\
&= \frac{e^{\theta_j^T x^{(i)}} e^{-\psi^Tx^{(i)}}}{\sum_{l=1}^k e^{\theta_l^T x^{(i)}} e^{-\psi^Tx^{(i)}}} \\
&= \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}}}.
\end{align}
 %]]></script>

<p>In other words, it does not affect our hypothesis’ predictions at all! This shows that softmax regression’s parameters are “redundant.” More formally, we say that our softmax model is over parameterized, meaning that for any hypothesis we might fit to the data, <strong>there are multiple parameter settings that give rise to exactly the same hypothesis function</strong> $hθ$ mapping from inputs x to the predictions.</p>

<h2 id="softmax-is-a-generalization-of-logistic-regression">softmax is a generalization of logistic regression</h2>

<p>In the special case where k = 2, the softmax regression hypothesis outputs:</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
h_\theta(x) &=

\frac{1}{ e^{\theta_1^Tx}  + e^{ \theta_2^T x^{(i)} } }
\begin{bmatrix}
e^{ \theta_1^T x } \\
e^{ \theta_2^T x }
\end{bmatrix}
\end{align}
 %]]></script>

<p>Taking advantage of the fact that this hypothesis is over parameterized and setting $ψ$ = $θ$1, we can subtract $θ$1 from each of the two parameters, giving us</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
h(x) &=

\frac{1}{ e^{\vec{0}^Tx}  + e^{ (\theta_2-\theta_1)^T x^{(i)} } }
\begin{bmatrix}
e^{ \vec{0}^T x } \\
e^{ (\theta_2-\theta_1)^T x }
\end{bmatrix} \\


&=
\begin{bmatrix}
\frac{1}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
\frac{e^{ (\theta_2-\theta_1)^T x }}{ 1 + e^{ (\theta_2-\theta_1)^T x^{(i)} } }
\end{bmatrix} \\

&=
\begin{bmatrix}
\frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
1 - \frac{1}{ 1  + e^{ (\theta_2-\theta_1)^T x^{(i)} } } \\
\end{bmatrix}
\end{align}
 %]]></script>

<p>Thus, replacing $θ$2 − $θ$1 with a single parameter vector $$’, we find that softmax regression predicts the probability of each class like with logistic regression. </p>

<script type="math/tex; mode=display">
\frac{1}{ 1  + e^{ (\theta')^T x^{(i)} } }
\\
1 - \frac{1}{ 1 + e^{ (\theta')^T x^{(i)} } }
</script>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/19/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/10/29/Rails-API-Versioning/">Rails Api Versioning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/29/Devise-Angular-Rails/">Wiring Devise into Angular + Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/27/Wire-Angular-Rails/">Wiring Angular into Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/24/Ruby-Meta-Frolicking/">Ruby Meta-Frolicking</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/20/JS-review/">JavaScript OO Review</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
