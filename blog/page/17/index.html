
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="This is a post about my third tutorial competition, Data Science London. The training data consists of 1,000 records with 40 features. We need to &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/17">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Developer of software, ideas, startups</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/28/Data-Science-London/">Data Science London</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-28T21:02:00-07:00" pubdate data-updated="true">Oct 28<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>This is a post about my third tutorial competition, Data Science London.</p>

<p>The training data consists of 1,000 records with 40 features. We need to classify each record as 1 or 0. The test set consists of 9,000 additional records.</p>

<p>After playing a bit with the training set one realizes that there is a ton of noise in the data, and that we need to carefully distill the features to get something of value. Furthermore, the noisy data is supported by the fact that the features were created artificially for the purpose of this tutorial competition.</p>

<h2 id="feature-selection">feature selection</h2>

<p>To start cleaning up the data a first approach is to see which features give the most predictive power. To do this I use an SVM with Gaussian kernel. Then I find the feature that gives the most predictive power. Then I repeat the process finding the feature that added to the previously selected achieves the highest accuracy. </p>

<p>Another way is to use the ExtraTreesClassifier from the scikit package which results in the following chart of the importance of features (<a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#example-ensemble-plot-forest-importances-py">link</a>):</p>

<p><img class="center" src="/images/kaggle_scilondon/features_power_svm.png" width="600" /></p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
      <th style="text-align: right"> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">SVM</td>
      <td style="text-align: right">14</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">29</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">36</td>
      <td style="text-align: right">39</td>
      <td style="text-align: right">7</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">28</td>
      <td style="text-align: right">19</td>
      <td style="text-align: right">22</td>
      <td style="text-align: right">18</td>
      <td style="text-align: right">38</td>
      <td style="text-align: right">3</td>
      <td style="text-align: right">32</td>
      <td style="text-align: right">34</td>
      <td style="text-align: right">13</td>
    </tr>
    <tr>
      <td style="text-align: left">TreesClassif</td>
      <td style="text-align: right">14</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">6</td>
      <td style="text-align: right">39</td>
      <td style="text-align: right">36</td>
      <td style="text-align: right">18</td>
      <td style="text-align: right">4</td>
      <td style="text-align: right">32</td>
      <td style="text-align: right">38</td>
      <td style="text-align: right">29</td>
      <td style="text-align: right">28</td>
      <td style="text-align: right">23</td>
      <td style="text-align: right">22</td>
      <td style="text-align: right">34</td>
      <td style="text-align: right">7</td>
      <td style="text-align: right">9</td>
      <td style="text-align: right">10</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<p>Only using the feature number 14 we already guess right more than 2/3 of the times. As we add features we increase the accuracy until it stabilizes somewhere between the first 10 and 22 features. Then, the added noise starts to degrade the model and lose predictive power. Optimizing hyper-parameters for the first 18 features we achieve 93.6% on cross validation and 91.84% on the 9,000 samples test set.</p>

<p>ALTERNATIVE WAYS TO CHOOSE FEATURES FROM SCIKIT</p>

<h2 id="dimensionality-reduction">dimensionality reduction</h2>

<p>Even after choosing the set of features we still have way too much noise in the data. One way to ‘clean it up’ is through dimensionality reduction. This has a good and a bad aspect. On one hand, it is a dangerous practice since what we do is project into a lower dimension space all the features and in the process we loose data, maybe valuable. On the other hand, it allows us to apply ‘whitening’, removing data points that are close by and highly correlated.</p>

<p>In this case it pays to apply PCA because I rather start with less valuable data but more salient, than more noisy and diffuse information. Here are two images with accuracy on the first 100 records of training an SVM on the subsequent 900 for different sets of features and dimensions. The left one comes from the main features derived from the tree classifier, the right most from the cycling svm:</p>

<p><img class="left" src="/images/kaggle_scilondon/feat_dim_trees.png" width="420" title="Features from trees" /></p>

<p><img class="right" src="/images/kaggle_scilondon/feat_dim_svm.png" width="420" title="Features from SVM" /></p>

<p><br /></p>

<p>My SVM set achieves a maximum accuracy of 97% for the first 14 features reduced to 12 dimensions. The forest one achieves a top 96% for a wider set of combinations (&gt; 12 features and 12-13 dimensions).</p>

<p>As an aside, I tried all kinds of different approaches available in the scikit package for dim reduction, except for the ‘kernel pca’ that took too long. The best result was achieved with Random PCA on my reduced set of 14 features and reduced to 12 dimensions. These 12 dimensions explain something like half of the variance when considering the whole feature set, and around 90% of the variance in the reduced feature set.</p>

<p><img class="center" src="/images/kaggle_scilondon/explained_var.png" width="500" title="Variance explained from dimensions" /></p>

<p>One final note. It is easy to code the PCA algorithm. Nevertheless I used the SciKit tool because it includes the Random PCA and whitening (which still escapes me). Here is the code for bare-bones PCA:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>barebones PCA</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">pca</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
</span><span class="line">	<span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
</span><span class="line">	<span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</span><span class="line">	<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</span><span class="line">	<span class="n">Sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cov</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span class="line">	<span class="k">print</span> <span class="s">&#39;Sigma shape should be nxn:&#39;</span><span class="p">,</span> <span class="n">Sigma</span><span class="o">.</span><span class="n">shape</span>
</span><span class="line">	<span class="k">print</span> <span class="s">&#39;U will be m x &#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">Sigma</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span class="line">	<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Sigma</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</span><span class="line">	<span class="k">return</span> <span class="p">[</span><span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">]</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">shrink</span><span class="p">(</span><span class="n">example_norm</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">):</span>
</span><span class="line">	<span class="n">Ureduce</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">dimensions</span><span class="p">]</span>
</span><span class="line">	<span class="n">Z</span> <span class="o">=</span> <span class="n">example_norm</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ureduce</span><span class="p">)</span>
</span><span class="line">	<span class="k">return</span> <span class="n">Z</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">inflate</span><span class="p">(</span><span class="n">shrunk_data</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">):</span>
</span><span class="line">	<span class="n">Ureduce</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">dimensions</span><span class="p">]</span>
</span><span class="line">	<span class="n">X_rec</span> <span class="o">=</span> <span class="n">shrunk_data</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Ureduce</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</span><span class="line">	<span class="k">return</span> <span class="n">X_rec</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="engorging-the-training-by-guessing">engorging the training by guessing</h2>

<p>We have an inherent asymmetry: 1000 training records and 9000 for testing. If we could pass records from the testing to the training set we could boost the model’s power. The problem is that we don’t have the labels of the testing set records. But they are so many that we could run our model on the testing set and believe that the few predictions made with very high probability are so certain as to take them as ‘real’. </p>

<p>In an initial pass of the SVM over the testing set we select the predicted records with probability higher than 99% and add them to our training set. We measure the accuracy on a separated cross validation set and store the result. We repeat the process over and over and only stop when the accuracy on the CV set goes down. We end up with around 8,508 records, 2,316 support vectors, and an accuracy of around 97% on cross validation.</p>

<p>The intuition behind the idea is that although we don’t gain anything from certain added records (because we were already pretty sure where they fell), the addition to the training set will allow other less apparent features and subtleties gain contrast and predictive power.</p>

<p>At the end of this process we have a huge, if slightly screwed up training set that gives us the following accuracies against our cross validation set:</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">precision</th>
      <th style="text-align: center">recall</th>
      <th style="text-align: center">f1-score</th>
      <th style="text-align: right">support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">label 0</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">149</td>
    </tr>
    <tr>
      <td style="text-align: left">label 1</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">151</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: left">avg / total</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">300</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h2 id="experiment-with-nn">experiment with NN</h2>

<p>As an experiment, we train a neural network with quite an excess of nodes [input: 18, hidden_1: 36, hidden_2: 36, output: 2] on the engorged training set. Then for each prediction made by SVM and NN, in case the models disagree, whoever assigns the highest probability to his choice decides.</p>

<p>Run on the CV set we got the same result:</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: left"> </th>
      <th style="text-align: center">precision</th>
      <th style="text-align: center">recall</th>
      <th style="text-align: center">f1-score</th>
      <th style="text-align: right">support</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">label 0</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">149</td>
    </tr>
    <tr>
      <td style="text-align: left">label 1</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">151</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td style="text-align: left">avg / total</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: center">0.97</td>
      <td style="text-align: right">300</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<p>Both models differed on just 10 samples (3%) and in 5 cases the NN decided. The NN didn’t do better than the SVM, it just had a different opinion on some borderline cases.</p>

<h2 id="final-submission">final submission</h2>

<p>I have rerun my algorithm multiple times trying to find the best initial feature set and best randomized path that engorges the data. My last few consistent scores were between 95.45 and 95.827.</p>

<p>My final submissions came down to: </p>

<ul>
  <li>a newly distilled feature set of 20 features, </li>
  <li>tweaked hyper-parameters,</li>
  <li>the patience to run the engorgement algorithm until I got 97.66% on CV</li>
  <li>8,808 training records guessed with p&gt;0.99 and 2,635 support vectors</li>
  <li>and using the mix model</li>
</ul>

<p>which achieves a 95.566% (a tad better than the SVM alone). </p>

<p>Two days back, out of chance with the same mix strategy on a randomly optimized set I got my best (though I suspect overfit) result:</p>

<blockquote><p>Kaggle&#8217;s position 9 with 95.827% accuracy.</p></blockquote>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/22/Neurotic-Titanic/">Neurotic Titanic</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-22T21:02:00-07:00" pubdate data-updated="true">Oct 22<span>nd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I have spent the last 13th submissions to beat my previous position (77<sup>th</sup> with 81.81% accuracy) without result. I tried concocting and adding new features, combining them in weird ways, estimating the missing values to the highest accuracy possible. All to no avail.</p>

<p>I have tried not only screwing with the SVM, but Neural Networks with the code of previous posts. The Neural Nets work great but tend to overfit tremendously. I get 84-88% in CV only to get 75-80% on the Kaggle test set. The overfitting got me thinking about ways to simplify so the model could generalize better. This is what I have learn:</p>

<h2 id="less-features-are-better-than-more">less features are better than more</h2>

<p>From my previous like 10 features I have realized that we get the same results with less. Actually, the title level alone will give you 79% accuracy on the cross validation set. Then you can experiment adding new features to see how well you do and realize that in order of incremental power come: family (as Parch + SibSp), Fare paid, Pclass, overseer, charisma (binAge * Pclass), Port_Q (embarked on Queenssomething), ….</p>

<p>After the first 5 you don’t add much more and a bit further you star to add noise and lose generalization power. This means that things like Age, Sex or Embarked port are not even considered!</p>

<h2 id="simpler-beats-complex-sometimes">simpler beats complex (sometimes)</h2>

<p>I tried to simplify as much as possible the features. First I only use 5 (‘title_level’, ‘family’, ‘Fare’, ‘Pclass’, ‘overseer’). Then I tried to simplify them:</p>

<p>I started estimating the correct Age (as bin) with a SVM and then with a Neural Network. The SVM got 68% right on CV, and the NN north of 90%. The problem is that Age is not key, the title already incorporates all the age information that is relevant. So out it went.</p>

<p>The same with Sex. Title already incorporates the info. Adding Sex, in this case, only confuses things.</p>

<p>On title I aggregate it to just [‘mr’, ‘mrs’, ‘mss’, ‘master’, ‘rev’, ‘major’, ‘sir’, ‘col’, ‘capt’, ‘the countess’]. The less the merrier.</p>

<p>Family went from a sum of relatives to 3 classes, [traveling alone, 1 relative, more than 1 relative on board].</p>

<p>Overseer, was a bit more built up. essentially for each passenger I look for other relatives and companions that survived. For women, if the survivor was older give 10 points, if not get just 1. For men, if the survivor was a man older by 20 years give 10 points, if not get 1. Any other way get -1. The idea already explained that in a crisis we gather with who we know, and that the elder look after the younger (up to a point).</p>

<p>By the way, I have been trying avoid 0 values, in the suspicion (material or not) that zeros are traps from which is difficult to escape (when most we do is multiply stuff).</p>

<h2 id="svm-or-nn--why-not-both">SVM or NN =&gt; why not both?</h2>

<p>With the above simplifications either SVM or NN got stuck under 80%. There was no way to move the dial so I tried a crazy approach: run them both (with optimized hyperparameters) and then:</p>

<ul>
  <li>commit when both agree,</li>
  <li>when they disagree on the outcome, choose the one with the highest assigned probability (yes, SVM can estimate the probability too!)</li>
</ul>

<p>Stupidly simple, and with great results. With my latest iteration, number 26, I landed on the 57 spot of 7,035 participants (as of now).</p>

<blockquote><p>Score on the test set (Kaggle): 82.775%.</p></blockquote>

<p>The NN alone (submission 27) is a big network with three hidden layers and overfits the data to death (Kaggle 80.38%). The SVM alone does not get past 80% either. But both of them working together work wonders. I am sure more could be extracted from this approach, but I need to get going and I should try a new competition and learn a new model.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/19/Neural-Net-Cost/">Neural Networks Theory: Backwards</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-19T09:02:00-07:00" pubdate data-updated="true">Oct 19<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="cost-function">cost function</h2>

<p>We use a generalization of the logistic regression cost function:</p>

<script type="math/tex; mode=display">
\begin{align}
J(\theta) = -\frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} (\log h_\theta(x^{(i)})) + (1-y^{(i)}) (\log (1-h_\theta(x^{(i)})))\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2 
\end{align}
\</script>

<p>To create our neurotic cost function:</p>

<script type="math/tex; mode=display">
\begin{align}
J(\theta) = -\frac{1}{m} \left[ \sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log (h_\Theta(x^{(i)}))_k + (1-y_k^{(i)}) \log (1-(h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m} \sum_{l=1}^{L-1} \sum_{i=1}^s_{l} \sum_{j=1}^{s_{l+1}} {\Theta_{ji}^{(l)}}^2
\end{align}
</script>

<p>In logit there was a single output unit, so the generalized for only adds a sum over the K output units of the layer. The new regularization component is just adding over all possible values of $\Theta_{ji}^{(l)}$ for each l, j and i. Also, like logit, it excludes all the parameters for bias units.</p>

<p>The intuition is simple, in a fast and dirty way, the cost of example i is ‘like’ the square error, or squared difference between what the network says and the reality dictates.</p>

<p>Finally, consider that the cost function is not convex, so we can get stuck.end up in a local minimum when optimizing the parameters. In practice this is not a big problem and the usual suspects algorithms will get to a very good local minimum even if not the global one.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>cost function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">j</span><span class="p">(</span><span class="n">thetas_rolled</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
</span><span class="line">    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span><span class="line">    <span class="n">thetas</span> <span class="o">=</span> <span class="n">unroll_thetas</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">thetas_rolled</span><span class="p">)</span>
</span><span class="line">    <span class="n">y_bloat</span> <span class="o">=</span> <span class="n">groundTruth</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">    <span class="n">a</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="mf">0.</span>
</span><span class="line">    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">thetas</span><span class="p">:</span>
</span><span class="line">        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">a</span><span class="p">])</span>
</span><span class="line">        <span class="n">a</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</span><span class="line">        <span class="n">r</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">t</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</span><span class="line">    <span class="n">J</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">y_bloat</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y_bloat</span><span class="p">)</span><span class="o">*</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">a</span><span class="p">))</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="o">*</span><span class="n">lam</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
</span><span class="line">    <span class="k">return</span> <span class="n">J</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="gradient">gradient</h2>

<p>So the idea is to find the parameters $\theta$ that minimize the cost function. Like before, we need to compute both the cost and the gradient at each iteration of an optimization algorithm.</p>

<p>The same way that $a_j^{(l)}$ is the activation (result, output) of node j in layer l, we are going to have a $\delta_j^{(l)}$, which will represent the “error” on the activation of that node, kind of like how much is the output off from where we needed it to be.</p>

<p>If we go all the way to the end, we do know that the final activation is the predicted h(x) and should be equal to the ‘true’ observed label y. So in a case with 4 layers (4<sup>th</sup>) layer is the output layer) we have, $\delta_j^{(4)} = a_j^{(4)} - y_j = (h_{\Theta}(x))_j - y_j$ or in vectorized form: $\delta^{(4)} = a^{(4)} - y$</p>

<p>Now we can go backwards and compute the deltas one layer at a time considering that g’ is the derivative of the activation function.</p>

<script type="math/tex; mode=display">
\begin{align}
\delta^{(3)} = (\Theta^{(3)})^T \delta^{(4)} .* g'(z^{(3)}) = (\Theta^{(3)})^T \delta^{(4)} .* (a^{(3)} .* (1-a^{(3)}))
\\
\delta^{(2)} = (\Theta^{(2)})^T \delta^{(3)} .* g'(z^{(2)}) = (\Theta^{(2)})^T \delta^{(3)} .* (a^{(2)} .* (1-a^{(2)}))
\end{align}
</script>

<p>There is no $\delta^{(1)}$ because the first layer has no errors, is the input.</p>

<p>In a sense, if we forward propagate the computation of the activations in each layer from the input forward, we back propagate the computations of the errors from the output backwards.</p>

<p>It is possible to demonstrate mathematically that the partial derivative of the cost function is equal to $a_j^{(l)} \delta_i^{(l+1)}$ when no considering regularization.</p>

<p>Intuition: Imagine a node A from which two activation outputs connect it to another two nodes B and C. The $\delta$ of A is computed as a weighted average of the $\delta$ of B and C weighted by the $\Theta$ that connect them to node A.</p>

<script type="math/tex; mode=display">
\delta_A \approx \Theta_{A \to B} \delta_B + \Theta_{A \to C} \delta_C
</script>

<h2 id="backpropagation-algorithm">backpropagation algorithm</h2>

<ul>
  <li>Set $\Delta_{ij}^{(l)}$ for all l, i, j, that we will use to later compute the gradient</li>
  <li>For each example:
    <ul>
      <li>set $a^{(1)} = x^{(i)}$</li>
      <li>forward propagate to compute all $a^{(l)}$</li>
      <li>backpropagate using first $y^{(i)}$ to compute all $\delta^{(l)}$ up to $\delta^{(2)}$</li>
      <li>compute the $\Delta$ updating with the partial derivative:</li>
    </ul>
  </li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
\Delta_{ij}^{(l)} &:= \Delta_{ij}^{(l)} + a_{j}^{(l)} \delta_{i}^{(l+1)}
\\
\Delta^{(l)} &:= \Delta^{(l)} + \delta^{(l+1)} (a^{(l)})^T
\end{align}
 %]]></script>

<ul>
  <li>Compute D terms that correspond to the gradient, partial derivative of $J(\Theta)$ now with regularization:</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
D_{ij}^{(l)} &:= \frac{1}{m} \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{(l)} \mbox{  if j!=0}
\\
D_{ij}^{(l)} &:= \frac{1}{m} \Delta_{ij}^{(l)} \mbox{  if j=0}
\end{align}
 %]]></script>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>gradient function and backpropagation</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">thetas_rolled</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
</span><span class="line">    <span class="n">m</span><span class="p">,</span><span class="n">n</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
</span><span class="line">    <span class="n">thetas</span> <span class="o">=</span> <span class="n">unroll_thetas</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">thetas_rolled</span><span class="p">)</span>
</span><span class="line">    <span class="n">num_thetas</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</span><span class="line">    <span class="n">y_bloat</span> <span class="o">=</span> <span class="n">groundTruth</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">    <span class="c"># forward =&gt; fill lists of &#39;activations&#39; and &#39;z&#39;</span>
</span><span class="line">    <span class="n">a</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_thetas</span><span class="p">):</span>
</span><span class="line">        <span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</span><span class="line">        <span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</span><span class="line">        <span class="n">a</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># backwards =&gt; fill list of deltas</span>
</span><span class="line">    <span class="n">d</span> <span class="o">=</span> <span class="p">[(</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">y_bloat</span><span class="p">)]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_thetas</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
</span><span class="line">        <span class="n">d</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">thetas</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">][:,</span><span class="mi">1</span><span class="p">:]))</span> <span class="o">*</span> <span class="n">sigmoid_gradient</span><span class="p">(</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># forward =&gt; compute gradient</span>
</span><span class="line">    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_thetas</span><span class="p">):</span>
</span><span class="line">        <span class="n">Vwip</span> <span class="o">=</span> <span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="n">lam</span><span class="o">*</span><span class="n">thetas</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="o">/</span><span class="n">m</span>
</span><span class="line">        <span class="n">Vwip</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">thetas</span><span class="p">[</span><span class="n">i</span><span class="p">][:,</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">        <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">V</span><span class="p">,</span> <span class="n">Vwip</span><span class="o">.</span><span class="n">flatten</span><span class="p">()])</span>
</span><span class="line">    <span class="k">return</span> <span class="n">V</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">optimizeThetas</span><span class="p">(</span><span class="n">tinit</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">j</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">fprime</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">v</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="p">[</span><span class="n">thetas</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">fmin_l_bfgs_b</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">tinit</span><span class="p">,</span> <span class="n">fprime</span><span class="o">=</span><span class="n">fprime</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">visual</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</span><span class="line">        <span class="k">print</span> <span class="n">f</span>
</span><span class="line">        <span class="k">print</span> <span class="n">d</span>
</span><span class="line">    <span class="k">return</span> <span class="n">thetas</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/18/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/16/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/03/01/Seattlerb-GoL/">Seattle Ruby Game Of Life</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/01/CodeFellows/">Article Published</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/15/Scheme-GOL/">Game of Life in Scheme</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/18/4-PSD/">4 Principles of Simple Design Reflections</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/17/Code-Retreat/">Code Retreat Reflections</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
