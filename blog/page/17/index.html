
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="Compared to both logistic regression and neural networks, a Support Vector Machine (SVM) sometimes gives a cleaner way for learning non-linear &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/17">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Developer of software, ideas, startups</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/08/SVM-Theory1/">SVM: Derivation From Logit</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-08T09:02:00-07:00" pubdate data-updated="true">Oct 8<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Compared to both logistic regression and neural networks, a Support Vector Machine (SVM) sometimes gives a cleaner way for learning non-linear functions.</p>

<p>The logistic regression hypothesis and cost functions for a single example are:</p>

<script type="math/tex; mode=display">
h_{\theta}(x) = \frac{1}{ 1  + e^{ -(\theta)^T x } } = g(z) \mbox{ where } z = \theta^T x
\\
J_{\theta}(x) = -(\log h_{\theta}(x) + (1-y) \log(1 - h-{\theta}(x)))
</script>

<p>Looking at the sigmoid activation function, how does logistic regression work?</p>

<p><img class="center" src="/images/oct13/sigmoid.png" width="400" height="400" title="sigmoid activation function" /></p>

<p>If we have an example x where y = 1, we hope that h(x) is close to 1. For that to happen $z = θ^T x$ must be much larger than 0. If we look at the cost function, we see that only the first term works (for y = 1).  So a single example with y = 1 will have a z » 0, and its cost contribution will be very small.</p>

<div style="text-align:center">
<img src="/images/oct13/term1.png" width="400" height="400" title="sigmoid activation function" />
<img src="/images/oct13/term2.png" width="400" height="400" title="sigmoid activation function" />
</div>

<p>Similarly if y = 0, we need h to be close to 0, for which z needs to be very negative. And in the cost function now only the second term is active. So a single example with y = 0 will have a z « 0, and its cost contribution will be very big.</p>

<p>SVM just takes the above cost functions and simplifies them with straight lines in the following manner:</p>

<div style="text-align:center">
<img src="/images/oct13/term1b.png" width="400" height="400" title="sigmoid activation function" />
<img src="/images/oct13/term2b.png" width="400" height="400" title="sigmoid activation function" />
</div>

<p>We call this simplified cost curves $cost<em>1$ and $cost</em>2$. The key is that SVM does not outputs probabilities like before, but still classifies pretty well. Actually, for learning non linearly, it gives us a cleaner and efficient approach.</p>

<p>So finding the parameters from the cost function in logit:</p>

<script type="math/tex; mode=display">
\begin{align}
min_{\theta} \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} (-\log h_\theta(x^{(i)})) + (1-y^{(i)}) (-\log (1-h_\theta(x^{(i)})))\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2 
\end{align}
</script>

<p>becomes:</p>

<script type="math/tex; mode=display">
\begin{align}
min_{\theta} \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2 
\end{align}
</script>

<p>Since we don’t care about probabilities, only classification results, as we search for the minimizing parameters we neither care about multiplying constants. So by convention, SVM notation:</p>

<ul>
  <li>gets rid of constant 1/m.</li>
  <li>use C = 1/$\lambda$ instead of $\lambda$.</li>
</ul>

<script type="math/tex; mode=display">
\begin{align}
J(\theta) = min_{\theta} C \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{1}{2} \sum_{i=1}^m \theta_j^2 
\end{align}
</script>

<p>So we have that if $\theta^T x &gt;= 1 =&gt; h_{\theta}(x) = 1$.</p>

<p>Consider that in logistic regression we were classifying as y = 1 if z was positive. SVM needs it to be &gt;= 1. In a sense, SVM wants to be more sure and moves the goalpost by adding an extra safety margin.</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/07/Kaggle-Digit-2-regress/">Kaggle Digit OCR: 2 Step Regression</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-07T09:02:00-07:00" pubdate data-updated="true">Oct 7<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Based on the previous posts we are going to extend our softmax model so we classify the handwritten images by using logistic regressions in two steps:</p>

<ol>
  <li>
    <p>First we run our Softmax (multivariate logistic regression) classifier as before.</p>
  </li>
  <li>
    <p>When classifying a particular image, if the first choice has less than 99% probability and the second choice has more than 1%, then we run a logistic regression between those two top choices to make sure we pick the right one. These limits are completely arbitrary, pretty irrelevant and stated to make the concept clearer.</p>
  </li>
</ol>

<p>This means computing the 784+1 parameters of the logystic regression 43 times, which is quite hair-raising but not heart-stopping (30 mins in my old mac).</p>

<p>So if it is very clear we go softmax, but if there is a doubt we rely on a more accurate logistic regression between the top two softmax choices. </p>

<blockquote><p>The new accuracy from Kaggle is 93.64%.</p></blockquote>

<p>Here is the modified code:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>kaggle 2 step regression model</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
<span class="line-number">79</span>
<span class="line-number">80</span>
<span class="line-number">81</span>
<span class="line-number">82</span>
<span class="line-number">83</span>
<span class="line-number">84</span>
<span class="line-number">85</span>
<span class="line-number">86</span>
<span class="line-number">87</span>
<span class="line-number">88</span>
<span class="line-number">89</span>
<span class="line-number">90</span>
<span class="line-number">91</span>
<span class="line-number">92</span>
<span class="line-number">93</span>
<span class="line-number">94</span>
<span class="line-number">95</span>
<span class="line-number">96</span>
<span class="line-number">97</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="kn">import</span> <span class="nn">sys</span>
</span><span class="line"><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="s">&#39;../../.&#39;</span> <span class="p">)</span> <span class="c"># regression modules address</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">Logysoft</span> <span class="kn">as</span> <span class="nn">soft</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">Logysterical</span> <span class="kn">as</span> <span class="nn">logit</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">KaggleTop2</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span> <span class="o">=</span> <span class="mi">10</span>              <span class="c"># N. of possible values of Y (labels)</span>
</span><span class="line">        <span class="c"># load data files</span>
</span><span class="line">        <span class="n">training_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/train.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">        <span class="n">testing_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/test.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">training_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">training_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</span><span class="line">
</span><span class="line">        <span class="c"># evaluation set</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">xe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="p">[</span><span class="mi">30000</span><span class="p">:,</span> <span class="p">:]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">ye</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">yt</span><span class="p">[</span><span class="mi">30000</span><span class="p">:,</span> <span class="p">:]</span>
</span><span class="line">
</span><span class="line">        <span class="c"># testing set</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">testing_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,:])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">optimize_logit_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="p">):</span>
</span><span class="line">        <span class="c"># extract the right images from the set</span>
</span><span class="line">        <span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
</span><span class="line">
</span><span class="line">        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/train.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="n">data_t</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:,</span> <span class="p">:]</span>
</span><span class="line">
</span><span class="line">        <span class="n">data_a</span> <span class="o">=</span> <span class="n">data_t</span><span class="p">[</span><span class="n">data_t</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">a</span><span class="p">]</span>
</span><span class="line">        <span class="n">data_b</span> <span class="o">=</span> <span class="n">data_t</span><span class="p">[</span><span class="n">data_t</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">b</span><span class="p">]</span>
</span><span class="line">        <span class="n">data_ab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">data_a</span><span class="p">,</span> <span class="n">data_b</span><span class="p">])</span>
</span><span class="line">        <span class="n">xt</span> <span class="o">=</span> <span class="n">data_ab</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
</span><span class="line">        <span class="n">yt</span> <span class="o">=</span> <span class="n">data_ab</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;int8&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="n">yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">yt</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span><span class="line">
</span><span class="line">        <span class="c"># Perform logistic regression</span>
</span><span class="line">        <span class="n">xt2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">xt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">xt</span><span class="p">])</span>
</span><span class="line">        <span class="n">yt2</span> <span class="o">=</span> <span class="n">yt</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span class="line">        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">yt2</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
</span><span class="line">            <span class="k">if</span> <span class="n">yt2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="n">a</span><span class="p">:</span>
</span><span class="line">                <span class="n">yt2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line">                <span class="n">count</span> <span class="o">+=</span><span class="mi">1</span>
</span><span class="line">            <span class="k">else</span><span class="p">:</span>
</span><span class="line">                <span class="n">yt2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">        <span class="n">ini_thetas</span> <span class="o">=</span> <span class="mf">0.005</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">xt2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">        <span class="n">L</span> <span class="o">=</span> <span class="mf">1e+5</span>
</span><span class="line">        <span class="n">opt_thetas</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">optimizeThetas</span><span class="p">(</span><span class="n">ini_thetas</span><span class="p">,</span> <span class="n">xt2</span><span class="p">,</span> <span class="n">yt2</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">opt_thetas</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">test_model_submit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">logit_thetas</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class="line">
</span><span class="line">        <span class="n">soft_thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/submit_optimized_thetas.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">))</span>
</span><span class="line">        <span class="n">soft_thetas</span> <span class="o">=</span> <span class="n">soft_thetas</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span>
</span><span class="line">        <span class="n">h</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">h</span><span class="p">(</span><span class="n">soft_thetas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span><span class="p">)</span>
</span><span class="line">        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</span><span class="line">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">            <span class="p">[</span><span class="n">ml_1</span><span class="p">,</span> <span class="n">ml_2</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">:][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c"># 1st and 2nd model choices</span>
</span><span class="line">            <span class="n">p1</span><span class="p">,</span><span class="n">p2</span> <span class="o">=</span> <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,:][</span><span class="n">ml_1</span><span class="p">],</span> <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,:][</span><span class="n">ml_2</span><span class="p">]</span>
</span><span class="line">            <span class="n">right_order</span> <span class="o">=</span> <span class="bp">True</span>
</span><span class="line">            <span class="k">if</span> <span class="n">ml_1</span> <span class="o">&gt;</span> <span class="n">ml_2</span><span class="p">:</span>
</span><span class="line">                <span class="n">right_order</span> <span class="o">=</span> <span class="bp">False</span>
</span><span class="line">                <span class="n">s</span> <span class="o">=</span> <span class="sb">`ml_2`</span><span class="o">+</span><span class="sb">`ml_1`</span>
</span><span class="line">            <span class="k">else</span><span class="p">:</span>
</span><span class="line">                <span class="n">s</span> <span class="o">=</span> <span class="sb">`ml_1`</span><span class="o">+</span><span class="sb">`ml_2`</span>
</span><span class="line">
</span><span class="line">            <span class="k">if</span> <span class="n">p1</span><span class="o">&lt;</span><span class="mf">0.99</span> <span class="ow">and</span> <span class="n">p2</span><span class="o">&gt;</span><span class="mf">0.01</span><span class="p">:</span>
</span><span class="line">                <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">logit_thetas</span><span class="p">:</span>
</span><span class="line">                    <span class="n">logit_thetas</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimize_logit_for</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">                <span class="n">l_t</span> <span class="o">=</span> <span class="n">logit_thetas</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
</span><span class="line">                <span class="n">logix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">,:]])</span>
</span><span class="line">
</span><span class="line">                <span class="n">p</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">h</span><span class="p">(</span><span class="n">l_t</span><span class="p">,</span> <span class="n">logix</span><span class="p">)</span>
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">):</span>
</span><span class="line">                    <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">([</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_1</span><span class="p">]</span> <span class="k">if</span> <span class="n">right_order</span> <span class="k">else</span> <span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_2</span><span class="p">])</span>
</span><span class="line">                <span class="k">else</span><span class="p">:</span>
</span><span class="line">                    <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">([</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_2</span><span class="p">]</span> <span class="k">if</span> <span class="n">right_order</span> <span class="k">else</span> <span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_1</span><span class="p">])</span>
</span><span class="line">            <span class="k">else</span><span class="p">:</span>
</span><span class="line">                <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_1</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">        <span class="k">print</span> <span class="s">&#39;To submitt add header: ImageId,Label&#39;</span>
</span><span class="line">        <span class="k">print</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">,:]</span>
</span><span class="line">        <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s">&#39;./data/kaggle/predictions_2steps.csv&#39;</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">&#39;</span><span class="si">%i</span><span class="s">,</span><span class="si">%i</span><span class="s">&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="k">pass</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">k2t</span> <span class="o">=</span> <span class="n">KaggleTop2</span><span class="p">()</span>
</span><span class="line"><span class="n">k2t</span><span class="o">.</span><span class="n">test_model_submit</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="conclusion">conclusion</h2>

<p>We have gained a small improvement of 1.6%. The right direction, yet far away from the results achieved with other models. This seems to suggest a limitation for this problem of a learning function that is linear. In future posts I’ll look for models that rely on non-linear functions.</p>

<p>I also wonder what would be the result of using as model logistic regression one-vs-all for each label. And how it compares to plain softmax.</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/06/OCR-Analaysis/">Kaggle Digit OCR: Softmax Analysis</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-06T09:03:00-07:00" pubdate data-updated="true">Oct 6<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Reading this <a href="http://peekaboo-vision.blogspot.com/2012/12/another-look-at-mnist.html">post</a> I realized that we may be able to use a simple logistic regression on a 1-to-1 basis for those cases the softmax model gets wrong. </p>

<p>We extract the 970 images that we misclassified (out of the 12.000 of our validation set). Then for each image we compute the hypothesis function of each image using the optimized parameters (previously stored). </p>

<p>Let’s get as an example the 42<sup>nd</sup> image from the evaluation set:</p>

<p><img class="center" src="/images/kaggle_ocr/xe_42.png" width="400" height="400" title="Seven" /></p>

<p>The hypothesis function of the image outputs the following probabilities for each label (0 to 9) that we have multiplied by 100: <code>[43.79, 7.6e-09, 0.37, 0.0008, 0.12, 0.44, 55.11, 0.02, 0.09, 0.02]</code></p>

<p>Our model mis classifies it as a 6 (highest probability of 55%) but it is really meant to be a 0. Now, realize that the second choice, the second maximum is for label 0 (with 43% probability), the right one!. Furthermore, the second choice is at a big distance from everyone else (less than 1%). So even if softmax failed, it was pretty close.</p>

<p>If we could pick and choose the cases where first and second choice were close by, we could tell the model to disregard the softmax choice and instead, for that particular case, perform a logistic regression between the two choices. Generalized to all possible combinations of first and second choices we would need to compute $(n-1)n/2 = 45$ times the 784 parameters.</p>

<p>We can derive from the data that, of the 970 misclassified images on the evaluation set, 608 (or 62%) happen to have as second choice the true class. For example, the most frequent misclassification in this later group happens differentiating between 7 and 9, (69 times out of 608).</p>

<p>Let’s make a small experiment and see if it is true that logit works better than softmax when figuring 7s and 9s. We,</p>

<ul>
  <li>extract from the training and evaluation test all images whose true label is 7 or 9. </li>
  <li>derive the opt thetas for a simple logistic regression from the extracted training set.</li>
  <li>compare on the extracted evaluation set the accuracy of both, softmax and logistic regression.</li>
</ul>

<p>Softmax gets it right 90.8% and logit 95.8%. Lets repeat it for the most conflicting pairs previously identified:</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: center">pairs</th>
      <th style="text-align: center">count</th>
      <th style="text-align: center">softmax_acc(%)</th>
      <th style="text-align: center">logit_acc(%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">7 vs. 9</td>
      <td style="text-align: center">69</td>
      <td style="text-align: center">90.8</td>
      <td style="text-align: center">95.8</td>
    </tr>
    <tr>
      <td style="text-align: center">4 vs. 9</td>
      <td style="text-align: center">53</td>
      <td style="text-align: center">91.3</td>
      <td style="text-align: center">96.8</td>
    </tr>
    <tr>
      <td style="text-align: center">5 vs. 8</td>
      <td style="text-align: center">46</td>
      <td style="text-align: center">86.6</td>
      <td style="text-align: center">96.5</td>
    </tr>
    <tr>
      <td style="text-align: center">3 vs. 5</td>
      <td style="text-align: center">40</td>
      <td style="text-align: center">87.6</td>
      <td style="text-align: center">96.0</td>
    </tr>
    <tr>
      <td style="text-align: center">1 vs. 8</td>
      <td style="text-align: center">37</td>
      <td style="text-align: center">93.5</td>
      <td style="text-align: center">98.4</td>
    </tr>
    <tr>
      <td style="text-align: center">2 vs. 8</td>
      <td style="text-align: center">33</td>
      <td style="text-align: center">89.4</td>
      <td style="text-align: center">97.7</td>
    </tr>
    <tr>
      <td style="text-align: center">2 vs. 3</td>
      <td style="text-align: center">32</td>
      <td style="text-align: center">90.2</td>
      <td style="text-align: center">97.3</td>
    </tr>
    <tr>
      <td style="text-align: center">3 vs. 8</td>
      <td style="text-align: center">29</td>
      <td style="text-align: center">89.6</td>
      <td style="text-align: center">97.1</td>
    </tr>
    <tr>
      <td style="text-align: center">2 vs. 7</td>
      <td style="text-align: center">24</td>
      <td style="text-align: center">91.0</td>
      <td style="text-align: center">98.7</td>
    </tr>
    <tr>
      <td style="text-align: center">5 vs. 6</td>
      <td style="text-align: center">21</td>
      <td style="text-align: center">90.2</td>
      <td style="text-align: center">97.8</td>
    </tr>
    <tr>
      <td style="text-align: center">0 vs. 6</td>
      <td style="text-align: center">19</td>
      <td style="text-align: center">95.8</td>
      <td style="text-align: center">98.6</td>
    </tr>
    <tr>
      <td style="text-align: center">3 vs. 9</td>
      <td style="text-align: center">19</td>
      <td style="text-align: center">90.0</td>
      <td style="text-align: center">98.5</td>
    </tr>
    <tr>
      <td style="text-align: center">2 vs. 6</td>
      <td style="text-align: center">18</td>
      <td style="text-align: center">92.9</td>
      <td style="text-align: center">98.5</td>
    </tr>
    <tr>
      <td style="text-align: center">0 vs. 5</td>
      <td style="text-align: center">15</td>
      <td style="text-align: center">90.4</td>
      <td style="text-align: center">98.4</td>
    </tr>
    <tr>
      <td style="text-align: center">0 vs. 2</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">93.0</td>
      <td style="text-align: center">98.5</td>
    </tr>
    <tr>
      <td style="text-align: center">3 vs. 7</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">91.2</td>
      <td style="text-align: center">98.4</td>
    </tr>
    <tr>
      <td style="text-align: center">5 vs. 9</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">87.1</td>
      <td style="text-align: center">98.5</td>
    </tr>
    <tr>
      <td style="text-align: center">4 vs. 6</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">94.4</td>
      <td style="text-align: center">99.0</td>
    </tr>
    <tr>
      <td style="text-align: center">8 vs. 9</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">89.2</td>
      <td style="text-align: center">98.7</td>
    </tr>
    <tr>
      <td style="text-align: center">4 vs. 5</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">88.8</td>
      <td style="text-align: center">98.6</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<p>Here is the module for logistic regression:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>logit module</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">scipy</span> <span class="kn">as</span> <span class="nn">sc</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin_l_bfgs_b</span>
</span><span class="line">
</span><span class="line"><span class="c"># MODULE FOR LOGISTIC REGRESSION</span>
</span><span class="line"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1E-11</span><span class="p">))(</span><span class="n">z</span><span class="p">)</span>
</span><span class="line">    <span class="c">#return np.vectorize(lambda x: 1/(1+np.exp(-x)))(z)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;hypothesis function. probability of input x with params t&#39;&#39;&#39;</span>
</span><span class="line">    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">j</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;cost function J(theta)&#39;&#39;&#39;</span>
</span><span class="line">    <span class="n">prediction</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
</span><span class="line">    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">J</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prediction</span><span class="p">))</span> <span class="o">-</span>        \
</span><span class="line">            <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">prediction</span><span class="p">))</span> <span class="o">+</span>    \
</span><span class="line">            <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">::],</span> <span class="mi">2</span><span class="p">)))</span><span class="o">/</span><span class="n">m</span>
</span><span class="line">    <span class="k">return</span> <span class="n">J</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;gradient function, first partial derivation of J(theta)&#39;&#39;&#39;</span>
</span><span class="line">    <span class="n">prediction</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
</span><span class="line">    <span class="n">regu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">::,]</span><span class="o">*</span><span class="n">lam</span><span class="p">])</span>
</span><span class="line">    <span class="n">grad</span> <span class="o">=</span> <span class="p">((</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">regu</span><span class="p">)</span><span class="o">/</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="k">return</span> <span class="n">grad</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">optimizeThetas</span><span class="p">(</span><span class="n">tinit</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;derive thetas using l_bfgs algorithm&#39;&#39;&#39;</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">j</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">fprime</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">v</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span>
</span><span class="line">    <span class="p">[</span><span class="n">thetas</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">fmin_l_bfgs_b</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">tinit</span><span class="p">,</span> <span class="n">fprime</span><span class="o">=</span><span class="n">fprime</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">visual</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</span><span class="line">        <span class="k">print</span> <span class="n">f</span>
</span><span class="line">        <span class="k">print</span> <span class="n">d</span>
</span><span class="line">    <span class="k">return</span> <span class="n">thetas</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="n">acc</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span class="line">    <span class="n">m</span><span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">        <span class="n">p</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,::])</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">&gt;</span><span class="mf">0.5</span> <span class="ow">and</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">p</span><span class="o">&lt;=</span><span class="mf">0.5</span> <span class="ow">and</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">):</span>
</span><span class="line">                <span class="n">acc</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">    <span class="k">return</span> <span class="n">acc</span><span class="o">/</span><span class="n">m</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/18/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/16/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/10/29/Rails-API-Versioning/">Rails Api Versioning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/27/Wire-Angular-Rails/">Wiring Angular into Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/24/Binary-Tree-Meta/">Ruby Metaprogramming Frolicking</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/20/JS-review/">JavaScript OO Review</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/20/Octopress-GH/">Installing Octopress at github.io</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
