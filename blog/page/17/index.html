
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="Let’s start pointing to the best tutorial I have found in the Internet “A Gentle Introduction to Support Vector Machines in Biomedicine” by Alexander &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/17">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Developer of software, ideas, startups</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/09/SVM-C/">SVM: Graphical Intuition</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-09T10:02:00-07:00" pubdate data-updated="true">Oct 9<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Let’s start pointing to the best tutorial I have found in the Internet “<a href="http://webdoc.nyumc.org/nyumc/files/chibi/user-content/Final.pdf">A Gentle Introduction to Support Vector Machines in Biomedicine</a>” by Alexander Statnikov, Douglas Hardin, Isabelle Guyon and Constantin F. Aliferis.</p>

<p>And without much ado, lets begin by simplifying things. We are going to work on two dimensions so we can visualize better how SVM works. </p>

<h2 id="the-simplest-analogy">the simplest analogy</h2>

<p>Consider that we want to classify object as houses or boats. We are given data that is represented in the pictures below. We know that all the green boxed objects are houses, and all the red circles are boats. That is our training data. Then we can ‘draw’ the shoreline with a yellow line as we realize that houses are on land (on one side of the shoreline) and boats are usually on water (on the other side of the decision boundary). The shoreline is our decision boundary, the line that separates one type of objects from the other. New objects will be asked on which side of the line they are, and depending on the answer they will be classified as boats or houses. Caveat: this is not foolproof since, for example, small catamarans could be moored on the sand and would be misclassified as houses.</p>

<p><img src="/images/oct13/house_boat_1.png" width="410" title="classifying houses vs boats" />
<img src="/images/oct13/house_boat_2.png" width="410" title="classifying houses vs boats" /></p>

<h2 id="decision-boundary-in-svm">decision boundary in svm</h2>

<p>We start with a set of points and their correct labels (north and south koreans in the border). A support vector machine constructs a decision boundary (hyper-plane or set of hyper-planes in a high or infinite dimensional space), which separate different classes. </p>

<p><img src="/images/oct13/korea_0.png" width="410" title="korean buffer zone" />
<img src="/images/oct13/korea_3.png" width="410" title="one possible boundary" />
<img src="/images/oct13/korea_1.png" width="410" title="a better boundary" />
<img src="/images/oct13/korea_2.png" width="410" title="choosing the boundary" /></p>

<p>If we need to choose a border that separates the armies of both Koreas, both lines (green and black) are valid. Nevertheless, we have the intuition that the black one will give us the most robust one. Not only because we can grasp the importance of a buffer zone between the armies, but because given a new point to classify, the black boundary seems to work better (‘generalize’). Formally expressed, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since <strong>in general the larger the margin the lower the generalization error</strong>.</p>

<p>From before we are looking to solve :</p>

<script type="math/tex; mode=display">
\begin{align}
min_{\theta} C \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{1}{2} \sum_{i=1}^m \theta_j^2 = min_{\theta} (C A + B)
\end{align}
</script>

<p>As we will see soon, C influences the size of the margin. A large C is good (extra safety and distance). But as it grows, it is more sensitive to outliers (it will include them all), which may be an undesired effect. A low C makes the decision surface simpler and smoother, while a high C aims at classifying all training examples correctly. </p>

<h2 id="now-for-something-entirely-different-vectors">Now, for something entirely different: vectors…</h2>

<p>In 2D, we have two vectors u and v stated as:</p>

<script type="math/tex; mode=display">
u = 
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix}
\mbox{ and }
v = 
\begin{bmatrix}
v_1 \\
v_2
\end{bmatrix}
</script>

<p>where the u1 and u2 are the u’s components or projections along the axis. The length of u is also called the norm of u or $||u||$, and can derived from Pythagoras’ theorem. </p>

<p>Let’s define also the projection of v onto u, what we call ‘p’, and which is the length of the shadow of v over u if the light is perpendicular to u. p is the projection of v onto u. It is a scalar, not a vector. </p>

<p>The inner product of u and v is:</p>

<script type="math/tex; mode=display">u^T . v = p . \|u\| = u_1.v_1 + u_2.v_2 = v^T . u = p^{'} . \|v\|</script>

<h2 id="and-back-to-margins">…and back to margins</h2>

<p>As we were saying, we needed to minimize the cost function as in:</p>

<script type="math/tex; mode=display">
\begin{align}
min_{\theta} C \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{1}{2} \sum_{i=1}^m \theta_j^2 = min_{\theta} (C A + B)
\end{align}
</script>

<p>Let’s imagine we choose a huge C = 100.000. To minimize that sausage we’ll need to make A as small as possible. Actually, we know that A = 0 if we can find parameters that perfectly comply with:</p>

<script type="math/tex; mode=display">% <![CDATA[

\theta^T x^{(i)} >= 1 \mbox{ if } y^{(i)} = 1
\\
\theta^T x^{(i)} <= -1 \mbox{ if } y^{(i)} = 0 
 %]]></script>

<p>In that case, because A==0, our minimization problem will have become:</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
min_{\theta} \sum_{i=1}^m \theta_j^2
\\
\theta^T x^{(i)} >= 1 \mbox{ if } y^{(i)} = 1
\\
\theta^T x^{(i)} <= -1 \mbox{ if } y^{(i)} = 0 
\end{align}
 %]]></script>

<p>For clarity we are going to simplify our conditions by considering $\theta_0 = 0$ which means that the decision boundary crosses the origin and that the $\theta$ vector also starts at the origin. Also, since we are in 2D, our vectors have only two features.</p>

<script type="math/tex; mode=display">
\begin{align}
min_{\theta} (\theta_1^2 + \theta_1^2) = \|\theta\|_2
\end{align}
</script>

<p>So all SVM is doing is minimizing the length of the parameter vector.</p>

<p>If we plot an example $x^{(i)}$ in its 2D space we can see that it can be expressed as a point in the space or as a vector. Since we also have the two components of $\theta$ we can also plot it as a vector. Then, from above, the inner product $\theta^T x^{(i)} = p^{(i)} ||\theta|| = \theta^1 x<em>1^{(i)} + \theta^2 x</em>2^{(i)}$ where p is the projection of $x^{(i)}$ into $\theta$.</p>

<p>So our optimization objective becomes:</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
min_{\theta} \|\theta\|^2
\\
p^{(i)} \|\theta\| >= 1 \mbox{ if } y^{(i)} = 1
\\
p^{(i)} \|\theta\| <= -1 \mbox{ if } y^{(i)} = 0 
\end{align}
 %]]></script>

<p>Article of faith: The $\theta$ vector is perpendicular to the decision boundary that separates the two classes. The idea is then, which $\theta$, or which decision boundary works best?</p>

<p>The simplification of $\theta_0 = 0$ only means that the decision boundary (and also the $\theta$ vector) passes through the origin.</p>

<p>DEVELOPING……</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/08/SVM-Theory1/">SVM: Derivation From Logit</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-08T09:02:00-07:00" pubdate data-updated="true">Oct 8<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Compared to both logistic regression and neural networks, a Support Vector Machine (SVM) sometimes gives a cleaner way for learning non-linear functions.</p>

<p>The logistic regression hypothesis and cost functions for a single example are:</p>

<script type="math/tex; mode=display">
h_{\theta}(x) = \frac{1}{ 1  + e^{ -(\theta)^T x } } = g(z) \mbox{ where } z = \theta^T x
\\
J_{\theta}(x) = -(\log h_{\theta}(x) + (1-y) \log(1 - h-{\theta}(x)))
</script>

<p>Looking at the sigmoid activation function, how does logistic regression work?</p>

<p><img class="center" src="/images/oct13/sigmoid.png" width="400" height="400" title="sigmoid activation function" /></p>

<p>If we have an example x where y = 1, we hope that h(x) is close to 1. For that to happen $z = θ^T x$ must be much larger than 0. If we look at the cost function, we see that only the first term works (for y = 1).  So a single example with y = 1 will have a z » 0, and its cost contribution will be very small.</p>

<div style="text-align:center">
<img src="/images/oct13/term1.png" width="400" height="400" title="sigmoid activation function" />
<img src="/images/oct13/term2.png" width="400" height="400" title="sigmoid activation function" />
</div>

<p>Similarly if y = 0, we need h to be close to 0, for which z needs to be very negative. And in the cost function now only the second term is active. So a single example with y = 0 will have a z « 0, and its cost contribution will be very big.</p>

<p>SVM just takes the above cost functions and simplifies them with straight lines in the following manner:</p>

<div style="text-align:center">
<img src="/images/oct13/term1b.png" width="400" height="400" title="sigmoid activation function" />
<img src="/images/oct13/term2b.png" width="400" height="400" title="sigmoid activation function" />
</div>

<p>We call this simplified cost curves $cost<em>1$ and $cost</em>2$. The key is that SVM does not outputs probabilities like before, but still classifies pretty well. Actually, for learning non linearly, it gives us a cleaner and efficient approach.</p>

<p>So finding the parameters from the cost function in logit:</p>

<script type="math/tex; mode=display">
\begin{align}
min_{\theta} \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} (-\log h_\theta(x^{(i)})) + (1-y^{(i)}) (-\log (1-h_\theta(x^{(i)})))\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2 
\end{align}
</script>

<p>becomes:</p>

<script type="math/tex; mode=display">
\begin{align}
min_{\theta} \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{\lambda}{2m} \sum_{i=1}^m \theta_j^2 
\end{align}
</script>

<p>Since we don’t care about probabilities, only classification results, as we search for the minimizing parameters we neither care about multiplying constants. So by convention, SVM notation:</p>

<ul>
  <li>gets rid of constant 1/m.</li>
  <li>use C = 1/$\lambda$ instead of $\lambda$.</li>
</ul>

<script type="math/tex; mode=display">
\begin{align}
J(\theta) = min_{\theta} C \left[ \sum_{i=1}^m y^{(i)} cost_1(z^{(i)}) + (1-y^{(i)}) cost_0(z^{(i)})\right] + \frac{1}{2} \sum_{i=1}^m \theta_j^2 
\end{align}
</script>

<p>So we have that if $\theta^T x &gt;= 1 =&gt; h_{\theta}(x) = 1$.</p>

<p>Consider that in logistic regression we were classifying as y = 1 if z was positive. SVM needs it to be &gt;= 1. In a sense, SVM wants to be more sure and moves the goalpost by adding an extra safety margin.</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/07/Kaggle-Digit-2-regress/">Kaggle Digit OCR: 2 Step Regression</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-07T09:02:00-07:00" pubdate data-updated="true">Oct 7<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Based on the previous posts we are going to extend our softmax model so we classify the handwritten images by using logistic regressions in two steps:</p>

<ol>
  <li>
    <p>First we run our Softmax (multivariate logistic regression) classifier as before.</p>
  </li>
  <li>
    <p>When classifying a particular image, if the first choice has less than 99% probability and the second choice has more than 1%, then we run a logistic regression between those two top choices to make sure we pick the right one. These limits are completely arbitrary, pretty irrelevant and stated to make the concept clearer.</p>
  </li>
</ol>

<p>This means computing the 784+1 parameters of the logystic regression 43 times, which is quite hair-raising but not heart-stopping (30 mins in my old mac).</p>

<p>So if it is very clear we go softmax, but if there is a doubt we rely on a more accurate logistic regression between the top two softmax choices. </p>

<blockquote><p>The new accuracy from Kaggle is 93.64%.</p></blockquote>

<p>Here is the modified code:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>kaggle 2 step regression model</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
<span class="line-number">53</span>
<span class="line-number">54</span>
<span class="line-number">55</span>
<span class="line-number">56</span>
<span class="line-number">57</span>
<span class="line-number">58</span>
<span class="line-number">59</span>
<span class="line-number">60</span>
<span class="line-number">61</span>
<span class="line-number">62</span>
<span class="line-number">63</span>
<span class="line-number">64</span>
<span class="line-number">65</span>
<span class="line-number">66</span>
<span class="line-number">67</span>
<span class="line-number">68</span>
<span class="line-number">69</span>
<span class="line-number">70</span>
<span class="line-number">71</span>
<span class="line-number">72</span>
<span class="line-number">73</span>
<span class="line-number">74</span>
<span class="line-number">75</span>
<span class="line-number">76</span>
<span class="line-number">77</span>
<span class="line-number">78</span>
<span class="line-number">79</span>
<span class="line-number">80</span>
<span class="line-number">81</span>
<span class="line-number">82</span>
<span class="line-number">83</span>
<span class="line-number">84</span>
<span class="line-number">85</span>
<span class="line-number">86</span>
<span class="line-number">87</span>
<span class="line-number">88</span>
<span class="line-number">89</span>
<span class="line-number">90</span>
<span class="line-number">91</span>
<span class="line-number">92</span>
<span class="line-number">93</span>
<span class="line-number">94</span>
<span class="line-number">95</span>
<span class="line-number">96</span>
<span class="line-number">97</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="kn">import</span> <span class="nn">sys</span>
</span><span class="line"><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="s">&#39;../../.&#39;</span> <span class="p">)</span> <span class="c"># regression modules address</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">Logysoft</span> <span class="kn">as</span> <span class="nn">soft</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">Logysterical</span> <span class="kn">as</span> <span class="nn">logit</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">KaggleTop2</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span> <span class="o">=</span> <span class="mi">10</span>              <span class="c"># N. of possible values of Y (labels)</span>
</span><span class="line">        <span class="c"># load data files</span>
</span><span class="line">        <span class="n">training_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/train.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">        <span class="n">testing_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/test.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">training_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">training_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</span><span class="line">
</span><span class="line">        <span class="c"># evaluation set</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">xe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="p">[</span><span class="mi">30000</span><span class="p">:,</span> <span class="p">:]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">ye</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">yt</span><span class="p">[</span><span class="mi">30000</span><span class="p">:,</span> <span class="p">:]</span>
</span><span class="line">
</span><span class="line">        <span class="c"># testing set</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">testing_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,:])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">optimize_logit_for</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pair</span><span class="p">):</span>
</span><span class="line">        <span class="c"># extract the right images from the set</span>
</span><span class="line">        <span class="p">[</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">int</span><span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
</span><span class="line">
</span><span class="line">        <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/train.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="n">data_t</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:,</span> <span class="p">:]</span>
</span><span class="line">
</span><span class="line">        <span class="n">data_a</span> <span class="o">=</span> <span class="n">data_t</span><span class="p">[</span><span class="n">data_t</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">a</span><span class="p">]</span>
</span><span class="line">        <span class="n">data_b</span> <span class="o">=</span> <span class="n">data_t</span><span class="p">[</span><span class="n">data_t</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">b</span><span class="p">]</span>
</span><span class="line">        <span class="n">data_ab</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">data_a</span><span class="p">,</span> <span class="n">data_b</span><span class="p">])</span>
</span><span class="line">        <span class="n">xt</span> <span class="o">=</span> <span class="n">data_ab</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
</span><span class="line">        <span class="n">yt</span> <span class="o">=</span> <span class="n">data_ab</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;int8&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="n">yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">yt</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</span><span class="line">
</span><span class="line">        <span class="c"># Perform logistic regression</span>
</span><span class="line">        <span class="n">xt2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">xt</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="mi">1</span><span class="p">)),</span> <span class="n">xt</span><span class="p">])</span>
</span><span class="line">        <span class="n">yt2</span> <span class="o">=</span> <span class="n">yt</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span><span class="line">        <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">yt2</span><span class="o">.</span><span class="n">size</span><span class="p">):</span>
</span><span class="line">            <span class="k">if</span> <span class="n">yt2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="n">a</span><span class="p">:</span>
</span><span class="line">                <span class="n">yt2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line">                <span class="n">count</span> <span class="o">+=</span><span class="mi">1</span>
</span><span class="line">            <span class="k">else</span><span class="p">:</span>
</span><span class="line">                <span class="n">yt2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">
</span><span class="line">        <span class="n">ini_thetas</span> <span class="o">=</span> <span class="mf">0.005</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">xt2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">        <span class="n">L</span> <span class="o">=</span> <span class="mf">1e+5</span>
</span><span class="line">        <span class="n">opt_thetas</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">optimizeThetas</span><span class="p">(</span><span class="n">ini_thetas</span><span class="p">,</span> <span class="n">xt2</span><span class="p">,</span> <span class="n">yt2</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</span><span class="line">        <span class="k">return</span> <span class="n">opt_thetas</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">test_model_submit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="n">logit_thetas</span> <span class="o">=</span> <span class="p">{}</span>
</span><span class="line">
</span><span class="line">        <span class="n">soft_thetas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/submit_optimized_thetas.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">))</span>
</span><span class="line">        <span class="n">soft_thetas</span> <span class="o">=</span> <span class="n">soft_thetas</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span>
</span><span class="line">        <span class="n">h</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">h</span><span class="p">(</span><span class="n">soft_thetas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span><span class="p">)</span>
</span><span class="line">        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</span><span class="line">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">            <span class="p">[</span><span class="n">ml_1</span><span class="p">,</span> <span class="n">ml_2</span><span class="p">]</span> <span class="o">=</span> <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">:][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c"># 1st and 2nd model choices</span>
</span><span class="line">            <span class="n">p1</span><span class="p">,</span><span class="n">p2</span> <span class="o">=</span> <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,:][</span><span class="n">ml_1</span><span class="p">],</span> <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,:][</span><span class="n">ml_2</span><span class="p">]</span>
</span><span class="line">            <span class="n">right_order</span> <span class="o">=</span> <span class="bp">True</span>
</span><span class="line">            <span class="k">if</span> <span class="n">ml_1</span> <span class="o">&gt;</span> <span class="n">ml_2</span><span class="p">:</span>
</span><span class="line">                <span class="n">right_order</span> <span class="o">=</span> <span class="bp">False</span>
</span><span class="line">                <span class="n">s</span> <span class="o">=</span> <span class="sb">`ml_2`</span><span class="o">+</span><span class="sb">`ml_1`</span>
</span><span class="line">            <span class="k">else</span><span class="p">:</span>
</span><span class="line">                <span class="n">s</span> <span class="o">=</span> <span class="sb">`ml_1`</span><span class="o">+</span><span class="sb">`ml_2`</span>
</span><span class="line">
</span><span class="line">            <span class="k">if</span> <span class="n">p1</span><span class="o">&lt;</span><span class="mf">0.99</span> <span class="ow">and</span> <span class="n">p2</span><span class="o">&gt;</span><span class="mf">0.01</span><span class="p">:</span>
</span><span class="line">                <span class="k">if</span> <span class="n">s</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">logit_thetas</span><span class="p">:</span>
</span><span class="line">                    <span class="n">logit_thetas</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimize_logit_for</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">                <span class="n">l_t</span> <span class="o">=</span> <span class="n">logit_thetas</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
</span><span class="line">                <span class="n">logix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span><span class="p">[</span><span class="n">i</span><span class="p">,:]])</span>
</span><span class="line">
</span><span class="line">                <span class="n">p</span> <span class="o">=</span> <span class="n">logit</span><span class="o">.</span><span class="n">h</span><span class="p">(</span><span class="n">l_t</span><span class="p">,</span> <span class="n">logix</span><span class="p">)</span>
</span><span class="line">                <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">):</span>
</span><span class="line">                    <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">([</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_1</span><span class="p">]</span> <span class="k">if</span> <span class="n">right_order</span> <span class="k">else</span> <span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_2</span><span class="p">])</span>
</span><span class="line">                <span class="k">else</span><span class="p">:</span>
</span><span class="line">                    <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">([</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_2</span><span class="p">]</span> <span class="k">if</span> <span class="n">right_order</span> <span class="k">else</span> <span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_1</span><span class="p">])</span>
</span><span class="line">            <span class="k">else</span><span class="p">:</span>
</span><span class="line">                <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ml_1</span><span class="p">]</span>
</span><span class="line">
</span><span class="line">        <span class="k">print</span> <span class="s">&#39;To submitt add header: ImageId,Label&#39;</span>
</span><span class="line">        <span class="k">print</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">,:]</span>
</span><span class="line">        <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s">&#39;./data/kaggle/predictions_2steps.csv&#39;</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">&#39;</span><span class="si">%i</span><span class="s">,</span><span class="si">%i</span><span class="s">&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="k">pass</span>
</span><span class="line">
</span><span class="line">
</span><span class="line"><span class="n">k2t</span> <span class="o">=</span> <span class="n">KaggleTop2</span><span class="p">()</span>
</span><span class="line"><span class="n">k2t</span><span class="o">.</span><span class="n">test_model_submit</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="conclusion">conclusion</h2>

<p>We have gained a small improvement of 1.6%. The right direction, yet far away from the results achieved with other models. This seems to suggest a limitation for this problem of a learning function that is linear. In future posts I’ll look for models that rely on non-linear functions.</p>

<p>I also wonder what would be the result of using as model logistic regression one-vs-all for each label. And how it compares to plain softmax.</p>

</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/18/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/16/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/10/29/Rails-API-Versioning/">Rails Api Versioning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/29/Devise-Angular-Rails/">Wiring Devise into Angular + Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/27/Wire-Angular-Rails/">Wiring Angular into Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/24/Ruby-Meta-Frolicking/">Ruby Meta-Frolicking</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/20/JS-review/">JavaScript OO Review</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
