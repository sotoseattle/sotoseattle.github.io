
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="Reading this post I realized that we may be able to use a simple logistic regression on a 1-to-1 basis for those cases the softmax model gets wrong. &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/19">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Developer of software, ideas, startups</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/06/OCR-Analaysis/">Kaggle Digit OCR: Softmax Analysis</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-06T09:03:00-07:00" pubdate data-updated="true">Oct 6<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Reading this <a href="http://peekaboo-vision.blogspot.com/2012/12/another-look-at-mnist.html">post</a> I realized that we may be able to use a simple logistic regression on a 1-to-1 basis for those cases the softmax model gets wrong. </p>

<p>We extract the 970 images that we misclassified (out of the 12.000 of our validation set). Then for each image we compute the hypothesis function of each image using the optimized parameters (previously stored). </p>

<p>Let’s get as an example the 42<sup>nd</sup> image from the evaluation set:</p>

<p><img class="center" src="/images/kaggle_ocr/xe_42.png" width="400" height="400" title="Seven" /></p>

<p>The hypothesis function of the image outputs the following probabilities for each label (0 to 9) that we have multiplied by 100: <code>[43.79, 7.6e-09, 0.37, 0.0008, 0.12, 0.44, 55.11, 0.02, 0.09, 0.02]</code></p>

<p>Our model mis classifies it as a 6 (highest probability of 55%) but it is really meant to be a 0. Now, realize that the second choice, the second maximum is for label 0 (with 43% probability), the right one!. Furthermore, the second choice is at a big distance from everyone else (less than 1%). So even if softmax failed, it was pretty close.</p>

<p>If we could pick and choose the cases where first and second choice were close by, we could tell the model to disregard the softmax choice and instead, for that particular case, perform a logistic regression between the two choices. Generalized to all possible combinations of first and second choices we would need to compute $(n-1)n/2 = 45$ times the 784 parameters.</p>

<p>We can derive from the data that, of the 970 misclassified images on the evaluation set, 608 (or 62%) happen to have as second choice the true class. For example, the most frequent misclassification in this later group happens differentiating between 7 and 9, (69 times out of 608).</p>

<p>Let’s make a small experiment and see if it is true that logit works better than softmax when figuring 7s and 9s. We,</p>

<ul>
  <li>extract from the training and evaluation test all images whose true label is 7 or 9. </li>
  <li>derive the opt thetas for a simple logistic regression from the extracted training set.</li>
  <li>compare on the extracted evaluation set the accuracy of both, softmax and logistic regression.</li>
</ul>

<p>Softmax gets it right 90.8% and logit 95.8%. Lets repeat it for the most conflicting pairs previously identified:</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: center">pairs</th>
      <th style="text-align: center">count</th>
      <th style="text-align: center">softmax_acc(%)</th>
      <th style="text-align: center">logit_acc(%)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">7 vs. 9</td>
      <td style="text-align: center">69</td>
      <td style="text-align: center">90.8</td>
      <td style="text-align: center">95.8</td>
    </tr>
    <tr>
      <td style="text-align: center">4 vs. 9</td>
      <td style="text-align: center">53</td>
      <td style="text-align: center">91.3</td>
      <td style="text-align: center">96.8</td>
    </tr>
    <tr>
      <td style="text-align: center">5 vs. 8</td>
      <td style="text-align: center">46</td>
      <td style="text-align: center">86.6</td>
      <td style="text-align: center">96.5</td>
    </tr>
    <tr>
      <td style="text-align: center">3 vs. 5</td>
      <td style="text-align: center">40</td>
      <td style="text-align: center">87.6</td>
      <td style="text-align: center">96.0</td>
    </tr>
    <tr>
      <td style="text-align: center">1 vs. 8</td>
      <td style="text-align: center">37</td>
      <td style="text-align: center">93.5</td>
      <td style="text-align: center">98.4</td>
    </tr>
    <tr>
      <td style="text-align: center">2 vs. 8</td>
      <td style="text-align: center">33</td>
      <td style="text-align: center">89.4</td>
      <td style="text-align: center">97.7</td>
    </tr>
    <tr>
      <td style="text-align: center">2 vs. 3</td>
      <td style="text-align: center">32</td>
      <td style="text-align: center">90.2</td>
      <td style="text-align: center">97.3</td>
    </tr>
    <tr>
      <td style="text-align: center">3 vs. 8</td>
      <td style="text-align: center">29</td>
      <td style="text-align: center">89.6</td>
      <td style="text-align: center">97.1</td>
    </tr>
    <tr>
      <td style="text-align: center">2 vs. 7</td>
      <td style="text-align: center">24</td>
      <td style="text-align: center">91.0</td>
      <td style="text-align: center">98.7</td>
    </tr>
    <tr>
      <td style="text-align: center">5 vs. 6</td>
      <td style="text-align: center">21</td>
      <td style="text-align: center">90.2</td>
      <td style="text-align: center">97.8</td>
    </tr>
    <tr>
      <td style="text-align: center">0 vs. 6</td>
      <td style="text-align: center">19</td>
      <td style="text-align: center">95.8</td>
      <td style="text-align: center">98.6</td>
    </tr>
    <tr>
      <td style="text-align: center">3 vs. 9</td>
      <td style="text-align: center">19</td>
      <td style="text-align: center">90.0</td>
      <td style="text-align: center">98.5</td>
    </tr>
    <tr>
      <td style="text-align: center">2 vs. 6</td>
      <td style="text-align: center">18</td>
      <td style="text-align: center">92.9</td>
      <td style="text-align: center">98.5</td>
    </tr>
    <tr>
      <td style="text-align: center">0 vs. 5</td>
      <td style="text-align: center">15</td>
      <td style="text-align: center">90.4</td>
      <td style="text-align: center">98.4</td>
    </tr>
    <tr>
      <td style="text-align: center">0 vs. 2</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">93.0</td>
      <td style="text-align: center">98.5</td>
    </tr>
    <tr>
      <td style="text-align: center">3 vs. 7</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">91.2</td>
      <td style="text-align: center">98.4</td>
    </tr>
    <tr>
      <td style="text-align: center">5 vs. 9</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">87.1</td>
      <td style="text-align: center">98.5</td>
    </tr>
    <tr>
      <td style="text-align: center">4 vs. 6</td>
      <td style="text-align: center">12</td>
      <td style="text-align: center">94.4</td>
      <td style="text-align: center">99.0</td>
    </tr>
    <tr>
      <td style="text-align: center">8 vs. 9</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">89.2</td>
      <td style="text-align: center">98.7</td>
    </tr>
    <tr>
      <td style="text-align: center">4 vs. 5</td>
      <td style="text-align: center">10</td>
      <td style="text-align: center">88.8</td>
      <td style="text-align: center">98.6</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<p>Here is the module for logistic regression:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>logit module</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
<span class="line-number">46</span>
<span class="line-number">47</span>
<span class="line-number">48</span>
<span class="line-number">49</span>
<span class="line-number">50</span>
<span class="line-number">51</span>
<span class="line-number">52</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">scipy</span> <span class="kn">as</span> <span class="nn">sc</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin_l_bfgs_b</span>
</span><span class="line">
</span><span class="line"><span class="c"># MODULE FOR LOGISTIC REGRESSION</span>
</span><span class="line"><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
</span><span class="line">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1E-11</span><span class="p">))(</span><span class="n">z</span><span class="p">)</span>
</span><span class="line">    <span class="c">#return np.vectorize(lambda x: 1/(1+np.exp(-x)))(z)</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">h</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;hypothesis function. probability of input x with params t&#39;&#39;&#39;</span>
</span><span class="line">    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">j</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;cost function J(theta)&#39;&#39;&#39;</span>
</span><span class="line">    <span class="n">prediction</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
</span><span class="line">    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">J</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prediction</span><span class="p">))</span> <span class="o">-</span>        \
</span><span class="line">            <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">prediction</span><span class="p">))</span> <span class="o">+</span>    \
</span><span class="line">            <span class="p">(</span><span class="n">lam</span><span class="o">/</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">::],</span> <span class="mi">2</span><span class="p">)))</span><span class="o">/</span><span class="n">m</span>
</span><span class="line">    <span class="k">return</span> <span class="n">J</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;gradient function, first partial derivation of J(theta)&#39;&#39;&#39;</span>
</span><span class="line">    <span class="n">prediction</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
</span><span class="line">    <span class="n">regu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span><span class="n">t</span><span class="p">[</span><span class="mi">1</span><span class="p">::,]</span><span class="o">*</span><span class="n">lam</span><span class="p">])</span>
</span><span class="line">    <span class="n">grad</span> <span class="o">=</span> <span class="p">((</span><span class="n">prediction</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">regu</span><span class="p">)</span><span class="o">/</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="k">return</span> <span class="n">grad</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">optimizeThetas</span><span class="p">(</span><span class="n">tinit</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;derive thetas using l_bfgs algorithm&#39;&#39;&#39;</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">j</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">fprime</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">v</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lam</span><span class="p">)</span>
</span><span class="line">    <span class="p">[</span><span class="n">thetas</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">fmin_l_bfgs_b</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">tinit</span><span class="p">,</span> <span class="n">fprime</span><span class="o">=</span><span class="n">fprime</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</span><span class="line">    <span class="k">if</span> <span class="n">visual</span><span class="p">:</span>
</span><span class="line">        <span class="k">print</span> <span class="n">thetas</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">]</span>
</span><span class="line">        <span class="k">print</span> <span class="n">f</span>
</span><span class="line">        <span class="k">print</span> <span class="n">d</span>
</span><span class="line">    <span class="k">return</span> <span class="n">thetas</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</span><span class="line">    <span class="n">acc</span> <span class="o">=</span> <span class="mf">0.0</span>
</span><span class="line">    <span class="n">m</span><span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">        <span class="n">p</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,::])</span>
</span><span class="line">        <span class="k">if</span> <span class="p">(</span><span class="n">p</span><span class="o">&gt;</span><span class="mf">0.5</span> <span class="ow">and</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">p</span><span class="o">&lt;=</span><span class="mf">0.5</span> <span class="ow">and</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">):</span>
</span><span class="line">                <span class="n">acc</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">    <span class="k">return</span> <span class="n">acc</span><span class="o">/</span><span class="n">m</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/06/Kaggle-Digit-OCR-Softmax/">Kaggle Digit OCR: Softmax Model</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-06T09:02:00-07:00" pubdate data-updated="true">Oct 6<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>Instead of using a Random Forest as suggested I am going to start with Softmax regression.</p>

<h2 id="datasets">datasets</h2>

<p>The original kaggle/mnist data is therefore divided into e sets:</p>

<ul>
  <li>a training set with the first 30.000 images (xt, yt) from train.csv.</li>
  <li>an evaluation set of the remaining 12.000 examples (xe, ye) from train.csv.</li>
  <li>the testing set with 28.000 images (x_test) to classify from test.csv.</li>
</ul>

<p>I also initialize some parameters that I may later optimize:</p>

<ul>
  <li>lambda, the regularization parameter (L).</li>
  <li>number of labels (LABS = 10), digits 0 to 9.</li>
  <li>number of features (N = 784).</li>
  <li>thetas initialized to zeros as a matrix (10, 784)</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>class KaggleTrain initialization</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">math</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">scipy</span> <span class="kn">as</span> <span class="nn">sc</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin_l_bfgs_b</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</span><span class="line">
</span><span class="line"><span class="kn">import</span> <span class="nn">sys</span>
</span><span class="line"><span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="s">&#39;../../.&#39;</span> <span class="p">)</span> <span class="c"># logysoft module address</span>
</span><span class="line"><span class="kn">import</span> <span class="nn">Logysoft</span> <span class="kn">as</span> <span class="nn">softmax</span>  <span class="c"># module with softmax utility functions</span>
</span><span class="line">
</span><span class="line"><span class="k">class</span> <span class="nc">KaggleTrain</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="c"># initialize general parameters</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">=</span> <span class="mi">7</span>                  <span class="c"># lambda, regularization parameter</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span> <span class="o">=</span> <span class="mi">10</span>              <span class="c"># N. of possible values of Y (labels)</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="mi">784</span>                <span class="c"># N. of features per image (28x28)</span>
</span><span class="line">
</span><span class="line">        <span class="c"># load data files</span>
</span><span class="line">        <span class="n">training_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/train.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">        <span class="n">testing_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">&#39;./data/kaggle/test.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">training_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">training_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
</span><span class="line">
</span><span class="line">        <span class="c"># training set</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">xt</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30000</span><span class="p">,</span> <span class="p">:]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">yt</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">30000</span><span class="p">,</span> <span class="p">:]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">gt</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">groundTruth</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">yt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">)</span>
</span><span class="line">        <span class="c"># evaluation set</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">xe</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="mi">30000</span><span class="p">:,</span> <span class="p">:]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">ye</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="mi">30000</span><span class="p">:,</span> <span class="p">:]</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">ge</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">groundTruth</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ye</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">)</span>
</span><span class="line">        <span class="c"># testing set</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">testing_data</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,:])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="choosing-lambda-">choosing lambda ($λ$)</h2>

<p>To choose the regularization parameter first we choose the range of lambdas for which to test as [1e-3, 1e-2, 1e-1, 1, 10, 100] and then we zero in in a smaller range (with minor modifications to the code).</p>

<p>For each lambda we find the optimum $θ<em>{opt}$ in the training set and then compute the cost $J(θ</em>{opt})$ in the evaluation set. We choose the lambda that works best and minimizes that cost/error measure.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>choosing lambda</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">choose_lambda</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;train with different regularization parameters and choose</span>
</span><span class="line"><span class="sd">       the one that minimizes the cost in the evaluation set.&#39;&#39;&#39;</span>
</span><span class="line">    <span class="n">tinit</span> <span class="o">=</span> <span class="mf">0.005</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># initialize some working vars</span>
</span><span class="line">    <span class="n">rango</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
</span><span class="line">    <span class="n">Jt</span><span class="p">,</span> <span class="n">Je</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span><span class="line">    <span class="n">bestC</span><span class="p">,</span> <span class="n">bestL</span> <span class="o">=</span> <span class="mf">1e+10</span><span class="p">,</span> <span class="mf">0.0</span>
</span><span class="line">
</span><span class="line">    <span class="c"># cycle through lambdas and choose the one with lowest cost</span>
</span><span class="line">    <span class="k">for</span> <span class="n">chosen_lambda</span> <span class="ow">in</span> <span class="n">rango</span><span class="p">:</span>
</span><span class="line">        <span class="n">t</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">optimizeThetas</span><span class="p">(</span><span class="n">tinit</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gt</span><span class="p">,</span> \
</span><span class="line">            <span class="n">numLabels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">chosen_lambda</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="n">cost_t</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">j</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="n">chosen_lambda</span><span class="p">)</span>
</span><span class="line">        <span class="n">cost_e</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">j</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xe</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ge</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="n">chosen_lambda</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="n">Jt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Jt</span><span class="p">,</span> <span class="n">cost_t</span><span class="p">)</span>
</span><span class="line">        <span class="n">Je</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Je</span><span class="p">,</span> <span class="n">cost_e</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="n">cost_e</span> <span class="o">&lt;</span> <span class="n">bestC</span><span class="p">:</span>
</span><span class="line">            <span class="n">bestC</span> <span class="o">=</span> <span class="n">cost_e</span>
</span><span class="line">            <span class="n">bestL</span> <span class="o">=</span> <span class="n">chosen_lambda</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&quot;</span><span class="se">\n\n</span><span class="s">the best lambda is&quot;</span><span class="p">,</span> <span class="n">bestL</span>
</span><span class="line">
</span><span class="line">    <span class="c"># plot</span>
</span><span class="line">    <span class="n">line1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">rango</span><span class="p">),</span> <span class="n">Jt</span><span class="p">)</span>
</span><span class="line">    <span class="n">line2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">rango</span><span class="p">),</span> <span class="n">Je</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">line1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;training&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="n">solid_joinstyle</span><span class="o">=</span><span class="s">&#39;round&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">line2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;training&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">,</span> <span class="n">solid_joinstyle</span><span class="o">=</span><span class="s">&#39;round&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;log10(Lambda)&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;J&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">    <span class="k">pass</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>The code not only informs us of the best lambda available but plots the cost curves for each lambda. The blue line represents the cost (error) for each lambda value in the training set. The red line is the cost in the evaluation set and from where we choose $λ$ that minimizes the cost/error. The first chart is log10 of lambda and the second is just lambdalone.</p>

<div style="text-align:center">
<img src="/images/kaggle_ocr/choosing_lambda_kaggle_softmax.png" width="400" height="500" title="Training Error (blue) &amp; Validation Error (red)" />
<img src="/images/kaggle_ocr/choosing_lambda_kaggle_softmax_2.png" width="400" height="500" title="Training Error (blue) &amp; Validation Error (red)" />
</div>

<p>In our case, we choose a small lambda of 0.002. With input data that is not scaled, the same exercise leads to choosing a $λ$ of around 7.</p>

<h2 id="optimizing-thetas">optimizing thetas</h2>

<p>Once we have the regularization fixed we compute the optimized thetas on the training set and measure the accuracy on the evaluation set.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>optimize parameters and check accuracy</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">check_accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">        <span class="sd">&#39;&#39;&#39;computes thetas on training set, saves them, and checks</span>
</span><span class="line"><span class="sd">           accuracy on evaluation set&#39;&#39;&#39;</span>
</span><span class="line">        <span class="n">tinit</span> <span class="o">=</span> <span class="mf">0.005</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
</span><span class="line">        <span class="n">thetas</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">optimizeThetas</span><span class="p">(</span><span class="n">tinit</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)</span>
</span><span class="line">        <span class="n">thetas</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">        <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s">&#39;./data/kaggle/optimized_thetas.csv&#39;</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&#39;,&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="n">h</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">h</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xe</span><span class="p">)</span>
</span><span class="line">        <span class="n">predictions</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">        <span class="n">zeros_are_right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ye</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</span><span class="line">        <span class="n">misses</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">zeros_are_right</span><span class="p">)</span>
</span><span class="line">        <span class="n">acc</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">misses</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
</span><span class="line">        <span class="k">print</span> <span class="s">&#39;accuracy:&#39;</span><span class="p">,</span> <span class="n">acc</span>
</span><span class="line">        <span class="k">pass</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>We achieve an accuracy rate of accuracy: 0.919.</p>

<h2 id="learning-curves">learning curves</h2>

<p>Finally we plot and analyze the learning curves by repeating the exercise for different sample sizes.</p>

<p>The cost (error) on the training set is represent in blue and the cost on the validation set in red. For small training sets the error is small because any curve can be made to fit few data points. The validation error is big because the simplistic model has little to do with reality. </p>

<p>As we grow the sample size the training error grows (the pains of fitting all points in the curve) while the validation cost diminishes as more complex models generalize better to untrained points.</p>

<p><img class="center" src="/images/kaggle_ocr/learning_kaggle_soft.png" width="600" height="500" title="Training Error (blue) &amp; Validation Error (red)" /></p>

<p>A priori we see that our model does not suffer from a bout of bias. Nevertheless, since we still have room to improve in terms of accuracy we may be suffering from a bit of high variance (over fitting to the training data and lack of generalization). Ways to solve this are:</p>

<ul>
  <li>increase $λ$. Not really since we know it was optimized and from here we will only rise the error and bias our model.</li>
  <li>gather more training examples. Not available either in kaggle, although we could create new examples by deforming, rotating, etc the existing ones.</li>
  <li>reduce the number of features. I tried with PCA and the results were definitively poorer, maybe because we were indiscriminately removing parameters?</li>
  <li>get creative (more later).</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>learning curves</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">learning_curves</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">    <span class="n">tinit</span> <span class="o">=</span> <span class="mf">0.005</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
</span><span class="line">    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="o">.</span><span class="n">shape</span>
</span><span class="line">    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span><span class="o">*</span><span class="mi">1000</span>
</span><span class="line">    <span class="n">Jt</span><span class="p">,</span> <span class="n">Je</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">:</span>
</span><span class="line">        <span class="n">my_t</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">optimizeThetas</span><span class="p">(</span><span class="n">tinit</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">,:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">gt</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">,:],</span> \
</span><span class="line">            <span class="n">numLabels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">        <span class="n">Jt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Jt</span><span class="p">,</span> <span class="n">soft</span><span class="o">.</span><span class="n">j</span><span class="p">(</span><span class="n">my_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">,:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">gt</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">m</span><span class="p">,:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">))</span>
</span><span class="line">        <span class="n">Je</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Je</span><span class="p">,</span> <span class="n">soft</span><span class="o">.</span><span class="n">j</span><span class="p">(</span><span class="n">my_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xe</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ge</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># plot (m, Jtr) and (m, Jcv)</span>
</span><span class="line">    <span class="n">line1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">Jt</span><span class="p">)</span>
</span><span class="line">    <span class="n">line2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">Je</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">line1</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;training&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;b&#39;</span><span class="p">,</span> <span class="n">solid_joinstyle</span><span class="o">=</span><span class="s">&#39;round&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">line2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">&#39;training&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">&#39;r&#39;</span><span class="p">,</span> <span class="n">solid_joinstyle</span><span class="o">=</span><span class="s">&#39;round&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&#39;Number of Examples&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Cost / Error&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span><span class="line">    <span class="k">pass</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="submission">Submission</h2>

<p>For the submission I optimized parameters for the original set of 42.000 images (not just the 30.000 examples of the training set). </p>

<blockquote><p>The accuracy achieved in the Kaggle competition is 92.086%.</p></blockquote>

<p>I also tried a couple of small tweaks:</p>

<ul>
  <li>Re-scaling [to range (0,1), mean normalize and feature scale] and using an optimized $λ$ of 0.002 the kaggle accuracy was a tad smaller at 91.6% </li>
  <li>Reducing dimensions with PCA while keeping 90 to 99% of variance. The accuracy went down to 90%, proving that PCA may have been removing valuable information. PCA is not always a good recipe for high variance problems.</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>predicting on test set</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">test_model_submit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">    <span class="c"># compute thetas on whole training set</span>
</span><span class="line">    <span class="n">tinit</span> <span class="o">=</span> <span class="mf">0.005</span><span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
</span><span class="line">    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">xt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">xe</span><span class="p">])</span>
</span><span class="line">    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">yt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ye</span><span class="p">])</span>
</span><span class="line">    <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">gt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ge</span><span class="p">])</span>
</span><span class="line">
</span><span class="line">    <span class="c"># find thetas and save them</span>
</span><span class="line">    <span class="n">thetas</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">optimizeThetas</span><span class="p">(</span><span class="n">tinit</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)</span>
</span><span class="line">    <span class="n">thetas</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">LABS</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s">&#39;./data/kaggle/submit_optimized_thetas.csv&#39;</span><span class="p">,</span> <span class="n">thetas</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&#39;,&#39;</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="c"># compute predictions</span>
</span><span class="line">    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span>
</span><span class="line">    <span class="n">h</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">h</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">x_test</span><span class="p">)</span>
</span><span class="line">    <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">        <span class="n">a</span> <span class="o">=</span> <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
</span><span class="line">        <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span>
</span><span class="line">    <span class="k">print</span> <span class="s">&#39;To submitt add header: ImageId,Label&#39;</span>
</span><span class="line">    <span class="k">print</span> <span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">,:]</span>
</span><span class="line">    <span class="n">np</span><span class="o">.</span><span class="n">savetxt</span><span class="p">(</span><span class="s">&#39;./data/kaggle/predictions.csv&#39;</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s">&#39;</span><span class="si">%i</span><span class="s">,</span><span class="si">%i</span><span class="s">&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">pass</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="conclusion">conclusion</h2>

<p>I have read that for classification problems, in practice, different algorithms often yield similar accuracy measures. Most of the time algorithm doesn’t matter as much. You can easily reach 90% recognition rate with the simplest algorithm, but every next percent is very difficult to achieve. I seem to have to struck the softmax wall.</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/05/Kaggle-Digit-OCR/">Kaggle Digit OCR: The Data</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-05T09:02:00-07:00" pubdate data-updated="true">Oct 5<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>My first ML program is going to be the <a href="http://www.kaggle.com/c/digit-recognizer">Kaggle OCR digit competition</a>. </p>

<p>The original data comes in two files: train.csv and a test.csv. Each file contain gray-scale images of hand-drawn digits, from zero through nine.</p>

<p>Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive.</p>

<p>The training data set, (train.csv), has 785 columns and 42.0001 rows. The first column, called “label”, is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image. The first row is a header and each subsequent row holds an image in vectorized form for a total of 42.000 images</p>

<p>The test file is similar but without a label column, which is the goal of the competition.</p>

<p>We can visualize each handwritten digit with the following code:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>visualize handwritten digit</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="n">x</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>	<span class="c"># all the training images</span>
</span><span class="line"><span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>	<span class="c"># all the training labels</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="n">example_number</span><span class="p">):</span>
</span><span class="line">	<span class="k">print</span> <span class="s">&#39;should be a&#39;</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">example_number</span><span class="p">]</span>
</span><span class="line">	<span class="n">matplotlib</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
</span><span class="line">	<span class="n">matplotlib</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">example_number</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">))</span>
</span><span class="line">	<span class="n">matplotlib</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>For example, from train.csv, the 6<sup>th</sup> row is: [7, 0, 0, …, 82, 152, 71, 51, 51, …, 0, 0, 0, 0, 0.]</p>

<p>Where the first digit tells us that the “real” number is a “7” and the following 784 numbers can be reshaped into a grid to show the following image:</p>

<p><img class="center" src="/images/kaggle_ocr/mnist_good_7.png" width="400" height="400" title="Seven" /></p>

<p>Not everything is so clear cut. Two additional examples from the training set: a One and a Seven, which could be misclassified even by hand.</p>

<div style="text-align:center">
<img src="/images/kaggle_ocr/mnist_1.png" width="400" height="400" title="One" />
<img src="/images/kaggle_ocr/mnist_7.png" width="400" height="400" title="Seven" />
</div>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/20/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/18/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/11/17/Code-Retreat/">Code Retreat Reflections</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/11/02/Primo-Lives/">Primo Lives!</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/30/Devise-Angular-Rails/">Wiring Devise into Angular + Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/28/Rails-API-Versioning/">Rails Api Versioning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/27/Wire-Angular-Rails/">Wiring Angular into Rails</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
