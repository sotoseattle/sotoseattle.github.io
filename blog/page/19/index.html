
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="To train the model and find the parameters we need some sort of gradient descent algorithm. In python fmin_bfgs takes way too long and hogs memory, &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/19">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Developer of software, ideas, startups</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li>
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/04/Softmax-IV/">Softmax Theory: $θ$</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-04T09:02:00-07:00" pubdate data-updated="true">Oct 4<span>th</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>To train the model and find the parameters we need some sort of gradient descent algorithm. In python <a href="http://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm">fmin_bfgs</a> takes way too long and hogs memory, while it’s sister <a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS">fmin_l_bfgs_b</a> is much faster and lighter.</p>

<p>To call it we need to consider the following issues:</p>

<ul>
  <li>we need to pass the cost and gradient function separately (unlike in Octave).</li>
  <li>the first argument of the function and of its derivation has to be the parameter to be optimized.</li>
  <li>tinit is an initial set of parameters from which to start the search, a good compromise is <code>0.005 * np.random.rand(labels, n)</code>.</li>
  <li>for the l_bfgs to work the gradient format must be rolled out (not in matrix form).</li>
  <li>the optimizing algorithm provides additional useful information [f, d] about the search performed.</li>
  <li>other options, like the maximum number of iterations to perform or the convergence tolerance can be researched online.</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>optimizing thetas</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line"><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">fmin_l_bfgs_b</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">optimizeThetas</span><span class="p">(</span><span class="n">tinit</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">GT</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">j</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">GT</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">fprime</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
</span><span class="line">        <span class="k">return</span> <span class="n">v</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">GT</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="p">[</span><span class="n">thetas</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">fmin_l_bfgs_b</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="n">tinit</span><span class="p">,</span> <span class="n">fprime</span><span class="o">=</span><span class="n">fprime</span><span class="p">,</span> <span class="n">maxiter</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">thetas</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/03/Softmax-III/">Softmax Theory: $∇(θ)$</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-03T09:02:00-07:00" pubdate data-updated="true">Oct 3<span>rd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>There is no known closed-form way to solve for the minimum of $J(θ)$, and thus as usual we’ll resort to an iterative optimization algorithm such as gradient descent or L-BFGS. Taking derivatives, one can show that the gradient is:</p>

<script type="math/tex; mode=display">
\begin{align}
\nabla_{\theta_j} J(\theta) = - \frac{1}{m} \sum_{i=1}^{m}{ \left[ x^{(i)} ( 1\{ y^{(i)} = j\}  - p(y^{(i)} = j | x^{(i)}; \theta) ) \right]  } + \lambda \theta_j
\end{align}
</script>

<p>By minimizing $J(θ)$ with respect to $θ$, we will have a working implementation of softmax regression. Note that the gradient output is flatten from a matrix form to a vector form. This is needed format for the gradient descent algorithm to work.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>gradient function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">):</span>
</span><span class="line">    <span class="n">thetas</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">numLabels</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</span><span class="line">    <span class="n">hx</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span><span class="line">    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">grad</span> <span class="o">=</span> <span class="p">((</span><span class="n">groundTruth</span><span class="o">-</span><span class="n">hx</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="o">-</span><span class="n">m</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">regul_lambda</span><span class="o">*</span><span class="n">thetas</span><span class="p">)</span>
</span><span class="line">    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">grad</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="checking-the-gradient-computation">checking the gradient computation</h2>

<p>It is very a healthy habit to check gradients numerically before proceeding to train the model. The norm of the difference between the numerical gradient and your analytical gradient should be small, on the order of 10 − 9. The following code uses some random data to compare the above gradient against a numerical approximation based on the formula</p>

<p>The key to compute the gradient by hand is to keep all $θ$ fixed while we change a single one by a small amount (epsilon) $\theta_j := \theta_j - \alpha \nabla_{\theta_j} J(\theta) (for each j=1,\ldots,k)$.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>checking gradient with numerical approximation</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">grad_by_hand</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">):</span>
</span><span class="line">    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-4</span>
</span><span class="line">    <span class="n">t</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span><span class="line">    <span class="n">n</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">size</span>
</span><span class="line">    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">n</span><span class="p">,))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
</span><span class="line">        <span class="n">t1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span><span class="line">        <span class="n">t2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</span><span class="line">        <span class="n">t1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
</span><span class="line">        <span class="n">t2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
</span><span class="line">        <span class="n">a</span> <span class="o">=</span> <span class="n">j</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">)</span>
</span><span class="line">        <span class="n">b</span> <span class="o">=</span> <span class="n">j</span><span class="p">(</span><span class="n">t2</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">)</span>
</span><span class="line">        <span class="n">grad</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">epsilon</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">grad</span>
</span><span class="line">
</span><span class="line"><span class="n">x_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
</span><span class="line"><span class="n">y_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</span><span class="line"><span class="n">g_check</span> <span class="o">=</span> <span class="n">soft</span><span class="o">.</span><span class="n">groundTruth</span><span class="p">(</span><span class="n">y_check</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">)</span>
</span><span class="line"><span class="n">t_check</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;float64&#39;</span><span class="p">)</span>
</span><span class="line"><span class="n">g_theo</span> <span class="o">=</span> <span class="n">v</span><span class="p">(</span><span class="n">t_check</span><span class="p">,</span> <span class="n">x_check</span><span class="p">,</span> <span class="n">g_check</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
</span><span class="line"><span class="n">g_hand</span> <span class="o">=</span> <span class="n">grad_by_hand</span><span class="p">(</span><span class="n">t_check</span><span class="p">,</span> <span class="n">x_check</span><span class="p">,</span> <span class="n">g_check</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
</span><span class="line"><span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_hand</span> <span class="o">-</span> <span class="n">g_theo</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g_hand</span> <span class="o">+</span> <span class="n">g_theo</span><span class="p">)</span>
</span><span class="line"><span class="k">assert</span> <span class="n">diff</span> <span class="o">&lt;=</span> <span class="mf">1e-9</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2013/10/02/Softmax-II/">Softmax Theory: $J(θ)$</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2013-10-02T09:02:00-07:00" pubdate data-updated="true">Oct 2<span>nd</span>, 2013</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>The cost function of logistic regression was defined as:</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align}
J(\theta)
&= - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=0}^{1} 1\left\{y^{(i)} = j\right\} \log p(y^{(i)} = j | x^{(i)} ; \theta) \right]
\end{align}
 %]]></script>

<p>Softmax is just a generalization of logistic regression from 2 to k classes. In our case we have:</p>

<script type="math/tex; mode=display">
p(y^{(i)}=j | x^{(i)} ; \theta) = \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)}} }
</script>

<p>And the softmax cost function is</p>

<script type="math/tex; mode=display">
\begin{align}
J(\theta) = - \frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y^{(i)} = j\right\} \log \frac{e^{\theta_j^T x^{(i)}}}{\sum_{l=1}^k e^{ \theta_l^T x^{(i)} }}  \right]
              + \frac{\lambda}{2} \sum_{i=1}^k \sum_{j=0}^n \theta_{ij}^2
\end{align}
</script>

<p>…which is similar to the cost function of logistic regression, except that we now sum over the k different possible values of the class label. </p>

<h2 id="regularization">regularization</h2>

<p>We have included in the cost function a regularization component, a weight decay term which penalizes large values of the parameters.</p>

<script type="math/tex; mode=display">
\frac{\lambda}{2} \sum_{i=1}^k \sum_{j=0}^{n} \theta_{ij}^2
</script>

<p>With this weight decay term (for any $λ &gt; 0$), the cost function $J(θ)$ is now strictly convex, and is guaranteed to have a unique solution. The Hessian is now invertible, and because $J(θ)$ is convex, algorithms such as gradient descent, L-BFGS, etc. are guaranteed to converge to the global minimum.</p>

<h2 id="indicator-function">indicator Function</h2>

<p>In the equation above, $1\{\cdot\}$ is the indicator function, so that 1{a true statement} = 1, and 1{a false statement} = 0. For example, 1{2 + 2 = 4} evaluates to 1; whereas 1{1 + 1 = 5} evaluates to 0.</p>

<p>The following code takes in a vector y with observed labels (from training set) and outputs the indicator function. So for example, for…</p>

<script type="math/tex; mode=display">% <![CDATA[

\begin{align} 
y &=

\begin{bmatrix} 
3 \\
0 \\
8 \\
\end{bmatrix} \\
\end{align} 
 %]]></script>

<p>…it ouputs</p>

<script type="math/tex; mode=display">
[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\\
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\\
[0, 0, 0, 0, 0, 0, 0, 0, 1, 0]
</script>

<p>An (m,1) label vector is converted into a (m, num_labels) sparse matrix.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>indicator function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">groundTruth</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">):</span>
</span><span class="line">    <span class="n">m</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">groundTruth</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">))</span>
</span><span class="line">    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
</span><span class="line">        <span class="n">groundTruth</span><span class="p">[</span><span class="n">row</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
</span><span class="line">    <span class="k">return</span> <span class="n">groundTruth</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Once we have the indicator function built, the code for the cost function is simple.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>cost function</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
</span><span class="line">
</span><span class="line"><span class="k">def</span> <span class="nf">j</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">groundTruth</span><span class="p">,</span> <span class="n">numLabels</span><span class="p">,</span> <span class="n">regul_lambda</span><span class="p">):</span>
</span><span class="line">    <span class="n">thetas</span> <span class="o">=</span> <span class="n">thetas</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">numLabels</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>                    <span class="c"># just making sure well formed</span>
</span><span class="line">    <span class="n">m</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="n">hx</span> <span class="o">=</span> <span class="n">h</span><span class="p">(</span><span class="n">thetas</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>                                         <span class="c"># compute hypothesys matrix</span>
</span><span class="line">    <span class="n">b</span> <span class="o">=</span> <span class="n">groundTruth</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">hx</span><span class="p">))</span>                              <span class="c"># indicator function computation</span>
</span><span class="line">    <span class="n">lambdaEffect</span> <span class="o">=</span> <span class="p">(</span><span class="n">regul_lambda</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">thetas</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c"># regularization cost component</span>
</span><span class="line">    <span class="n">J</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">b</span><span class="p">))</span><span class="o">/</span><span class="n">m</span> <span class="o">+</span> <span class="n">lambdaEffect</span>                   <span class="c"># add up to compute scalar cost</span>
</span><span class="line">    <span class="k">return</span> <span class="n">J</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/20/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/18/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2014/10/29/Rails-API-Versioning/">Rails Api Versioning</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/29/Devise-Angular-Rails/">Wiring Devise into Angular + Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/27/Wire-Angular-Rails/">Wiring Angular into Rails</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/24/Ruby-Meta-Frolicking/">Ruby Meta-Frolicking</a>
      </li>
    
      <li class="post">
        <a href="/blog/2014/10/20/JS-review/">JavaScript OO Review</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
