
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="How to sample a distribution The first method we need is to find a random assignment given a discrete distribution. For example, if we have a &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/9">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Curious Investor & Coder</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <!-- <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li> -->
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/10/Gibbs-code/">MCMC Gibbs - Code</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-10T08:01:00-08:00" pubdate data-updated="true">Feb 10<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><h2 id="how-to-sample-a-distribution">How to sample a distribution</h2>

<p>The first method we need is to find a random assignment given a discrete distribution. For example, if we have a distribution over a single variable with cardinality 3, [p0, p1, p2] = [0.25, 0.35, 0.4] we:</p>

<ul>
  <li>create the cumulative distr =&gt; [c1, c2, c3] = [0.25, 0.60, 1.0]</li>
  <li>pick up a random number r between 0 and 1 (based on uniform dist), let’s say we get 0.43</li>
  <li>create a new array based on [c &gt;= r] =&gt; [False, True, True]</li>
  <li>the first True gives us the assignment, in this case 1 out of possible assignments (0, 1, 2).</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Sample Randomly From Given Distribution</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">randSampleDist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">card</span><span class="p">,</span> <span class="n">probs</span><span class="p">):</span>
</span><span class="line">    <span class="n">cum_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
</span><span class="line">    <span class="n">u</span> <span class="o">=</span> <span class="n">uniform</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">cum_prob</span><span class="p">))</span>
</span><span class="line">    <span class="n">included</span> <span class="o">=</span> <span class="n">cum_prob</span><span class="o">&gt;=</span><span class="n">u</span>
</span><span class="line">    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">included</span><span class="o">==</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c"># get first</span>
</span><span class="line">    <span class="k">return</span> <span class="n">sample</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="find-distribution-for-a-variable-given-a-complete-assignment">Find distribution for a variable given a complete assignment</h2>

<p>Now we are going to get the distribution of a variable given that all other variables have been assigned. We operate in log space for easier computations.</p>

<p>The key is to understand that for each factor fu that includes the variable we can reduce it with the assignment (excluding the sampling variable) and marginalize for all variables (excluding the sampling variable) to come up with the distribution for that variable. Operating in log space we add this arrays of probabilities to come up with the total distribution.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Log Distribution for a Variable Given an Assignment</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">logDistAss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_var</span><span class="p">,</span> <span class="n">evidence</span><span class="p">):</span>
</span><span class="line">    <span class="n">cardio</span> <span class="o">=</span> <span class="nb">set</span><span class="p">([</span><span class="n">e</span><span class="o">.</span><span class="n">totCard</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">sample_vars</span><span class="p">])</span>
</span><span class="line">    <span class="n">logbp</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">]</span><span class="o">*</span><span class="n">cardio</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
</span><span class="line">    <span class="k">for</span> <span class="n">fu</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">var_to_factors</span><span class="p">[</span><span class="n">sample_var</span><span class="p">]:</span>
</span><span class="line">        <span class="n">evi</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">evidence</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">fu</span><span class="o">.</span><span class="n">variables</span> <span class="k">if</span> <span class="n">k</span><span class="o">!=</span> <span class="n">sample_var</span><span class="p">)</span>
</span><span class="line">        <span class="n">fu</span> <span class="o">=</span> <span class="n">FactorOperations</span><span class="o">.</span><span class="n">reduce_to_dist</span><span class="p">(</span><span class="n">fu</span><span class="p">,</span> <span class="n">evi</span><span class="p">)</span>
</span><span class="line">        <span class="n">logbp</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">fu</span><span class="p">)</span>
</span><span class="line">    <span class="n">logbp</span> <span class="o">=</span> <span class="n">logbp</span> <span class="o">-</span> <span class="nb">min</span><span class="p">(</span><span class="n">logbp</span><span class="p">)</span> <span class="c"># to avoid underflow when back in normal space</span>
</span><span class="line">    <span class="k">return</span> <span class="n">logbp</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Note three things:</p>

<ul>
  <li>that FactorOperations.extract is an added method that works the same as marginalize but marginalizing in a sweep all variables except the one to extract.</li>
  <li>.var_to_factors is a dictionary where for each variable we have a set of the factors it is involved with. This structure is computed once and allows us to compute faster the sampling process.</li>
  <li>to speed up things I have coded a utility function based on reducing a factor by evidence. It gets an assignment to all factor variables except one and produces an array with the distribution resulting for that variable.</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Reducing Factor to a single var distribution</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">reduce_to_dist</span><span class="p">(</span><span class="n">fA</span><span class="p">,</span> <span class="n">evidence</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;reduce factor by complete assignment to a single variable distribution&#39;&#39;&#39;</span>
</span><span class="line">    <span class="n">ass</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">    <span class="n">counter</span> <span class="o">=</span> <span class="mi">0</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fA</span><span class="o">.</span><span class="n">variables</span><span class="p">:</span>
</span><span class="line">        <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">evidence</span><span class="p">:</span>
</span><span class="line">            <span class="n">ass</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">evidence</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>
</span><span class="line">            <span class="n">ass</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">slice</span><span class="p">(</span><span class="bp">None</span><span class="p">))</span>
</span><span class="line">    <span class="k">if</span> <span class="n">counter</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
</span><span class="line">        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s">&quot;Error in observed vars&quot;</span><span class="p">)</span>
</span><span class="line">    <span class="k">return</span> <span class="n">fA</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">ass</span><span class="p">]</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="one-step-down-the-chain">One step down the Chain</h2>

<p>The Gibbs sampling process, starts with a complete assignment and ends with another complete assignment. For each variable we:</p>

<ul>
  <li>compute the distribution based on the other assigned variables, </li>
  <li>find it’s new assignment based on rolling a dice on the distribution,</li>
  <li>update the assignment with this new value so for the new iteration one variable of the assignment has changed</li>
</ul>

<p>We repeat for all variables, updating all variables in the process, until we get a newly complete assignment that corresponds to the new state in the Markov Chain.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Gibbs 1 step down the Markov Chain</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">Gibbs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ass</span><span class="p">):</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">v</span><span class="p">:</span>
</span><span class="line">        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">logDistAss</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="n">ass</span><span class="p">)</span>
</span><span class="line">        <span class="n">ass</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">randSampleDist</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">totCard</span><span class="p">(),</span> <span class="n">exp</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">ass</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="running-around-the-chain">Running around the Chain</h2>

<p>Collecting samples from the Markov Chain is as easy as running iteratively the previous Gibbs method. We introduce two new inputs:</p>

<ul>
  <li>mix_time: how many steps to take before we think the chain has converged and we start collecting samples. Consider that at the start we are wondering near the initial assignment and to reach a ‘real’ assignment we need enough time walking around.</li>
  <li>num_samples: after mixing time, how many steps to take, how many samples to collect.</li>
</ul>

<p>The code is self explanatory:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Collecting Samples from the Makov Chain</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">sampleMarkovChain</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mix_time</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">seed_assignment</span><span class="p">):</span>
</span><span class="line">    <span class="n">max_iter</span> <span class="o">=</span> <span class="n">mix_time</span> <span class="o">+</span> <span class="n">num_samples</span>
</span><span class="line">    <span class="n">all_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">seed_assignment</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">seed_assignment</span><span class="p">)])</span>
</span><span class="line">
</span><span class="line">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
</span><span class="line">        <span class="n">seed_assignment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Gibbs</span><span class="p">(</span><span class="n">seed_assignment</span><span class="p">)</span>
</span><span class="line">        <span class="n">sol</span> <span class="o">=</span> <span class="p">[</span><span class="n">seed_assignment</span><span class="p">[</span><span class="n">e</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">seed_assignment</span><span class="p">)]</span>
</span><span class="line">        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="n">mix_time</span><span class="p">:</span>
</span><span class="line">            <span class="n">all_samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">all_samples</span><span class="p">,</span> <span class="n">sol</span><span class="p">))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">all_samples</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="inference">Inference</h2>

<p>Now that we have the samples we can compute the estimated marginals by counting, for each variable, how many times each as assignment shows up.</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Inference from MCMC using Gibbs Transitions</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">infere</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mix_time</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">seed_assignment</span><span class="p">):</span>
</span><span class="line">    <span class="n">all_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampleMarkovChain</span><span class="p">(</span><span class="n">mix_time</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">seed_assignment</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">M</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">V</span>
</span><span class="line">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="n">M</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">Factor</span><span class="o">.</span><span class="n">Factor</span><span class="p">([</span><span class="n">x</span><span class="p">])</span>
</span><span class="line">        <span class="n">M</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_values</span><span class="p">(</span><span class="n">FactorOperations</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">all_samples</span><span class="p">[:,</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)))</span>
</span><span class="line">    <span class="k">return</span> <span class="n">M</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/09/MCMC-Gibbs/">MCMC Gibbs - Theory</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-09T08:01:00-08:00" pubdate data-updated="true">Feb 9<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In the previous three posts we saw that we can compute approximate marginals in the mini Ising grid with uneven results. The more polarized the potentials, the more contradictions in the messages passed, the more difficult to converge to the true marginals.</p>

<p>In the following posts we are going to approach this problem (computing the aproximate marginals) from a different perspective, the Monte Carlo way. Beware that my grasp of these matters is flimsy at best, so excuse any mistakes I’ll made.</p>

<h2 id="sampling-for-inference">Sampling for Inference</h2>

<p>Our PGM is based on, and is a reflection of, a joint distribution of all its variables and the independence relationships among them. And we have already seen that if we have this joint distribution we have all the information necessary to answer any questions. Now, we also saw that except for the simplest graphs it is impractical to compute this joint distribution due to its size. </p>

<p>Imagine if we could come up with a way to sample this unknown and huge distribution. A way to generate perfectly viable, realistic assignments to all the variables in the joint distribution. With a sample big enough, we can estimate the answers we need, and we could get the approximate marginals.</p>

<p>For a Bayes Network, independently of its size this is doable. We start at the root nodes (eldest parents), roll a dice to see which assignments we get to those variables, and once computed start walking down the graph rolling dice based on the reduced tables of each node. When we have assigned values to all variables we have a complete assignment to the joint distribution. With enough of these we can infere estimated parameters. </p>

<p>For a Markov Network the previous process does not work, because there are no parents, no direction of flow. It is just a bunch of nodes talking to each other without concert and whose potentials depend on each other’s assignments. An egg and chicken conundrum.</p>

<h2 id="markov-chain">Markov Chain</h2>

<p>Markov Chain Monte Carlo (MCMC) gives us a way to generate samples from the joint distribution behind a Markov net using a trick over Markov Chains. (chains and nets, even by the same guy, are unrelated concepts).</p>

<p>A Markov chain consists of a set of states, where wach state has transition probabilities specifying the probabilities of stepping to each other state from that state. We start at some initial state, and in each step we move to another state, obeying the transition probabilities. Now, <em>some</em> Markov chains have the property that if we run them (hopping around) indefinitely, it will always converge to the same stationary probability distribution $ \pi $ for the states: that is, if we run it forever, the probability of being in some state s will be the same p no matter what the initial state was. So it won’t matter where you start, or where you went, because in the end, you’ll know where you’ll end up exactly (probabilisticly speaking, that is).</p>

<p>Better explanation, tons of examples and good writting in Wikipedia. Highly informative.</p>

<p><img class="center" src="/images/feb14/markov_example.png" width="500" title="Example of Markov Chain" /></p>

<h2 id="mutating-our-pgm-into-a-markov-chain">Mutating our PGM into a Markov Chain</h2>

<p>Back to our networks and graphs. We are going to build a Markov Chain where the states from the Markov chain will correspond to complete assignments to the variables of the model. In this way the probability to pass from one state to the next one corresponds to the probability of passing from one assignment of variables to another assignment. And if the whole mess converges over time, we should reach a set of constant probabilities for each assignment possible. Alas, the joint distribution. And sampling from it would get us to a good approximations of the marginals.</p>

<p>The trick is to come up with the right transition probabilities (between complete assignments / states) so a) the chain converges and 2) to the joint distribution we are after. There are different methods to get these transition probabilities right. We start with the simples one, Gibbs.</p>

<p>For example, given a Markov network of only three nodes, a clique. Each node holds a single binary variable [0,1,2]. Each node has a singleton factor on its variable, respectively var 0: [0.4, 0.6], var 1: [0.4, 0.6] and var 2: [0.6, 0.4]. There are additional pairwise factor between each pair of variables (0,1), (0,2) and (1,2), all with the same potentials [agree with value 1.0, and disagree with value 0.2]. The representation of such PGM is very simple, three nodes fully connected.</p>

<p><img class="center" src="/images/feb14/MN_simple.png" width="300" title="Mini Markov Net" /></p>

<p>Since all variables are binary, the total possible assignments is 2^3 = 8. We construct a Markov Chain with 8 nodes, one for each assignment. The connections between these nodes, better called transitions since they are directed edges, represent the ability to pass from one assignment to another. So if I am at assignment (0,0,0) and the next assignment I get is (0,0,1), then there is a transition between them.</p>

<p>Using the Gibbs transition process (to be studied below), we can collect a lot of samples from the distribution, a lot of assignments. The order of the samples collected will tell us about the individual hops, from and to which assignments me move in a single step, giving us the transitions (connections between nodes), and just counting the times we transition between each pair of nodes we can derive the transition probabilities.</p>

<p>In the following figure I have drawn the 8 nodes of the derived Markov Chain, and only drawn the transitions of two nodes, assignments (0,0,0), and (0,0,1) for clarity. We can see that once in assignment (0,0,0) there is a high probability of staying put (86.8%), while the probability of remaining in (0,0,1) is only 1%. Also we see that the highest probability for moving from (0,0,1) is to transition to (1,1,1) with 55.8% probability. Beware that these transition probabilities are not the stationary probabilities, but used to estimate them (furthermore there are only 8 stationary probabilities, because they are the probabilities of being at that assignment).</p>

<p><img class="center" src="/images/feb14/MC_simple.png" width="400" title="Mini Markov Chain" /></p>

<p>This is an exercise to visualize how the MC from a PGM looks like. For normal PGM it is just too messy and wild, and trying to visualize is futile.</p>

<h2 id="gibbs">Gibbs</h2>

<p>We start at some randomly chosen initial state, a  randomly chosen initial assignmentto all variables in the Markov network. </p>

<p>To move to the next state, we iterate through the variables. We replace each value in turn with a newly-selected value using probabilities <em>conditional on the values of all the other variables</em> – the new values for the variables we already changed this iteration and the old values for the variables we haven’t reached yet. When we have gone through all the variables, the new values, the new assignment to all variables, becomes our new state, and we’ve made one step in the Markov chain. </p>

<p>Now for some esoteric theory. A Markov chain is regular if there exists an integer k such that, for every x, x’, the probability of getting from x to x’ in exactly k steps is &gt; 0. The key is that first we choose the k, then that should hold for every pair of states! </p>

<p>From that definition we can extract the following: an MC is regular if the following sufficient conditions are met:</p>

<ul>
  <li>
    <p>Every two states are connected (I can go from every state to any other state, with positive probability)</p>
  </li>
  <li>
    <p>For every state, there is a self-transition (there should always be a positive probability of satying put in the state).</p>
  </li>
</ul>

<p>The importance of being regular is that by article of faith a regular Markov Chain converges to a unique stationary distribution regardless of start state. Aha!</p>

<p>Our Gibbs way of sampling creates an MC (derived itself from the PGM) that is regular, and therefore it has the same stationary distribution as the distribution of our PGM. So, in the long run, I can find/collect new states/assignments that show up with probability equal to the joint distribution of the PGM. </p>

<p>Finally, as another aside, for a Markov Network, another sufficient condition is that if all factors are positive, the Gibbs chain is regular. Beware, that being regular does not mean it is efficient and mixes.</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/08/LBP-3/">Loopy Belief Propagation (III)</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-08T08:01:00-08:00" pubdate data-updated="true">Feb 8<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>These are different approaches to improve convergence and accuracy of the marginals.</p>

<h2 id="smoothing-messages">1. Smoothing Messages</h2>

<p>Very easy to implement. When we compute every new message, instead of updating the delta, we update with smoothing: <code>coef*(new delta) + (1-coef)*(prev stored delta)</code>. This slows convergence in one hand but also avoids big jumps in the beliefs, giving time to achieve convergence at a more controlled pace. Trying for different coeficients I did not improve the marginals.</p>

<h2 id="tree-reparametrization">2. Tree Reparametrization</h2>

<p>Also easy to implement. Instead of using BFS for message scheduling of all nodes, we find a set of trees in the graph so all edges are covered. It is better if we have spanning trees (as long as possible). Then we propagate like with Clique Trees, only twice, forward and backward. The easiest way to get trees from a graph is using Depth First Search (DFS). Still, I don’t know how to get the minimum set of trees that encompass the whole graph, and I had to do it by hand :( Here is an example of 2 trees that span the grid:</p>

<p><img class="center" src="/images/feb14/TRP.png" width="500" title="Trees for LBP" /></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>DFS</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">class</span> <span class="nc">DFS_Paths</span><span class="p">:</span>
</span><span class="line">    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">G</span> <span class="o">=</span> <span class="n">G</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">s</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">discoveryPath</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># only the forward path, add backwards</span>
</span><span class="line">        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">V</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">marked</span> <span class="o">=</span> <span class="p">[</span><span class="bp">False</span><span class="p">]</span><span class="o">*</span><span class="n">V</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">edgeTo</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span><span class="o">*</span><span class="n">V</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">dfs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">)</span> <span class="c"># start only at designated root</span>
</span><span class="line">        <span class="k">pass</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">dfs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="bp">self</span><span class="o">.</span><span class="n">marked</span><span class="p">[</span><span class="n">v</span><span class="p">]</span> <span class="o">=</span> <span class="bp">True</span>
</span><span class="line">        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">adj</span><span class="p">[</span><span class="n">v</span><span class="p">]:</span>
</span><span class="line">            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">marked</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="o">==</span><span class="bp">False</span><span class="p">:</span>
</span><span class="line">                <span class="bp">self</span><span class="o">.</span><span class="n">discoveryPath</span><span class="o">.</span><span class="n">append</span><span class="p">([</span><span class="n">v</span><span class="p">,</span><span class="n">w</span><span class="p">])</span>
</span><span class="line">                <span class="bp">self</span><span class="o">.</span><span class="n">dfs</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
</span><span class="line">                <span class="bp">self</span><span class="o">.</span><span class="n">edgeTo</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">        <span class="k">pass</span>
</span><span class="line">
</span><span class="line">    <span class="k">def</span> <span class="nf">pathTo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
</span><span class="line">        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">marked</span><span class="p">[</span><span class="n">v</span><span class="p">]</span><span class="o">==</span><span class="bp">False</span><span class="p">:</span>
</span><span class="line">            <span class="k">return</span> <span class="bp">None</span>
</span><span class="line">        <span class="k">else</span><span class="p">:</span>
</span><span class="line">            <span class="n">path</span> <span class="o">=</span> <span class="p">[</span><span class="n">v</span><span class="p">]</span>
</span><span class="line">            <span class="n">x</span> <span class="o">=</span> <span class="n">v</span>
</span><span class="line">            <span class="k">while</span> <span class="n">x</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">:</span>
</span><span class="line">                <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">edgeTo</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
</span><span class="line">                <span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
</span><span class="line">            <span class="k">return</span> <span class="n">path</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>When I run on indiscriminate sets of trees I converge to the bad marginals. But if I use two trees, one long spanning DFS tree plus a manually computed one for the missed edges, I get somewhat better results (sqr error from 0.99 to 0.75):</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: center">var</th>
      <th style="text-align: right">exact marginal</th>
      <th style="text-align: right">approx. marg (LBP)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: right"> </td>
      <td style="text-align: right"> </td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: right">0.537310955208</td>
      <td style="text-align: right">0.723905954527</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: right">0.556558936419</td>
      <td style="text-align: right">0.728300716867</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: right">0.552439017812</td>
      <td style="text-align: right">0.592195736634</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: right">0.573509953445</td>
      <td style="text-align: right">0.835859875384</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: right">0.6</td>
      <td style="text-align: right">0.911873353528</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: right">0.607090867757</td>
      <td style="text-align: right">0.900624901637</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: right">0.611810074317</td>
      <td style="text-align: right">0.8791687735</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: right">0.621715017707</td>
      <td style="text-align: right">0.927292899198</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: right">0.626241528067</td>
      <td style="text-align: right">0.905241175589</td>
    </tr>
  </tbody>
</table>
<p><br /></p>

<h2 id="residual-belief-propagation">3. Residual Belief Propagation</h2>

<p>This last effort is based on the fact that not all messgaes have the same importance. Some change the beliefs a great deal, while others are less important. It is easy to look at the way messages achieve convergence, andd see that not all reach equilibrium at the same time. The same happens with the betas.</p>

<p>What I did was implement a priority queue, and as compute each message I store it in the queue with a key that measures how much the message changed. Since all messages are univariate (check cluster graph and realize that there are only pairwise potentials), each message has two values (p, 1-p). The measure I use is the abs of the difference in p. The first pass (BFS) goes through all edges and computes all messages, but starting with the second pass, we compute only the messages that change beyond a tolerance and ordered so the more important run first. </p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Calibrate with RBP</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">calibrate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span><span class="line">    <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span>
</span><span class="line">
</span><span class="line">    <span class="n">path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">computePath</span><span class="p">()</span>
</span><span class="line">    <span class="n">q</span> <span class="o">=</span> <span class="n">PriorityQueue</span><span class="p">()</span>
</span><span class="line">
</span><span class="line">    <span class="n">keep_going</span> <span class="o">=</span> <span class="bp">True</span>
</span><span class="line">    <span class="n">cycles</span><span class="p">,</span> <span class="n">messages</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
</span><span class="line">    <span class="k">while</span> <span class="n">keep_going</span><span class="p">:</span>
</span><span class="line">        <span class="n">cycles</span> <span class="o">+=</span><span class="mi">1</span>
</span><span class="line">        <span class="n">keep_going</span> <span class="o">=</span> <span class="bp">False</span>
</span><span class="line">        <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">path</span><span class="p">:</span>
</span><span class="line">            <span class="n">from_v</span><span class="p">,</span> <span class="n">to_w</span> <span class="o">=</span> <span class="n">e</span>
</span><span class="line">            <span class="n">pos_to</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adj</span><span class="p">[</span><span class="n">from_v</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">to_w</span><span class="p">)</span>
</span><span class="line">            <span class="n">messages</span> <span class="o">+=</span><span class="mi">1</span>
</span><span class="line">            <span class="n">prev_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">[</span><span class="n">from_v</span><span class="p">][</span><span class="n">pos_to</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span><span class="line">            <span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">[</span><span class="n">from_v</span><span class="p">][</span><span class="n">pos_to</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mssg</span><span class="p">(</span><span class="n">from_v</span><span class="p">,</span> <span class="n">to_w</span><span class="p">)</span>
</span><span class="line">            <span class="n">new_res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">[</span><span class="n">from_v</span><span class="p">][</span><span class="n">pos_to</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</span><span class="line">
</span><span class="line">            <span class="n">mag</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">prev_res</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="n">prev_res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">new_res</span><span class="p">,</span> <span class="n">prev_res</span><span class="p">,</span> <span class="mf">1e-6</span><span class="p">):</span>
</span><span class="line">                <span class="n">q</span><span class="o">.</span><span class="n">put</span><span class="p">({</span><span class="nb">abs</span><span class="p">(</span><span class="n">mag</span><span class="p">):[</span><span class="n">from_v</span><span class="p">,</span> <span class="n">to_w</span><span class="p">]})</span>
</span><span class="line">                <span class="n">keep_going</span> <span class="o">=</span> <span class="bp">True</span>
</span><span class="line">
</span><span class="line">        <span class="k">if</span> <span class="n">cycles</span><span class="o">&gt;</span><span class="mi">1000</span><span class="p">:</span>
</span><span class="line">            <span class="o">...</span>
</span><span class="line">
</span><span class="line">        <span class="n">path</span> <span class="o">=</span> <span class="p">[]</span>
</span><span class="line">        <span class="k">while</span> <span class="ow">not</span> <span class="n">q</span><span class="o">.</span><span class="n">empty</span><span class="p">():</span>
</span><span class="line">            <span class="n">a</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
</span><span class="line">            <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">values</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</span><span class="line">        <span class="n">path</span><span class="o">.</span><span class="n">reverse</span><span class="p">()</span> <span class="c"># because it is a minQ</span>
</span><span class="line">
</span><span class="line">    <span class="k">print</span> <span class="s">&#39;Cycles:&#39;</span><span class="p">,</span> <span class="n">cycles</span><span class="p">,</span> <span class="s">&#39;Messages passed&#39;</span><span class="p">,</span> <span class="n">messages</span>
</span><span class="line">    <span class="c"># compute the beliefs</span>
</span><span class="line">    <span class="o">...</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>This way we only need 3 cycles and 104 messages. The first cycle goes through all 48 edges. the second also computes 48 messages but in a different order (prioritized by heuristic importance), and the third only comutes 8 messages. The results are even better:</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: center">var</th>
      <th style="text-align: right">exact marginal</th>
      <th style="text-align: right">approx. marg (LBP)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: right"> </td>
      <td style="text-align: right"> </td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: right">0.537310955208</td>
      <td style="text-align: right">0.614445509854</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: right">0.556558936419</td>
      <td style="text-align: right">0.632046274666</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: right">0.552439017812</td>
      <td style="text-align: right">0.592793285825</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: right">0.573509953445</td>
      <td style="text-align: right">0.699814842287</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: right">0.6</td>
      <td style="text-align: right">0.759645004711</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: right">0.607090867757</td>
      <td style="text-align: right">0.749109157114</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: right">0.611810074317</td>
      <td style="text-align: right">0.739850591712</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: right">0.621715017707</td>
      <td style="text-align: right">0.822523867844</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: right">0.626241528067</td>
      <td style="text-align: right">0.848549594174</td>
    </tr>
  </tbody>
</table>
<p><br /></p>

<p>Square distance error is 0.42. If instead of using the BSF as the initial path, we compute all messages and create an initial path based on a priority queue as before, the error can be taken further down to 0.33.</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: center">var</th>
      <th style="text-align: right">exact marginal</th>
      <th style="text-align: right">approx. marg (LBP)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: right"> </td>
      <td style="text-align: right"> </td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: right">0.537310955208</td>
      <td style="text-align: right">0.399020732163</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: right">0.556558936419</td>
      <td style="text-align: right">0.378280542986</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: right">0.552439017812</td>
      <td style="text-align: right">0.443100330486</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: right">0.573509953445</td>
      <td style="text-align: right">0.465753424658</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: right">0.6</td>
      <td style="text-align: right">0.641605971227</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: right">0.607090867757</td>
      <td style="text-align: right">0.662337662338</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: right">0.611810074317</td>
      <td style="text-align: right">0.641605971227</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: right">0.621715017707</td>
      <td style="text-align: right">0.770344431088</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: right">0.626241528067</td>
      <td style="text-align: right">0.719502074689</td>
    </tr>
  </tbody>
</table>
<p><br /></p>

<p>And finally, if instead of prioritizing the queue based on the importance of the messages we do it based on how each new message changes the marginalized belief of the receiving node we lower the error further to 0.24, although we need to cycle 13 times and pass 296 messages.</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: center">var</th>
      <th style="text-align: right">exact marginal</th>
      <th style="text-align: right">approx. marg (LBP)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: right"> </td>
      <td style="text-align: right"> </td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: right">0.537310955208</td>
      <td style="text-align: right">0.444436529385</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: right">0.556558936419</td>
      <td style="text-align: right">0.605683356431</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: right">0.552439017812</td>
      <td style="text-align: right">0.494869086601</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: right">0.573509953445</td>
      <td style="text-align: right">0.562421013105</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: right">0.6</td>
      <td style="text-align: right">0.692305538097</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: right">0.607090867757</td>
      <td style="text-align: right">0.64302214095</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: right">0.611810074317</td>
      <td style="text-align: right">0.682604471501</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: right">0.621715017707</td>
      <td style="text-align: right">0.745950622502</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: right">0.626241528067</td>
      <td style="text-align: right">0.748513253977</td>
    </tr>
  </tbody>
</table>
<p><br /></p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Code to compute the magnitude based on belief change</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">magnitude</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">from_v</span><span class="p">,</span> <span class="n">to_w</span><span class="p">):</span>
</span><span class="line">    <span class="n">beta</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">factors</span><span class="p">[</span><span class="n">to_w</span><span class="p">])</span>
</span><span class="line">    <span class="n">past_beta</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
</span><span class="line">
</span><span class="line">    <span class="n">affected_var</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">beta</span><span class="o">.</span><span class="n">variables</span> <span class="k">if</span> <span class="n">e</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">box</span><span class="p">[</span><span class="n">from_v</span><span class="p">]]</span>
</span><span class="line">    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">adj</span><span class="p">[</span><span class="n">to_w</span><span class="p">]:</span>
</span><span class="line">        <span class="n">posX</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">adj</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">to_w</span><span class="p">)</span>
</span><span class="line">        <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">posX</span><span class="p">]</span>
</span><span class="line">        <span class="n">beta</span> <span class="o">=</span> <span class="n">FactorOperations</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span> <span class="c">#### NORMALIZING</span>
</span><span class="line">
</span><span class="line">    <span class="k">if</span> <span class="n">affected_var</span><span class="o">!=</span><span class="p">[]:</span>
</span><span class="line">        <span class="n">past_beta</span> <span class="o">=</span> <span class="n">FactorOperations</span><span class="o">.</span><span class="n">marginalize</span><span class="p">(</span><span class="n">past_beta</span><span class="p">,</span> <span class="n">affected_var</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class="line">        <span class="n">beta</span> <span class="o">=</span> <span class="n">FactorOperations</span><span class="o">.</span><span class="n">marginalize</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">affected_var</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span><span class="line">    <span class="n">mag</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">past_beta</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">/</span><span class="n">past_beta</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span><span class="line">    <span class="k">return</span> <span class="n">mag</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>Nevertheless, although it gives good results for a 9 node Ising grid, the more the nodes the worse it works. The RBP is the best approach found and I am sure that with a better designed key for the queue (to show which messages should be prioritized) bigger grids can also be optimized.</p>

</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/10/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/8/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/06/18/Founders/">Founders</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/12/05/ruby-continuations/">Ruby Continuations</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/04/Seattlerb-GoL/">Seattle Ruby Game Of Life</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/01/CodeFellows/">Article Published</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/15/Scheme-GOL/">Game of Life in Scheme</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
