
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>SotoSeattle</title>
  <meta name="author" content="Javier Soto">

  
  <meta name="description" content="The code for a Q based on the Uniform distribution has three steps: come up with a new complete assignment out of the blue (literally) compute the &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://sotoseattle.github.io/blog/page/8">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="" rel="alternate" title="SotoSeattle" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=Fjalla+One' rel='stylesheet' type='text/css'>
<!-- MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["$$","$$"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  

</head>

<body   class="collapse-sidebar sidebar-footer" >
  <header role="banner">
	<div class="header-title"><a href="/">SotoSeattle</a></div>


	<br><div class="header-subtitle">Curious Investor & Coder</div>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
<!--   <li><a href="" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li> -->
  
  <li class='icon'>
    <a href="https://github.com/sotoseattle/">
      <img src="/images/logos/github_icon.png" width="36" height="27">
      </img>
    </a>
  </li>
  <li class='icon'>
    <a href="https://www.linkedin.com/in/sotoseattle/">
      <img src="/images/logos/linkedin-256.png"  width="27" height="27">
      </img>
    </a>
  </li>
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:sotoseattle.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
<!--   <li><a href="projects/">Projects</a></li> -->
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <!-- <li><a href="/images/Javier_Soto_resume.pdf">Resume</a></li> -->
  <li><a href="/about">About</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/13/MCMC-MH-Uniform/">MCMC MH Uniform</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-13T08:01:00-08:00" pubdate data-updated="true">Feb 13<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>The code for a Q based on the Uniform distribution has three steps:</p>

<ul>
  <li>come up with a new complete assignment out of the blue (literally)</li>
  <li>compute the acceptance probability. When we compute π(x’)Q(x’ → x), π is the joint probability reduced by the complete assignment (unormalized), meaning the total probability of having that assignment. The question is, how do I compute Q(x’ → x)? Since it is the uniform distribution, Q(x’ → x) == Q(x → x’) == constant, so it cancels out and I only need to compute the ratio of π(x’) / π(x).</li>
  <li>decide with the new acceptance if, like The Clash asks, should we stay or should we go.</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Q from Uniform dist</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">MH_Uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">from_ass</span><span class="p">):</span>
</span><span class="line">    <span class="c"># get a random complete assignment from uniform distribution</span>
</span><span class="line">    <span class="n">cardio</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">totCard</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">v</span><span class="p">]</span>
</span><span class="line">    <span class="n">a</span> <span class="o">=</span> <span class="n">floor</span><span class="p">([</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span><span class="o">*</span><span class="n">cardio</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">cardio</span><span class="p">))])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
</span><span class="line">    <span class="n">to_ass</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">e</span><span class="p">,</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">e</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">v</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># compute the probabilities of transitions</span>
</span><span class="line">    <span class="n">fwd</span><span class="p">,</span> <span class="n">bwd</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span>
</span><span class="line">    <span class="k">for</span> <span class="n">fu</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">G</span><span class="o">.</span><span class="n">factors</span><span class="p">:</span>
</span><span class="line">        <span class="n">fwd</span> <span class="o">+=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">FactorOperations</span><span class="o">.</span><span class="n">reduce_to_value</span><span class="p">(</span><span class="n">fu</span><span class="p">,</span> <span class="n">to_ass</span><span class="p">))</span>
</span><span class="line">        <span class="n">bwd</span> <span class="o">+=</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">FactorOperations</span><span class="o">.</span><span class="n">reduce_to_value</span><span class="p">(</span><span class="n">fu</span><span class="p">,</span> <span class="n">from_ass</span><span class="p">))</span>
</span><span class="line">
</span><span class="line">    <span class="c"># compute acceptance probability and return the new state</span>
</span><span class="line">    <span class="n">p_acceptance</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">fwd</span><span class="p">)</span><span class="o">/</span><span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">bwd</span><span class="p">))])</span>
</span><span class="line">
</span><span class="line">    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">p_acceptance</span><span class="p">:</span>
</span><span class="line">        <span class="n">from_ass</span> <span class="o">=</span> <span class="n">to_ass</span>
</span><span class="line">    <span class="k">return</span> <span class="n">from_ass</span>
</span><span class="line"><span class="n">end</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>To speed up the process, I have added a new utility function to Factor Operations, to extract the potential/probability value of a complete assigment from a given factor. It is the same as reducing, but instead of giving back a reduced factor, since we have a complete assignment (all variables have assignments) we return a value (float).</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span>Get Factor Value of Complete Assignment</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="k">def</span> <span class="nf">reduce_to_value</span><span class="p">(</span><span class="n">fA</span><span class="p">,</span> <span class="n">evidence</span><span class="p">):</span>
</span><span class="line">    <span class="sd">&#39;&#39;&#39;reduce factor by complete assignment to a single value&#39;&#39;&#39;</span>
</span><span class="line">    <span class="n">complete_ass</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="n">evidence</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">fA</span><span class="o">.</span><span class="n">variables</span><span class="p">])</span>
</span><span class="line">    <span class="k">return</span> <span class="n">fA</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="n">complete_ass</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>

<h2 id="mixing">Mixing</h2>

<p>As before we run the MCMCMHU to collect a total of 70,000 samples. The initial assignment is initialized to all ones. </p>

<p>Now we get subsets every 1,000 samples of size 10,000. That is 59 windows of 10,000 samples collected after different mixing times.</p>

<p>Ploting the difference between the exact marginal and our estimated marginals of the three variables (number 0 as red, 4 as blue, and 8 as green), we have the following charts (from two different runs).</p>

<div>
	<img class="left" src="/images/feb14/MHU_mix.png" width="425" title="Mixing Windows Run 1" />
	<img class="right" src="/images/feb14/MHU_mixB.png" width="425" title="Mixing Windows Run 2" />
</div>
<p><br /></p>

<p>No clear mixing threshold.</p>

<h2 id="sample-size">Sample size</h2>

<p>Following the previous settings but fixing the mixing time to 1,000, we can plot the difference between exact and estimated marginals as we collect more samples. From the charts (two different runs) we see that it converges beyond 20,000. </p>

<div>
	<img class="left" src="/images/feb14/MHU_size.png" width="425" title="Sample Size Run 1" />
	<img class="right" src="/images/feb14/MHU_sizeB.png" width="425" title="Sample Size Run 2" />
</div>
<p><br /></p>

<p>Nevertheless, it seems to work worse than Gibbs.</p>

<h2 id="marginals-convergence">Marginals Convergence</h2>

<p>We again run twice the model and see how well the estimated marginals converge across runs. All models are Ising grids with 9 variables, have mixing time of 7,000 and sample size of 10,000. </p>

<div>
	<img class="left" src="/images/feb14/MHU_compare_runs_0703.png" width="425" />
	<img class="right" src="/images/feb14/MHU_compare_runs_102.png" width="425" />
	<img class="center" src="/images/feb14/MHU_compare_runs_095005.png" width="425" />
</div>

<p>Again, worse results.</p>

<h2 id="comparison-to-gibbs">Comparison to Gibbs</h2>

<p>Lile we did for Gibbs, we run 10 times this MCMCMHU model. Now with mixing time of 5,000 and sample size of 30,000. Starting assignment, 5 runs with 0s, and 5 runs with 1s. We compute the error as before and see what results we get as compared to the exact marginals for all variables.</p>

<p>The error, as defined in previous posts, from our 10 runs is 0.12 on the average (vs. 0.065 from Gibbs), with a range of [0.03, 0.24] (vs. [0.005, 0.17]) and a standard deviation of 0.07 (vs. 0.05). Definetively worse performance.</p>

<p>Finally, this is for a 9 node Ising net. For a 4 sided net with 16 nodes, with strong correlation this MHU method performs very poorly because we get stuck in specific assignments for very long times. Because the probability of a new assignment (randomly chosen) tends to be so small as compared to the probability we start with that the ratio is miniscule, and A() always rejects. The more correlated (i.e. higher on diagonal values vs off diagonal) the more steps we need to take, and even that doesn’t guarantee good results. Consider that for 100,000 steps, only around 50 are accepted, which means we need to run it much longer to achieve good results. </p>

<p>For the easiest 16 nodes with pairwise potentials [0.5, 0.5], 0 mixing and 30,000 steps, MHU and Gibs give similar good results. MHU gives us an average error and standard dev of [0.02, 0.004], while Gibbs achieves [0.01, 0.001].</p>

<div>
    <img class="left" src="/images/feb14/Gibbs_hist.png" width="425" title="Gibbs Error 100 Runs" />
    <img class="right" src="/images/feb14/MHU_hist.png" width="425" title="MHU Error 100 Runs" />
</div>
<p><br /></p>

<p>Finally we compare the histogram of errors for these two methods when applied to the [1.0, 0.2] in the 16 node Ising model. Each histogram is based on 100 runs. Both have a mixing time of 10,000 and a sampling size of 70,000. Gibbs’s histogram shown in red, MHU in blue.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/12/MCMC-MH-Theory/">MCMC ~ Metropolis Hastings</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-12T08:01:00-08:00" pubdate data-updated="true">Feb 12<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>With Gibbs sampling, we changed the assigment of our variables one at a time, and after cycling though all the variables we got to new state, a complete new assignment to all variables. </p>

<p>For regular chains, Gibbs sampling is a legal MCMC transition, meaning that it results in a Markov Chain with the right stationary distribution. But with high autocorrelations between variables, trying to change the value of one variable at a time tends to leave us in the same state or the same cluster of similar states, because the one variable we change at a time remains strongly constrained by its neighbors. So mixing is slow. </p>

<p>We’d like to use a transition that lets us make bigger moves around the state space, but we still have to make sure our transition is a legal MCMC. The Metropolis-Hastings algorithm tries to do this. We choose the transition we actually want – one that takes large steps around the state space. Then we modify that transition, using an acceptance probability, that forces it into a legal MCMC transition. </p>

<h2 id="q-and-a">Q and A</h2>

<p>Our MCMC transition then has two steps. First we choose a proposed new state (x’), using our transition probabilities Q(x → x’). Then we accept that new state, with probability A(x → x’). If the transition is accepted, our new assignment is the proposed new state, but if the transition is rejected, our new assignment is the same as the old one.</p>

<p>For a given proposal transition Q(x → x’), the acceptance probability is given by: </p>

<script type="math/tex; mode=display">
\begin{align}

A(x \rightarrow x') = min \left[ 1, \frac{\pi(x') Q(x' \rightarrow x)}{\pi(x) Q(x \rightarrow x')} \right]

\end{align}
</script>

<p>The key is that A is determined once we know Q, so the art is in finding the right Q. One that:</p>

<ul>
  <li>
    <p>is reversible, so if there is a positive probability of getting from x to x’, then the probability of getting from x’ to x is also positive.</p>
  </li>
  <li>
    <p>allows us to go far away places, but without getting so far away that the probability of acceptances becomes too low, because then we won’t move at all.</p>
  </li>
</ul>

<p>Finding Q is hard, and an art that depends of the PGM at hand. Also, a legal Q is such that for every two states/assignments (x, x’), the probability of transitioning (x → x’) is exactly the same as the probability of transitioning in reverse (x’ → x), [such property is called ‘detailed balance’]. </p>

<h2 id="q-as-gibbs-or-gibbs-as-mh">Q as Gibbs, or Gibbs as MH</h2>

<p>We can insert Gibbs into our MH framework with a trick. We consider that the Gibbs sampling method is our Q(x → x’), our way of finding new assignment (even if in litle steps which violates the MH goal, but it is a trick). Since Gibbs is already legal, all we need is to define an approval distribution A(x → x’) that always accepts whatever Q (Gibbs) brings, for example A(x → x’) = 1.</p>

<h2 id="q-as-the-uniform-distribution">Q as the Uniform distribution</h2>

<p>A very simple Q is based on the Uniform distribution. For all the variables in the assignment we randomly choose (independently) new assignments. Brutishly. All at once. Then we compute the transitions by multiplying the specific probabilities of getting that evidence (to and from the initial assignment from/to the new assignment). Now we can compute the acceptance probability A.</p>

<p>The problem is that because we are pulling out assignments out of a hat randomly, most of them will be very unique/unrelated and therefore with low-probability transition. A low probability transition will surely get rejected by the acceptance step, and so we end up lingering in one state rather than exploring the state space. So we may end up with tons of failed trials, tons of staying put, and we only move out of the assignment when we don’t go too far.</p>

<p>Ising models and image segmentation applications tend to rely on pairwise Markov networks. In these networks, adjacent variables tend to take on the same values. This makes it hard to explore the space for proposal distributions which change the value of one variable at a time, such as Gibbs or the uniform proposal distribution.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
  
    
      <h1 class="entry-title"><a href="/blog/2014/02/11/MCMC-Gibbs-Analysis/">MCMC Gibbs - Analysis</a></h1>
      
      
    
      <p class="meta">
        








  


<time datetime="2014-02-11T08:01:00-08:00" pubdate data-updated="true">Feb 11<span>th</span>, 2014</time>
        
      </p>
    
  </header>


  <div class="entry-content"><p>We are going to use our primitive MCMC Gibbs Inference engine on our 9 variable toy Ising grid (with histerical potentials [1.0, 0.2]). And then compare our results to our previous approximation using Loopy Belief Propagation.</p>

<p><img class="center" src="/images/feb14/toy_grid.png" width="300" title="Small Pairwise Markov Grid" /></p>

<p>The big problem is that there is no clear cut way to know when we have left the wandering zone around and entered the domain of the joint probability. We cannot know when the chain has mixed, we can only guess.</p>

<h2 id="mixing">Mixing</h2>

<p>As a first experiment we run our MCMC to collect a total of 70,000 samples. The initial assignment is initialized to all ones. </p>

<p>Now from the samples we get subsets every 1,000 samples of size 10,000. That is 59 subsets that go from sample [0 to 10,000], [1,000 to 11,000], [2,000 to 12,000], …. Each subset is a window, of 10,000 samples collected after different mixing times.</p>

<p>If we plot the difference between the exact marginal and our MCMC estimated marginal of the three variables (number 0 as red, 4 as blue, and 8 as green), we have the following charts (from two different runs).</p>

<div>
	<img src="/images/feb14/dist_to_exact_mcmcg.png" width="425" title="Run 1" />
	<img class="right" src="/images/feb14/dist_to_exact_mcmcgB.png" width="425" title="Run 2" />
</div>

<p>What I deduce from this (and here is pure amateur talking) is that all variables tend to move together. I don’t know what is a good mixing time, it looks like it is pretty much mixed from the start. Nevertheless, I’ll choose 7,000 to be safe. Also, as we walk around, the system oscilates and spike away from the exact marginals in a bounded range.</p>

<h2 id="sample-size">Sample size</h2>

<p>Following the previous settings but fixing the mixing time to 7,000, we can plot the difference between exact and estimated marginals as we collect more samples. From the charts (two different runs) we see that anything beyond 30,000 samples seems to be ok.</p>

<div>
	<img class="left" src="/images/feb14/dist_to_exact_mcmcg_3.png" width="425" title="Run 1" />
	<img class="right" src="/images/feb14/dist_to_exact_mcmcg_3B.png" width="425" title="Run 2" />
</div>

<h2 id="marginals-convergence">Marginals Convergence</h2>

<p>Gibbs walks one step at a time, so for grids with high potentials (high correlation between variables), it tends to stay for long periods next to high potential states (or clusters), slowing down mixing.</p>

<p>In the following charts we run twice different models and see how well the estimated marginals converge across runs. All models are Ising grids with 9 variables, have mixing time of 7,000 and sample size of 10,000. </p>

<div>
	<img class="left" src="/images/feb14/compare_runs_0703.png" width="425" />
	<img class="right" src="/images/feb14/compare_runs_102.png" width="425" />
	<img class="center" src="/images/feb14/compare_runs_095005.png" width="425" />
</div>

<p>The better convergence the closer the estimated marginals to the diagonal. The first model has potentials [0.7, 0.3] and all variables are along the diagonal. The same happens for the second model, our optimized [1., 0.2], where the a corner variable get a bit off, but good for all purposes. </p>

<p>A third model with even further heighened potentials [0.95, 0.05] shows discouraging results. The true marginals are all in the narrow range [0.5974, 0.6009], but our estimated marginals for the first run are tightly clustered around 0.0, while all marginals for the second run are tightly clustered around 0.6. This can be palliated with a bigger sample size up to a point.</p>

<p>Two points to make: 1) this shows convergence between runs, not to the exact marginal. 2) The lower correlation between variables, the better convergence.</p>

<h2 id="comparison-to-lbp">Comparison to LBP</h2>

<p>Finally we are going to run 10 times the model we have. Mixing time of 7,000. Sample size of 20,000. Starting assignment, 5 runs with 0s, and 5 runs with 1s. We are going to compute the error as before and see what results we get as compared to the exact marginals for all variables.</p>

<p>The error, as defined in previous posts, from our 10 runs is 0.065 on the average, with a range of [0.005, 0.17] and a standard deviation of 0.05. That is a further improvement from our best approximation of 0.24 with RBP.</p>

<p>Nevertheless, although Gibbs gives us on the average much better results, from time to time it go cyclothymic and spikes markedly away from the exact marginals.</p>

<p>As a data point for comparison purposes, taking the estimated marginals from our last run (the 10th run) we get an error of 0.059, and the already familiar table:</p>

<table class="widetable">
  <thead>
    <tr>
      <th style="text-align: center">var</th>
      <th style="text-align: right">exact marginal</th>
      <th style="text-align: right">MCMC Gibbs aprox</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"> </td>
      <td style="text-align: right"> </td>
      <td style="text-align: right"> </td>
    </tr>
    <tr>
      <td style="text-align: center">0</td>
      <td style="text-align: right">0.537310955208</td>
      <td style="text-align: right">0.52015</td>
    </tr>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: right">0.556558936419</td>
      <td style="text-align: right">0.541</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: right">0.552439017812</td>
      <td style="text-align: right">0.53745</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: right">0.573509953445</td>
      <td style="text-align: right">0.5518</td>
    </tr>
    <tr>
      <td style="text-align: center">4</td>
      <td style="text-align: right">0.6</td>
      <td style="text-align: right">0.57795</td>
    </tr>
    <tr>
      <td style="text-align: center">5</td>
      <td style="text-align: right">0.607090867757</td>
      <td style="text-align: right">0.58745</td>
    </tr>
    <tr>
      <td style="text-align: center">6</td>
      <td style="text-align: right">0.611810074317</td>
      <td style="text-align: right">0.5918</td>
    </tr>
    <tr>
      <td style="text-align: center">7</td>
      <td style="text-align: right">0.621715017707</td>
      <td style="text-align: right">0.59885</td>
    </tr>
    <tr>
      <td style="text-align: center">8</td>
      <td style="text-align: right">0.626241528067</td>
      <td style="text-align: right">0.60425</td>
    </tr>
  </tbody>
</table>
<p><br /></p>

<p>Even for higher order Ising grids, as long as we don’t have way too highly correlated variables, we mix long enhough and we collect a sufficiently big sample size, we get pretty decent results!</p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/9/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/blog/page/7/">Newer &rarr;</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2016/06/18/Founders/">Founders</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/12/05/ruby-continuations/">Ruby Continuations</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/04/Seattlerb-GoL/">Seattle Ruby Game Of Life</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/03/01/CodeFellows/">Article Published</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/02/15/Scheme-GOL/">Game of Life in Scheme</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2016 -  Javier Soto <br/>
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a> + <a href="https://github.com/ioveracker/mnml">mnml</a>.
	  
  </span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'sotoseattle';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>
